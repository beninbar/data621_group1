---
title: 'Homework #2: Classification Metris'
subtitle: 'Critical Thinking Group 1'
author: 'Ben Inbar, Cliff Lee, Daria Dubovskaia, David Simbandumwe, Jeff Parks, Nick Oliver'
output:
  html_document:
    toc: yes
    toc_float: yes
    theme: united
  pdf_document:
    toc: yes
editor_options:
  chunk_output_type: console
  markdown: 
    wrap: sentence
---


```{r setup, include=FALSE}
library(tidyverse)
library(dplyr)
library(caret)
library(pROC)
library(skimr)
library(lemon)
library(DT)
library(kableExtra)
library(forecast)
source("classification_functions.R")

knitr::opts_chunk$set(echo = TRUE)
```

## Overview
In this homework assignment, we will work through various classification metrics. We will create functions in R to carry out the various calculations. We will also investigate some functions in packages that will let you obtain the equivalent results. Finally, we will create graphical output that also can be used to evaluate the output of classification models, such as binary logistic regression.

<br></br>

## 1. Download the data
The classification output data set `classification-output-data.csv` was loaded into the variable `data`. The data set contains 181 observations of 11 variables. This report will focus on three variables: "class" (the true class for each observation), "scored.class" (the predicted class for each observation), and "scored.probability" (the predicted likelihood of success for each observation).
```{r data}
data <- read.csv("https://raw.githubusercontent.com/cliftonleesps/data621_group1/main/hw2/classification-output-data.csv")
```

```{r summary, echo=FALSE}
DT::datatable(
      data,
      extensions = c('Scroller'),
      options = list(scrollY = 350,
                     scrollX = 500,
                     deferRender = TRUE,
                     scroller = TRUE,
                     dom = 'lBfrtip',
                     fixedColumns = TRUE, 
                     searching = FALSE), 
      rownames = FALSE) 
```
<br></br>

## 2. Raw confusion matrix

The data set has three key columns we will use:

* `class`: the actual class for the observation \

* `scored.class`: the predicted class for the observation (based on a threshold of 0.5) \

* `scored.probability`: the predicted probability of success for the observation

Using the the table() function, we can check the raw confusion matrix for this scored dataset.

The confusion matrix below has rows and columns representing the actual and predicted classes respectively for two types of categories (0 and 1). 

* `119` is the number of observations that are correctly classified as class 0 (`TN`, true negative observations)

* `30` is the number of observations belonging to class 0 that are incorrectly classified as class 1 (`FN`, false negative observations)

* `5` is the number of observations belonging to class 1 but classified incorrectly as class 0 (`FP`, false positive observations)

* `27` is the number of observations that are correctly classified as positive class 1 (`TP`, true positive observations)

<br></br>
```{r raw-matrix, echo=FALSE}
data %>%
  select(scored.class, class) %>%
  table() |>
  kable() |>
  kable_styling(latex_options = "striped")

```

<br></br>

## 3. Accuracy
The function classify_predictions() calculates the amount of true positive, true negative, false negative, and false positive observations. Then, the function test_accuracy() takes the data set as a dataframe, with actual and predicted classifications identified, and returns the accuracy of the predictions. Based on the results of the functions, the accuracy is `0.81`.

$$Accuracy = \frac{TP+TN}{TP+FP+TN+FN}$$
**MAYBE improve classify_predictions() to accept df, x1, x2 (data frame, )?**
```{r classify_predictions}
classify_predictions <- function(df) {

    true_negative <- df %>% select(class, scored.class) %>% filter(scored.class == 0 & class == 0)
    true_positive <- df %>% select(class, scored.class) %>% filter(scored.class == 1 & class == 1)

    false_negative <- df %>% select(class, scored.class) %>% filter(scored.class == 0 & class == 1)
    false_positive <- df %>% select(class, scored.class) %>% filter(scored.class == 1 & class == 0)

    matrix <- list("tn" = nrow(true_negative),
                   "tp" = nrow(true_positive),
                   "fn" = nrow(false_negative),
                   "fp" = nrow(false_positive))
    return (matrix)
}

```

```{r accuracy}
test_accuracy <- function(df) {
    m <- classify_predictions(df)
    accuracy <- (m$tp + m$tn) / (m$tp + m$fp + m$tn + m$fn)
    return (accuracy)
}
```

```{r check_accuracy, echo=FALSE}
sprintf("The accuracy of the predictions: %.3f", test_accuracy(data))
```
<br></br>

## 4. Classification error rate
The function test_classification_error_rate() takes the data set as a dataframe, with actual and predicted classifications identified, and returns the classification error rate of the predictions. The function is based on the formula below. Based on the results of the functions, the classification error rate of the predictions is `0.19`.

$$Classification\: Error\: Rate = \frac{FP+FN}{TP+FP+TN+FN}$$

```{r classification_error_rate}
test_classification_error_rate <- function(df) {
    m <- classify_predictions(df)
    cer <- (m$fp + m$fn) / (m$tp + m$fp + m$tn + m$fn)
    return (cer)
}
```

```{r check_error_rate, echo=FALSE}
sprintf("The classification error rate of the predictions: %.3f", test_classification_error_rate(data))
```
As we see below, the accuracy and the error rate sums to one which means our custom functions work correctly.
```{r check_sum, echo=FALSE}
sprintf("The sum of the accuracy and classification error: %.1f", test_classification_error_rate(data) + test_accuracy(data))
```
<br></br>

## 5. Precision
The function test_precision() takes the data set as a dataframe, with actual and predicted classifications identified, and returns the precision of the predictions. The function is based on the formula below. Based on the results of the functions, the precision of the predictions is `0.84`.

$$Precision = \frac{TP}{TP+FP}$$

```{r precision}
test_precision <- function(df) {
    m <- classify_predictions(df)
    p <- m$tp / (m$tp + m$fp)
    return (p)
}
```

```{r check_precision, echo=FALSE}
sprintf("The precision of the predictions: %.3f", test_precision(data))
```
<br></br>

## 6. Sensitivity
The test_sensitivity() function takes the data set as a dataframe, with actual and predicted classifications identified, and returns the sensitivity of the predictions. The function is based on the formula below. Based on the results of the functions, the precision of the predictions is `0.47`.

$$Sensitivity = \frac{TP}{TP+FN}$$

```{r sensitivity}
test_sensitivity <- function(df) {
    m <- classify_predictions(df)
    s <- (m$tp) / (m$tp + m$fn)
    return (s)
}
```

```{r check_sensitivity, echo=FALSE}
sprintf("The sensitivity of the predictions: %.3f", test_sensitivity(data))
```
<br></br>

## 7. Specificity
The test_specificity() function takes the data set as a dataframe, with actual and predicted classifications identified, and returns the specificity of the predictions. The function is based on the formula below. Based on the results of the functions, the precision of the predictions is `0.96`.

$$Specificity = \frac{TN}{TN+FP}$$

```{r specificity}
test_specificity <- function(df) {
    m <- classify_predictions(df)
    s <- (m$tn) / (m$tn + m$fp)
    return (s)
}
```

```{r check_specificity, echo=FALSE}
sprintf("The specificity of the predictions: %.3f", test_specificity(data))
```
<br></br>

## 8. F1 score
The test_f1_score() function takes the data set as a dataframe, with actual and predicted classifications identified, and returns the F1 score of the predictions. The function is based on the formula below. Based on the results of the functions, the precision of the predictions is `0.61`.

$$F1 \: Score = \frac{2 \cdot Precision \cdot Sensitivity}{Precision + Sensitivity}$$

```{r f1_score}
test_f1_score <- function(df) {
    precision <- test_precision(df)
    sensitivity <- test_sensitivity(df)

    score <- (2 * precision * sensitivity) / (precision + sensitivity)
    return(score)
}
```

```{r check_f1_score, echo=FALSE}
sprintf("The f1_score of the predictions: %.3f", test_f1_score(data))
```
<br></br>

## 9. Bounds on the F1 score

The F1 score is calculated from precision and sensitivity. They both have a range from 0 to 1. Based on the rule that if *0<a<1 and 0<b<1, then ab<a*: $$ If \; 0<Precision<1 \; and \; 0<Sensitivity<1, \\
then \; Precision \cdot Sensitivity<Precision \; \\
and \; Precision \cdot Sensitivity<Sensitivity, \\ 
then \; 2 \cdot Precision \cdot Sensitivity < Precision + Sensitivity$$

F1 score is the fraction with numerator *2•Precision•Sensitivity* and denominator *Precision + Sensitivity*, the numerator is less than denominator. Also, Precision and Sensitivity are both positive. The result of the fraction can not be greater than 1 and negative in this case. The F1 score can not be equal to 0, and if Precision is 0 or Sensitivity is 0, the F1 score is not defined. Thus, the only range for F1 score is $$0<F1 \; score<1$$


**Was here From Cliff from the beginning**
```{r call_functions, eval=FALSE}
## Verify that you get an accuracy and an error rate that sums to one.

print( accuracy(data) + classification_error_rate(data) == 1)
print( accuracy(data))
print( sensitivity(data))
print( specificity(data))
```

```{r}
xtab <- table(data$class, data$scored.class)
confusionMatrix(xtab)
```
<br></br>

# 10. ROC Curve

The test_roc() function takes the data set as a dataframe, with a true classification column and a probability column and returns a list that includes the plot of the ROC curve and the AUC.


```{r}

ret_lst <- test_roc(data$class, data$scored.probability)

# output plot
ret_lst[1]

# print the AUC
print(paste0('auc = ', round(as.numeric(ret_lst[2]),4)))


```
<br></br>

# 12. Using `caret` package
Our manually calculated metrics: <br></br>
 1. Sensitivity = `r round(test_sensitivity(data), 3)` <br></br>
 2. Specificity = `r round(test_specificity(data), 3)`
<br>
<br>
Using caret's confusionMatrix function:
```{r echo=FALSE}
confusionMatrix(as.factor(data$scored.class), as.factor(data$class))
```
Using caret's `sensitivity` function:
```{r echo=FALSE}
sensitivity(as.factor(data$scored.class), as.factor(data$class))
```
Using caret's `specificity` function:
```{r echo=FALSE}
specificity(as.factor(data$scored.class), as.factor(data$class))
```
Happily, as we can see, R's `caret` functions return the same values as our manually written functions.
<br></br>

# 13. pROC package

The auc() function in pROC package generates the same value as the manual process in questions 10. Furthermore the ROC curves have a similar profile.


```{r}

roc_obj <- roc(data$class, data$scored.probability)
auc(roc_obj)
plot(roc_obj, print.auc=TRUE)
axis(side=1, at=seq(0, 1, by=0.1))
axis(side=2, at=seq(0, 1, by=0.1))

```
