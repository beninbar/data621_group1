---
title: 'Homework #4:  Multiple linear regression and Binary logistic regression'
subtitle: 'Critical Thinking Group 1'
author: 'Ben Inbar, Cliff Lee, Daria Dubovskaia, David Simbandumwe, Jeff Parks'
output:
  html_document:
    toc: yes
    toc_float: yes
    theme: united
  pdf_document:
    toc: yes
editor_options:
  chunk_output_type: console
  markdown: 
    wrap: sentence
---

```{r setup, include=FALSE}
# chunks
knitr::opts_chunk$set(echo=FALSE, eval=TRUE, include=TRUE, 
message=FALSE, warning=FALSE, fig.height=5)

# libraries
library(yardstick)

library(mice)
library(readr)
library(skimr)
library(ggplot2)
library(tidyverse)
library(reshape2)
library(MASS)
library(forecast)
library(kableExtra)
library(ggpubr)
library(fastDummies)

## To load the below libraries you might have to do the following in the console first:
####  install.packages("devtools")
####  devtools::install_github("haleyjeppson/ggmosaic")
####  devtools::install_github("thomasp85/patchwork")

library(ggmosaic)
library(patchwork)

library(caTools)
library(corrplot)
library(Hmisc)
library(glmnet)
library(vip)
library(caret)

# ggplot
theme_set(theme_bw())
```

```{r common functions}
#' glmnet_cv_aicc
#'
#' @param fit
#' @param lambda
#'
#' @return
#' @export
#'
#' @examples
glmnet_cv_aicc <- function(fit, lambda = 'lambda.1se'){
  whlm <- which(fit$lambda == fit[[lambda]])
  with(fit$glmnet.fit,
       {
         tLL <- nulldev - nulldev * (1 - dev.ratio)[whlm]
         k <- df[whlm]
         n <- nobs
         return(list('AICc' = - tLL + 2 * k + 2 * k * (k + 1) / (n - k - 1),
                     'BIC' = log(n) * k - tLL))
       })
}
```

```{r data}
# read train and evaluation datasets
train_df <- read.csv("https://raw.githubusercontent.com/cliftonleesps/data621_group1/main/hw4/insurance_training_data.csv")
eval_df <- read.csv("https://raw.githubusercontent.com/cliftonleesps/data621_group1/main/hw4/insurance-evaluation-data.csv")
```

## Overview

In this homework assignment, we will explore, analyze and model a data set containing approximately 8000 records representing a customer at an auto insurance company.
We will build multiple linear regression models on the continuous variable `TARGET_AMT` and binary logistic regression model on the boolean variable `TARGET_FLAG` to predict the probability that a person will crash their car, and to predict the associated costs.

We are going to build several models using different techniques and variable selection.
In order to best assess our predictive models, we will create a validation set within our training data along an 80/20 training/testing proportion, before applying the finalized models to a separate evaluation dataset that does not contain the target.

## 1. Data Exploration

The insurance training dataset contains 8161 observations of 26 variables, each record represents a customer at an auto insurance company.
The evaluation dataset contains 2141 observations of 26 variables.
These include demographic measures such as age and gender, socioeconomic measures such as education and household income, and vehicle-specific metrics such as car model, age and assessed value.

<img src="https://raw.githubusercontent.com/cliftonleesps/data621_group1/main/hw4/hw4_variables.png" width="1200" style="display: block; margin-left: auto; margin-right: auto; width: 100%;"/>

Each record also has two response variables.
The first response variable, `TARGET_FLAG`, is a boolean where "1" means that the person was in a car crash.
The second response variable, `TARGET_AMT` is a numeric indicating the (positive) cost if a car crash occurred; this value is zero if the person did not crash their car.

We can explore a sample of the training data here, and make some initial observations:

-   Some of the variables are character though they should be numeric and vice-versa.
-   Some currency variables are strings with '\$' symbols instead of numerics.
-   Some character variables include a prefix `z_` that could be removed for readability.

```{r summary1}
DT::datatable(
      train_df[1:500,],
      extensions = c('Scroller'),
      options = list(scrollY = 350,
                     scrollX = 500,
                     deferRender = TRUE,
                     scroller = TRUE,
                     dom = 'lBfrtip',
                     fixedColumns = TRUE, 
                     searching = FALSE), 
      rownames = FALSE) 
```

### 1.1 Summary Statistics

```{r prep}
# Drop INDEX
train_df <- train_df %>% dplyr::select(-INDEX)

## Remove z_ from character class values
z_vars <- c("MSTATUS","SEX","JOB","CAR_TYPE","URBANICITY","EDUCATION")
for (v in z_vars) {
  train_df <- train_df %>% mutate(!!v := str_replace(get(v),"z_",""))
}

# Update RED_CAR, replace [no,yes] values with [No, Yes] values
train_df <- train_df %>% mutate( RED_CAR = ifelse(RED_CAR == "no","No","Yes"))

# Instead of level "" will be "Unknown"
train_df$JOB[train_df$JOB==""] <- "Unknown"

# Convert currency columns from character class to integer
dollar_vars <- c("INCOME","HOME_VAL","BLUEBOOK","OLDCLAIM")
for (v in dollar_vars) {
  train_df <- train_df %>% mutate(!!v := str_replace(get(v),"[\\$,]",""))
}

currency_vars <- c("INCOME","HOME_VAL","BLUEBOOK","OLDCLAIM")

for (v in currency_vars) {
  train_df <- train_df %>% mutate(!!v := parse_number(get(v)))
}
```

The table below provides valuable descriptive statistics about the training data:

```{r summary}
skim_without_charts(train_df)
```

/

Based on this summary table and exploration of the data, we can make the following observations:

-   14 variables are categorical, 12 are numeric.
-   There is no missing data for character variables.
-   Numeric variables with missing values include `YOJ` (6%), `INCOME` (5%), `HOME_VAL` (6%), `CAR_AGE` (6%), and `AGE` (1%).
-   Most of the numeric variables have a minimum of zero.
-   One variable, `CAR_AGE` has a negative value of -3, which doesn't make intuitive sense.

### 1.2 Distributions

Before building a model, we need to make sure that we have both classes equally represented in our `TARGET_FLAG` variable.
Class `1` takes 27% and class `0` takes 63% of the target variable.
As a result, we have unbalanced class distribution for our target variable that we have to deal with, we have to take some additional steps (bootstrapping, etc) before using logistic regression.

```{r balance}
prop.table(table(train_df$TARGET_FLAG)) %>% 
  kable(label = "Distribution of Target Flag", 
        caption = "Distribution of Target Flag", 
        col.names = c("Value", "%"), 
        digits = 2) %>% 
    kable_styling(full_width = F) 
```

Many of these distributions seem highly skewed and non-normal.
As part of our data preparation we'll use power transformations to find whether transforming variables to more normal distributions improves our models' efficacy.

```{r}
dist_df <- train_df %>% dplyr::select(where(is.numeric)) %>%
  pivot_longer(!c(TARGET_FLAG,TARGET_AMT), 
               names_to = 'variable', 
               values_to = 'value') %>% 
  drop_na()

dist_df %>% ggplot(aes(x=value, group=TARGET_FLAG, fill=TARGET_FLAG)) + 
  geom_density(color='#023020') + 
  facet_wrap(~variable, scales = 'free',  ncol = 4) + 
  theme_bw()
```

### 1.3 Box Plots

Commentary

```{r boxplot}
dist_df %>% ggplot(aes(x=value, group=TARGET_FLAG, fill=TARGET_FLAG)) + 
  geom_boxplot(color='#023020') + 
  facet_wrap(~variable, scales = 'free',  ncol = 4) + 
  theme_bw()
```

### 1.4 Scatter Plot

Interestingly, none of our predictors appear to have strong linear relationships to our `TARGET_AMT` response variable, which is a primary assumption of linear regression.
This suggests that alternative methods might be more successful in modeling the relationships.

```{r scatterplot}
dist_df %>% drop_na() %>% ggplot(aes(x=value, y=TARGET_AMT)) + 
  geom_smooth(method='glm', se=TRUE, na.rm=TRUE) +
  geom_point(color='#023020') + 
  facet_wrap(~variable, scales = 'free',  ncol = 4) + 
  theme_bw()
```

### 1.5 Correlation Matrix

Commentary

```{r corr}
rcore <- rcorr(as.matrix(train_df %>% dplyr::select(where(is.numeric))))
coeff <- rcore$r
corrplot(coeff, tl.cex = .5, tl.col="black", method = 'color', addCoef.col = "black",
         type="upper", order="hclust",
         diag=FALSE)
```

## 2. Data preparation

### 2.1 Data types

In order to work with our training dataset, we'll need to first convert some variables to more useful data types:

-   Convert currency columns from character to integer: `INCOME`,`HOME_VAL`,`BLUEBOOK` and `OLDCLAIM`.
-   Convert character columns to factors: `TARGET_FLAG`, `CAR_TYPE`, `CAR_USE`, `EDUCATION`, `JOB`, `MSTATUS`, `PARENT1`, `RED_CAR`, `REVOKED`, `SEX` and `URBANICITY`.

```{r}
# Other changes applied in Section 1.

# Convert these character variables to factors
factor_vars <- c("TARGET_FLAG","PARENT1","CAR_TYPE","JOB","CAR_USE",
                 "URBANICITY","RED_CAR","REVOKED","MSTATUS","EDUCATION","SEX")

for (v in factor_vars) {
  train_df <- train_df %>% mutate(!!v := factor(get(v)))
}

## print factor levels
# for (i in factor_vars) {
#   print(lapply(train_df[i], levels))
# }
```

### 2.3 Transformations and Missing Values

Before we go further, we need to identify and handle any missing, NA or negative data values so we can perform log transformations and regression.

First, we'll apply transformations to clean up and align formatting of our variables:

-   Drop the `INDEX` variable.
-   Remove "z\_" from all character class values.
-   Update RED_CAR, replace [no,yes] values with [No, Yes] values.
-   Replace `JOB` blank values with 'Unknown'.

Next, we'll manually adjust two special cases of missing or outlier values.

-   In cases where `YOJ` is zero and `INCOME` is NA, we'll set `INCOME` to zero to avoid imputing new values over legitimate instances of non-employment.
-   There is also at least one value of `CAR_AGE` that is less than zero - we'll assume this is a data collection error and set it to zero (representing a brand-new car.)

```{r}
## Check JOB values where YOJ==0 and INCOME is NA
# train_df %>% filter(YOJ == 0 & is.na(INCOME)) %>% dplyr::select(JOB) %>% unique()

# Manual correction of missing/outlier values
train_df <- train_df %>%
  mutate(CAR_AGE = ifelse(CAR_AGE < 0, 0, CAR_AGE)) %>% 
  mutate(INCOME = ifelse(YOJ == 0 & is.na(INCOME), 0, INCOME))
```

We'll use MICE to impute our remaining variables with missing values - `AGE`, `YOJ`, `CAR_AGE`, `INCOME` and `HOME_VALUE`:

-   We might reasonably assume that relationships exist between these variables (older, more years on the job may correlate with higher income and home value). Taking simple means or medians might suppress those features, but MICE should provide a better imputation.

```{r impute}
# MICE imputation of missing values
imputed_Data <- mice(train_df, m=5, maxit = 20, method = 'pmm', seed = 500, printFlag=FALSE)
train_df <- complete(imputed_Data)
```

Next we'll want to consider any power transformations for variables that have skewed distributions.
For example, our numeric response variable `TARGET_AMT` is a good candidate for transformation as its distribution is very highly skewed, and the assumption of normality is required in order to apply linear regression.

-   Log transformation will be applied to variables `INCOME`, `TARGET_AMT`, `OLDCLAIM` to transform their distributions from right-skewed to normally distributed.
-   Similarly, BoxCox transformation will be applied to variables `BLUEBOOK`, `TRAVTIME`, `TIF`, so they also are more normally distributed.

```{r boxcox, fig.keep="none"}
## Use basic log transformation on `TARGET_AMT`, `OLDCLAIM`, `INCOME`
train_df$TARGET_AMT <- log(train_df$TARGET_AMT + 0.01)
train_df$OLDCLAIM <- log(train_df$OLDCLAIM + 0.01)
#transformed_df$INCOME <-  log(transformed_df$transformed_df$INCOME + 0.01)

# use Box-Cox on `BLUEBOOK`, `TRAVTIME`,  `TIF`
bluebook_boxcox <- boxcox(lm(train_df$BLUEBOOK ~ 1))
bluebook_lambda <- bluebook_boxcox$x[which.max(bluebook_boxcox$y)]
bluebook_trans <- BoxCox(train_df$BLUEBOOK, bluebook_lambda)
train_df$BLUEBOOK <- bluebook_trans

travtime_boxcox <- boxcox(lm(train_df$TRAVTIME ~ 1))
travtime_lambda <- travtime_boxcox$x[which.max(travtime_boxcox$y)]
travtime_trans <- BoxCox(train_df$TRAVTIME, travtime_lambda)
train_df$TRAVTIME <- travtime_trans

tif_boxcox <- boxcox(lm(train_df$TIF ~ 1))
tif_lambda <- tif_boxcox$x[which.max(tif_boxcox$y)]
tif_trans <- BoxCox(train_df$TIF, tif_lambda)
train_df$TIF <- tif_trans
```

To give our models more variables to work with, we'll engineer some additional features:

-   Create bin values for `CAR_AGE`, `HOME_VAL` and `TIF`.
-   Create dummy variables for two-level factors, `MALE`, `MARRIED`, `LIC_REVOKED`, `CAR_RED`, `PRIVATE_USE`, `SINGLE_PARENT` and `URBAN`.

```{r}
## put into bins:  TIF; CAR_AGE; HOMEKIDS; HOME_VAL
q <- quantile(train_df$CAR_AGE)
q <- c(-1,  1,  8, 12, 28)
train_df <- train_df %>% mutate(CAR_AGE_BIN = cut(CAR_AGE, breaks=q, labels=FALSE))

q <- quantile(train_df$HOME_VAL)
q[1] <- -1
train_df <- train_df%>% mutate(HOME_VAL_BIN = cut(HOME_VAL, breaks=q, labels=FALSE))

q <- quantile(train_df$TIF); 
q[1] <- -1
train_df <- train_df %>% mutate(TIF_BIN = cut(TIF, breaks=q, labels=FALSE))
```

```{r dummy_vars}
## Create Dummy Variables for factors with two levels
dummy_vars <- function(df){
  df %>%
    mutate(
      MALE = factor(ifelse(SEX=="M", 1, 0)), 
      MARRIED = factor(ifelse(MSTATUS=="Yes", 1, 0)),
      LIC_REVOKED = factor(ifelse(REVOKED =="Yes", 1, 0)),
      CAR_RED = factor(ifelse(RED_CAR =="Yes", 1, 0)),
      PRIVATE_USE = factor(ifelse(CAR_USE =="Private", 1, 0)),
      SINGLE_PARENT = factor(ifelse(PARENT1 =="Yes", 1, 0)),
      URBAN = factor(ifelse(URBANICITY =="Highly Urban/ Urban", 1, 0))
    ) %>% 
    dplyr::select(-c(SEX, MSTATUS, REVOKED, RED_CAR, CAR_USE, PARENT1, URBANICITY))
      }

train_df <- dummy_vars(train_df)
```

We can examine our final, transformed training dataset and distributions below (with a temporary numeric variable `CAR_CRASH` to represent)

```{r}
skim_without_charts(train_df)
```

```{r}
## Create temporary CAR_CRASH variable from the target_flag for visualization
vis_df <- train_df %>% mutate(CAR_CRASH = ifelse(TARGET_FLAG == 1, 'Yes', 'No'))

a <- vis_df %>% 
  select_if(is.numeric) %>% 
  dplyr::select(!(c(TARGET_AMT))) %>%
  data.table::melt()

a %>% ggplot(aes(x=value)) + 
  geom_density(alpha=1, fill='#47A0F5') + 
  facet_wrap(~variable, scales='free')
```

### 2.4 Visualizations

We can use Mosaic Plots to illustrate the relationship of binary factor variables to `TARGET_FLAG`:

-   Observation
-   Observation

```{r}
## Mosaic plots to view any relationship to the TARGET_FLAG variable for binary factor variables
## PARENT1
plot_parent1 <- ggplot(data = vis_df) +
  geom_mosaic(aes(x = SINGLE_PARENT, fill=CAR_CRASH), offset = 0.005) +
  labs(y="", x="", title="Single Parent", margin=margin(b=10,unit="pt")) +
    theme(panel.background = element_blank(),
          axis.text.y = element_blank(),
          axis.ticks.y= element_blank(),
          axis.ticks.x= element_blank(),
          axis.text.x = element_text(angle = 0, margin = margin(t = -6, unit="mm")),
          plot.title = element_text(hjust = 0.5),
          axis.title.x = element_blank(),
          legend.position = "right")

## Sex
plot_sex <- ggplot(data = vis_df) +
  geom_mosaic(aes(x = MALE, fill=CAR_CRASH), offset = 0.005) +
  labs(y="", x="", title="Sex", margin=margin(b=10,unit="pt")) +
    theme(panel.background = element_blank(),
          axis.text.y = element_blank(),
          axis.ticks.y= element_blank(),
          axis.ticks.x= element_blank(),
          axis.text.x = element_text(angle = 0, margin = margin(t = -6, unit="mm")),
          plot.title = element_text(hjust = 0.5),
          axis.title.x = element_blank(),
          legend.position = "right")

## Marriage status
plot_mstatus <- ggplot(data = vis_df) +
  geom_mosaic(aes(x = MARRIED, fill=CAR_CRASH), offset = 0.005) +
  labs(y="", x="", title="Marriage Status", margin=margin(b=10,unit="pt")) +
    theme(panel.background = element_blank(),
          axis.text.y = element_blank(),
          axis.ticks.y= element_blank(),
          axis.ticks.x= element_blank(),
          axis.text.x = element_text(angle = 0, margin = margin(t = -6, unit="mm")),
          plot.title = element_text(hjust = 0.5),
          axis.title.x = element_blank(),
          legend.position = "right")

## Car use
plot_car_use <- ggplot(data = vis_df) +
  geom_mosaic(aes(x = PRIVATE_USE, fill=CAR_CRASH), offset = 0.005) +
  labs(y="", x="", title="Car Use", margin=margin(b=10,unit="pt")) +
    theme(panel.background = element_blank(),
          axis.text.y = element_blank(),
          axis.ticks.y= element_blank(),
          axis.ticks.x= element_blank(),
          axis.text.x = element_text(angle = 0, margin = margin(t = -6, unit="mm")),
          plot.title = element_text(hjust = 0.5),
          axis.title.x = element_blank(),
          legend.position = "right")

## Red car
plot_red_car <- ggplot(data = vis_df) +
  geom_mosaic(aes(x = CAR_RED, fill=CAR_CRASH), offset = 0.005) +
  labs(y="", x="", title="Red Car?", margin=margin(b=10,unit="pt")) +
    theme(panel.background = element_blank(),
          axis.text.y = element_blank(),
          axis.ticks.y= element_blank(),
          axis.ticks.x= element_blank(),
          axis.text.x = element_text(angle = 0, margin = margin(t = -6, unit="mm")),
          plot.title = element_text(hjust = 0.5),
          axis.title.x = element_blank(),
          legend.position = "right")

## license revoked
plot_revoked <- ggplot(data = vis_df) +
  geom_mosaic(aes(x = LIC_REVOKED, fill=CAR_CRASH), offset = 0.005) +
  labs(y="", x="", title="License Revoked (Past 7 Years)", margin=margin(b=10,unit="pt")) +
    theme(panel.background = element_blank(),
          axis.text.y = element_blank(),
          axis.ticks.y= element_blank(),
          axis.ticks.x= element_blank(),
          axis.text.x = element_text(angle = 0, margin = margin(t = -6, unit="mm")),
          plot.title = element_text(hjust = 0.5),
          axis.title.x = element_blank(),
          legend.position = "right")

## urban or rural area
plot_urbanicity <- ggplot(data = vis_df) +
  geom_mosaic(aes(x = URBAN, fill=CAR_CRASH), offset = 0.005) +
  labs(y="", x="", title="Home / Work Area", margin=margin(b=10,unit="pt")) +
    theme(panel.background = element_blank(),
          axis.text.y = element_blank(),
          axis.ticks.y= element_blank(),
          axis.ticks.x= element_blank(),
          axis.text.x = element_text(angle = 0, margin = margin(t = -6, unit="mm")),
          plot.title = element_text(hjust = 0.5),
          axis.title.x = element_blank(),
          legend.position = "right")

plot_parent1 + plot_sex + plot_mstatus + plot_car_use + plot_red_car + plot_revoked + plot_urbanicity + plot_layout(nrow = 4)

```

We can also use Mosaic Plots to illustrate the relationship of multi-level factor variables to `TARGET_FLAG`:

-   Observation
-   Observation

```{r}
## Mosaic plots to view any relationship to the TARGET_FLAG variable for multi-level factors 
plot_education <- ggplot(data = vis_df) +
  geom_mosaic(aes(x = EDUCATION, fill=CAR_CRASH), offset = 0.005) +
  labs(y="", x="", title="Education", margin=margin(b=10,unit="pt")) +
    theme(panel.background = element_blank(),
          axis.text.y = element_blank(),
          axis.ticks.y= element_blank(),
          axis.ticks.x= element_blank(),
          axis.text.x = element_text(angle = 90, margin = margin(t = -0.5, unit="pt")),
          plot.title = element_text(hjust = 0.5),
          axis.title.x = element_blank(),
          legend.position = "right")

plot_job <- ggplot(data = vis_df) +
  geom_mosaic(aes(x = JOB, fill=CAR_CRASH), offset = 0.005) +
  labs(y="", x="", title="Job Category", margin=margin(b=10,unit="pt")) +
    theme(panel.background = element_blank(),
          axis.text.y = element_blank(),
          axis.ticks.y= element_blank(),
          axis.ticks.x= element_blank(),
          axis.text.x = element_text(angle = 90, margin = margin(t = -0.5, unit="pt")),
          plot.title = element_text(hjust = 0.5),
          axis.title.x = element_blank(),
          legend.position = "right")

plot_car_type <- ggplot(data = vis_df) +
  geom_mosaic(aes(x = CAR_TYPE, fill=CAR_CRASH), offset = 0.005) +
  labs(y="", x="", title="Car Type", margin=margin(b=10,unit="pt")) +
    theme(panel.background = element_blank(),
          axis.text.y = element_blank(),
          axis.ticks.y= element_blank(),
          axis.ticks.x= element_blank(),
          axis.text.x = element_text(angle = 90, margin = margin(t = -0.5, unit="pt")),
          plot.title = element_text(hjust = 0.5),
          axis.title.x = element_blank(),
          legend.position = "right")

plot_education + plot_job + plot_car_type + plot_layout(nrow = 2)
```

## 2.x Training and Validation Sets

To proceed with modeling, we'll split our training data into train (80%) and validation (20%) datasets.

```{r}
set.seed(1233)

sample <- sample.split(train_df$TARGET_FLAG, SplitRatio = 0.8)
train_data  <- subset(train_df, sample == TRUE)
valid_data   <- subset(train_df, sample == FALSE)

```

## 3. Multiple linear regression

### 3.1 Model 1 - Lasso

```{r}

t_df <- train_data %>% dplyr::select(-c(TARGET_FLAG)) %>% drop_na()


# build X matrix and Y vector
X <- model.matrix(TARGET_AMT ~ ., data=t_df)[,-1]
Y <- t_df[,"TARGET_AMT"] 

```

```{r}


lasso.lm.model<- cv.glmnet(x=X,y=Y,
                       family = "gaussian",
                       #link = "logit",
                       standardize = TRUE,                       #standardize  
                       nfold = 5,
                       alpha=1)                                  #alpha=1 is lasso

l.min <- lasso.lm.model$lambda.min
l.1se <- lasso.lm.model$lambda.1se
coef(lasso.lm.model, s = "lambda.min" )
coef(lasso.lm.model, s = "lambda.1se" )
lasso.lm.model


```

```{r}

par(mfrow=c(2,2))

plot(lasso.lm.model)
plot(lasso.lm.model$glmnet.fit, xvar="lambda", label=TRUE)
plot(lasso.lm.model$glmnet.fit, xvar='dev', label=TRUE)

#rocs <- roc.glmnet(lasso.model, newx = X, newy = Y )
#plot(rocs,type="l")  

```

```{r}
assess.glmnet(lasso.lm.model,           
              newx = X,              
              newy = Y,
              family = 'gaussian')    

print(glmnet_cv_aicc(lasso.lm.model, 'lambda.min'))
print(glmnet_cv_aicc(lasso.lm.model, 'lambda.1se'))

```

```{r}
coef(lasso.lm.model, s = "lambda.min" )
vip(lasso.lm.model, num_features=20 ,geom = "col", include_type=TRUE, lambda = "lambda.min")
```

#### Model Performance

```{r}

# 
t0_df <- valid_data %>% dplyr::select(-c(TARGET_FLAG)) %>% drop_na()


# build X matrix and Y vector
X_test <- model.matrix(TARGET_AMT ~ ., data=t0_df)[,-1]
Y_test <- t0_df[,"TARGET_AMT"] 

# predict using coefficients at lambda.min
lassoPred <- predict(lasso.lm.model, newx = X_test, type = "response", s = 'lambda.min')

pred_ln_df <- valid_data %>% drop_na()
pred_ln_df$TARGET_AMT_PRED <- lassoPred[,1]
#pred_df$target_pred <- ifelse(lassoPred > 0.5, 1, 0)[,1]

```

```{r}

n <- nrow(t0_df)
x <- t0_df$TARGET_AMT
e <- lassoPred - t0_df$TARGET_AMT

plot(x, e,  
     xlab = "cost", 
     ylab = "residuals", 
     bg = "steelblue", 
     col = "darkgray", cex = 1.5, pch = 21, frame = FALSE)
abline(h = 0, lwd = 2)
for (i in 1 : n) 
  lines(c(x[i], x[i]), c(e[i], 0), col = "red" , lwd = 1)

```

```{r}

multi_metric <- metric_set(mape, smape, mase, mpe, rmse)
pred_ln_df %>% multi_metric(truth=TARGET_AMT, estimate=TARGET_AMT_PRED)

```

### 3.2 Model 2 -

### 3.3 Model 3 -

### 3.4 Model selection

## 4. Binary logistic regression

### 4.1 Model 1 - Lasso

```{r}

t_df <- train_data %>% dplyr::select(-c(TARGET_AMT)) %>% drop_na()


# build X matrix and Y vector
X <- model.matrix(TARGET_FLAG ~ ., data=t_df)[,-1]
Y <- t_df[,"TARGET_FLAG"] 

```

```{r}


lasso.log.model<- cv.glmnet(x=X,y=Y,
                       family = "binomial",
                       link = "logit",
                       standardize = TRUE,                       #standardize  
                       nfold = 5,
                       alpha=1)                                  #alpha=1 is lasso

l.min <- lasso.log.model$lambda.min
l.1se <- lasso.log.model$lambda.1se
coef(lasso.log.model, s = "lambda.min" )
coef(lasso.log.model, s = "lambda.1se" )
lasso.log.model


```

```{r}

par(mfrow=c(2,2))

plot(lasso.log.model)
plot(lasso.log.model$glmnet.fit, xvar="lambda", label=TRUE)
plot(lasso.log.model$glmnet.fit, xvar='dev', label=TRUE)

rocs <- roc.glmnet(lasso.log.model, newx = X, newy = Y )
plot(rocs,type="l")  

```

```{r}
assess.glmnet(lasso.log.model,           
                newx = X,              
                newy = Y,
                family = "binomial",
                s = 'lambda.min'
              )    

print(glmnet_cv_aicc(lasso.log.model, 'lambda.min'))
print(glmnet_cv_aicc(lasso.log.model, 'lambda.1se'))

```

```{r}
coef(lasso.log.model, s = "lambda.min" )
vip(lasso.log.model, num_features=20 ,geom = "col", include_type=TRUE, lambda = "lambda.min")
```

#### Model Performance

```{r}

# 
t0_df <- valid_data %>% dplyr::select(-c(TARGET_AMT)) %>% drop_na()


# build X matrix and Y vector
X_test <- model.matrix(TARGET_FLAG ~ ., data=t0_df)[,-1]
Y_test <- t0_df[,"TARGET_FLAG"] 

# predict using coefficients at lambda.min
lassoPred <- predict(lasso.log.model, newx = X_test, type = "response", s = 'lambda.min')

pred_log_df <- valid_data %>% drop_na()
pred_log_df$TARGET_FLAG_PROB <- lassoPred[,1]
pred_log_df$TARGET_FLAG_PRED <- ifelse(lassoPred > 0.5, 1, 0)[,1]

```

```{r}

confusion.glmnet(lasso.log.model, newx = X_test, newy = Y_test, s = 'lambda.min')

```

#### Checking Model Assumptions

```{r}

pred_log_df <- pred_log_df %>%
  mutate(logit = log(TARGET_FLAG_PROB/(1-TARGET_FLAG_PROB))) 

m_df <- pred_log_df %>% dplyr::select(where(is.numeric)) %>% pivot_longer(!c(TARGET_AMT,TARGET_FLAG_PROB,TARGET_FLAG_PRED,logit), 
                                 names_to='variable' , values_to = 'value')

#Scatter plot
ggplot(m_df, aes(logit, value))+
  geom_point(size = 0.5, alpha = 0.5) +
  geom_smooth(method = "loess") + 
  theme_bw() + 
  facet_wrap(~variable, scales = "free_y")

```

The lasso regression solve multicollinearity issue by selecting the variable with the largest coefficient while setting the rest to (nearly) zero.

```{r}

pred_log_df$index <- as.numeric(rownames(pred_log_df))
pred_log_df$resid <- pred_log_df$TARGET_FLAG_PROB - pred_log_df$TARGET_FLAG


ggplot(pred_log_df, aes(index, resid)) + 
  geom_point(aes(color = TARGET_FLAG), alpha = .5) +
  theme_bw()

```

### 4.2 Model 2 -

### 4.3 Model 3 -

### 4.4 Model selection

```{r}


results_lm_tbl <- tibble(
                      Model = character(),
                      mape = numeric(), 
                      smape = numeric(), 
                      mase = numeric(), 
                      mpe = numeric(), 
                      rmse = numeric(),
                      AIC = numeric()
                )

results_log_tbl <- tibble(
                      Model = character(),
                      Accuracy = numeric(), 
                      "Classification error rate" = numeric(),
                      F1 = numeric(),
                      Deviance= numeric(), 
                      R2 = numeric(),
                      Sensitivity = numeric(),
                      Specificity = numeric(),
                      Precision = numeric(),
                      AIC= numeric()
                )

```

\*\* Model 3.1: Lasso Linear Regression \*\*

```{r}

# build X matrix and Y vector
t0_df <- valid_data %>% dplyr::select(-c(TARGET_FLAG)) %>% drop_na()
X_test <- model.matrix(TARGET_AMT ~ ., data=t0_df)[,-1]
Y_test <- t0_df[,"TARGET_AMT"] 



lasso.r1 <- assess.glmnet(lasso.lm.model,
                                newx = X_test,
                                newy = Y_test )
lasso.r2 <- glmnet_cv_aicc(lasso.lm.model, 'lambda.min')


multi_metric <- metric_set(mape, smape, mase, mpe, rmse)
m_df <- pred_ln_df %>% multi_metric(truth=TARGET_AMT, estimate=TARGET_AMT_PRED)



results_lm_tbl <- results_lm_tbl %>% add_row(tibble_row(
                      Model = "M4.1:Lasso Linear",
                      mape = m_df[[1,3]],
                      smape = m_df[[2,3]],
                      mase = m_df[[3,3]],
                      mpe = m_df[[4,3]],
                      rmse = m_df[[5,3]],
                      AIC = lasso.r2$AICc)
                     )


```

\*\* Model 4.1: Lasso Logistic Regression \*\*

```{r}

conf_matrix_1 = confusionMatrix(factor(pred_log_df$TARGET_FLAG_PRED),factor(pred_log_df$TARGET_FLAG), "1")
conf_matrix_1$byClass["F1"]



# build X matrix and Y vector
t0_df <- valid_data %>% dplyr::select(-c(TARGET_AMT)) %>% drop_na()
X_test <- model.matrix(TARGET_FLAG ~ ., data=t0_df)[,-1]
Y_test <- t0_df[,"TARGET_FLAG"] 


lasso.r1 <- assess.glmnet(lasso.log.model,           
                                newx = X_test,              
                                newy = Y_test,
                                family = "binomial",
                                s = 'lambda.min'
                          )   
lasso.r2 <- glmnet_cv_aicc(lasso.log.model, 'lambda.min')



results_log_tbl <- results_log_tbl %>% add_row(tibble_row(
                      Model = "M4.1:Lasso Logistic", 
                      Accuracy=lasso.r1$auc[1], 
                      "Classification error rate" = 1 - lasso.r1$auc[1],
                      F1 = conf_matrix_1$byClass["F1"],
                      Deviance=lasso.r1$deviance[[1]], 
                      R2 = NA,
                      Sensitivity = conf_matrix_1$byClass["Sensitivity"],
                      Specificity = conf_matrix_1$byClass["Specificity"],
                      Precision = conf_matrix_1$byClass["Precision"],
                      AIC= lasso.r2$AICc)
                     )



```

Regression

```{r}

#t1 <- as_tibble(cbind(variables = names(results_tbl), t(results_tbl)))

kable(results_lm_tbl, digits=4) %>% 
  kable_styling(bootstrap_options = "basic", position = "center")

```

Logistic

```{r}


#t1 <- as_tibble(cbind(variables = names(results_tbl), t(results_tbl)))

kable(results_log_tbl, digits=4) %>% 
  kable_styling(bootstrap_options = "basic", position = "center")

```

## 5. Predictions

## 6. Conclusion

## 7. References

## Appendix: R code
