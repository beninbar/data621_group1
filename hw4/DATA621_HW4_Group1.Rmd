---
title: 'Homework #4:  Multiple linear regression and Binary logistic regression'
subtitle: 'Critical Thinking Group 1'
author: 'Ben Inbar, Cliff Lee, Daria Dubovskaia, David Simbandumwe, Jeff Parks'
output:
  html_document:
    toc: yes
    toc_float: yes
    theme: united
  pdf_document:
    toc: yes
editor_options:
  chunk_output_type: console
  markdown: 
    wrap: sentence
---


```{r setup, include=FALSE}

library(yardstick)

library(mice)
library(readr)
library(skimr)
library(ggplot2)
library(tidyverse)
library(reshape2)
library(MASS)
library(forecast)
library(kableExtra)
library(ggpubr)
library(fastDummies)

## To load the below libraries you might have to do the following in the console first:
####  install.packages("devtools")
####  devtools::install_github("haleyjeppson/ggmosaic")
####  devtools::install_github("thomasp85/patchwork")

library(ggmosaic)
library(patchwork)

library(caTools)
library(corrplot)
library(Hmisc)
library(glmnet)
library(vip)
library(caret)



knitr::opts_chunk$set(echo = FALSE)
```


```{r common functions}

#' glmnet_cv_aicc
#'
#' @param fit
#' @param lambda
#'
#' @return
#' @export
#'
#' @examples
glmnet_cv_aicc <- function(fit, lambda = 'lambda.1se'){
  whlm <- which(fit$lambda == fit[[lambda]])
  with(fit$glmnet.fit,
       {
         tLL <- nulldev - nulldev * (1 - dev.ratio)[whlm]
         k <- df[whlm]
         n <- nobs
         return(list('AICc' = - tLL + 2 * k + 2 * k * (k + 1) / (n - k - 1),
                     'BIC' = log(n) * k - tLL))
       })
}


```


```{r data}
# read train and evaluation datasets
train_df_orig <- read.csv("https://raw.githubusercontent.com/cliftonleesps/data621_group1/main/hw4/insurance_training_data.csv")
eval_df <- read.csv("https://raw.githubusercontent.com/cliftonleesps/data621_group1/main/hw4/insurance-evaluation-data.csv")

```



Clean up data
```{r}
train_df <- train_df_orig %>%  dplyr::select(-INDEX)


## Remove z_ from character class values
z_vars <- c("MSTATUS","SEX","JOB","CAR_TYPE","URBANICITY","EDUCATION")
for (v in z_vars) {
  train_df <- train_df %>% mutate(!!v := str_replace(get(v),"z_",""))
}


# Update RED_CAR, replace [no,yes] values with [No, Yes] values
train_df <- train_df %>% mutate( RED_CAR = ifelse(RED_CAR == "no","No","Yes"))

# Instead of level "" will be "Unknown"
train_df$JOB[train_df$JOB==""] <- "Unknown"


# Convert currency columns from character class to integer
dollar_vars <- c("INCOME","HOME_VAL","BLUEBOOK","OLDCLAIM")
for (v in dollar_vars) {
  train_df <- train_df %>% mutate(!!v := str_replace(get(v),"[\\$,]",""))
}

currency_vars <- c("INCOME","HOME_VAL","BLUEBOOK","OLDCLAIM")

for (v in currency_vars) {
  train_df <- train_df %>% mutate(!!v := parse_number(get(v)))
}

```



## Overview

In this homework assignment, we will explore, analyze and model a data set containing approximately 8000 records representing a customer at an auto insurance company. We will build multiple linear regression on the continuous variable `TARGET_AMT` and binary logistic regression on the `TARGET_FLAG` using the training data to predict the probability that a person will crash their car and also the amount of money it will cost if the person does crash their car. 

We are going to build several models using different techniques and variable selection. In order to best assess our predictive model, we created a test set within our training data, and split it along an 80/20 training/testing proportion, before applying the finalized models to a separate evaluation dataset that did not contain the target.


## 1. Data Exploration

The insurance training dataset contains 8161 observations of 26 variables, each record represents a customer at an auto insurance company. The evaluation dataset contains 2141 observations of 26 variables.
The descriptions of each column are below.

Each record has two response variables. The first response variable, `TARGET_FLAG`, is a 1 or a 0. A “1” means that the person was in a car crash. A zero means that the person was not in a car crash. The second response variable is `TARGET_AMT`. This value is zero if the person did not crash their car. But if they did crash their car, this number will be a value greater than zero.

<img src="https://raw.githubusercontent.com/cliftonleesps/data621_group1/main/hw4/hw4_variables.png" width="1200" style="display: block; margin-left: auto; margin-right: auto; width: 100%;"/>

### 1.1 Summary Statistics

The training data can be previewed below. The `TARGET_FLAG` column is the binary dependent variable denoting if a car was in a crash (target = 1) or not (target = 0).
`TARGET_AMT` is a numeric dependent variable and represents the amount of time the car spent on repairs in case of crash. The minimum is 0 (car wasn't in crash, no time spent on repairs), the maximum is 107586.1. 
```{r summary1}
DT::datatable(
      train_df[1:500,],
      extensions = c('Scroller'),
      options = list(scrollY = 350,
                     scrollX = 500,
                     deferRender = TRUE,
                     scroller = TRUE,
                     dom = 'lBfrtip',
                     fixedColumns = TRUE, 
                     searching = FALSE), 
      rownames = FALSE) 
```

The table below provides valuable descriptive statistics about the training data. 14 variables are categorical, 12 variables are numeric.
There is no missing data for categorical variables while numeric variables `YOJ` (years on job) has 5% of missing data, `CAR_AGE` (vehicle age) has 6%, and `AGE` (age of driver) has less than 1%. 
Most of the numeric variables have a minimum of zero. Some numbers seem strange, we should deal with it later. For example, `CAR_AGE` has the minimum value of -3.
Some of the variables are character though they should be numeric and vice versa. Variable `OLDCLAIM`, `BLUEBOOK`, `HOME_VAL`, `INCOME` have $ sign in front a number, we should remove the sign and transform the variable to numeric. Variables `MSTATUS` (Marital Status), `EDUCATION`, `JOB`, `CAR_TYPE`, `SEX` AND `URBANICITY` contain prefix `z_` that should be removed as well.
**ADD SOMETHING ELSE FOR SUMMARY**

```{r summary}
skim_without_charts(train_df)
```


### 1.2 Distributions

Before building a model, we need to make sure that we have both classes equally presented in our `TARGET_FLAG` variable. Class `1` takes 27% and class `0` takes 63% of the target variable. As a result, we have unbalanced class distribution for our target variable that we have to deal with, we have to take some additional steps (bootstrapping, etc) before using logistic regression.
```{r balance}
prop.table(table(train_df$TARGET_FLAG)) %>% 
  kable(label = "Distribution of Target Flag", 
        caption = "Distribution of Target Flag", 
        col.names = c("Value", "%"), 
        digits = 2) %>% 
    kable_styling(full_width = F) 
```

The distribution of the second target variable `TARGET_AMT` is right skewed, we will also transform the variable to make it follow the normal distribution (log/BoxCox).
```{r}

dist_df <- train_df %>% dplyr::select(where(is.numeric)) %>%
  pivot_longer(!c(TARGET_FLAG,TARGET_AMT), names_to='variable' , values_to = 'value') %>% 
  drop_na()

dist_df %>% ggplot(aes(x=value, group=TARGET_FLAG, fill=TARGET_FLAG)) + 
  geom_density(color='#023020') + 
  facet_wrap(~variable, scales = 'free',  ncol = 4) + 
  theme_bw()

```


### 1.3 Box Plots

```{r}

dist_df %>% ggplot(aes(x=value, group=TARGET_FLAG, fill=TARGET_FLAG)) + 
  geom_boxplot(color='#023020') + 
  facet_wrap(~variable, scales = 'free',  ncol = 4) + 
  theme_bw()

```


### 1.4 Scatter Plot

```{r}

dist_df %>% drop_na() %>% ggplot(aes(x=value, y=TARGET_AMT)) + 
  geom_smooth(method='glm', se=TRUE, na.rm=TRUE) +
  geom_point(color='#023020') + 
  facet_wrap(~variable, scales = 'free',  ncol = 4) + 
  theme_bw()

```


### 1.5 Correlation Matrix

```{r corr}

rcore <- rcorr(as.matrix(train_df %>% dplyr::select(where(is.numeric))))
coeff <- rcore$r
corrplot(coeff, tl.cex = .5, tl.col="black", method = 'color', addCoef.col = "black",
         type="upper", order="hclust",
         diag=FALSE)

```


## 2. Data preparation

### 2.1 Data types

First, we will remove prefixes `z_` and `$` together with the `INDEX` variable (identification Variable).

Transform to factor variables `TARGET_FLAG`, `CAR_TYPE`, `CAR_USE`, `EDUCATION`, `JOB`, `MSTATUS`, `PARENT1`, `RED_CAR`, `REVOKED`, `SEX` and `URBANICITY`.

Transform to numeric variables `INCOME`, `HOME_VAL`, `BLUEBOOK`, `OLDCLAIM`.

As we see below,  there are no unwanted characters in the factor variables. The `JOB` variable contains empty level `""`, it was substituted with `"Unknown"`. `RED_CAR` levels will be "Yes/No" instead of "yes/no".


```{r}
## Convert the below character variables to factors
factor_vars <- c("TARGET_FLAG","PARENT1","CAR_TYPE","JOB","CAR_USE",
                 "URBANICITY","RED_CAR","REVOKED","MSTATUS","EDUCATION","SEX")

for (v in factor_vars) {
  train_df <- train_df %>% mutate(!!v := factor(get(v)))
}
```


```{r}
# print factor levels
for (i in factor_vars) {
  print(lapply(train_df[i], levels))
}

```

### 2.3 Transformations and Missing Values

First, we'll manually adjust two special cases of missing or outlier values. 

In cases where `YOJ` is zero and `INCOME` is NA, we'll set `INCOME` to zero to avoid imputing new values over legitimate instances of non-employment.

```{r}
# Check JOB values where YOJ==0 and INCOME is NA
train_df %>% filter(YOJ == 0 & is.na(INCOME)) %>% dplyr::select(JOB) %>% unique()
```

There is also at least one value of `CAR_AGE` that is less than zero - we'll assume this is a data collection error and set it to zero (representing a brand-new car.) 

```{r}
# Manual correction of missing/outlier values
train_df <- train_df %>%
  mutate(CAR_AGE = ifelse(CAR_AGE < 0, 0, CAR_AGE)) %>% 
  mutate(INCOME = ifelse(YOJ == 0 & is.na(INCOME), 0, INCOME))
```

We'll use MICE to impute values for our remaining variables with missing values - `AGE`, `YOJ`, `CAR_AGE`, `INCOME` and `HOME_VALUE`.  We might reasonably assume there are relationships between them (older, more years on the job may correlate with higher income and home value).  Taking simple means or medians might suppress those features, but MICE should provide a better imputation.

```{r impute}
# MICE imputation of missing values
imputed_Data <- mice(train_df, m=5, maxit = 20, method = 'pmm', seed = 500, printFlag=FALSE)
train_df <- complete(imputed_Data)

# Add step to show before/after on imputed variables?
```



Log transformation will be applied to variables `INCOME`, `TARGET_AMT`, `OLDCLAIM` to transform their distributions from right-skewed to the normal.

BoxCox transformation will be applied to variables `BLUEBOOK`, `TRAVTIME`, `TIF`, so they follow the normal distribution.


```{r boxcox, fig.keep="none"}
## Use basic log transformation on `TARGET_AMT`, `OLDCLAIM`, `INCOME`
train_df$TARGET_AMT <- log(train_df$TARGET_AMT + 0.01)
train_df$OLDCLAIM <- log(train_df$OLDCLAIM + 0.01)
#transformed_df$INCOME <-  log(transformed_df$transformed_df$INCOME + 0.01)

# Issue -- -Inf values for log transforms on 0

# use Box-Cox on `BLUEBOOK`, `TRAVTIME`,  `TIF`
bluebook_boxcox <- boxcox(lm(train_df$BLUEBOOK ~ 1))
bluebook_lambda <- bluebook_boxcox$x[which.max(bluebook_boxcox$y)]
bluebook_trans <- BoxCox(train_df$BLUEBOOK, bluebook_lambda)
train_df$BLUEBOOK <- bluebook_trans

travtime_boxcox <- boxcox(lm(train_df$TRAVTIME ~ 1))
travtime_lambda <- travtime_boxcox$x[which.max(travtime_boxcox$y)]
travtime_trans <- BoxCox(train_df$TRAVTIME, travtime_lambda)
train_df$TRAVTIME <- travtime_trans

tif_boxcox <- boxcox(lm(train_df$TIF ~ 1))
tif_lambda <- tif_boxcox$x[which.max(tif_boxcox$y)]
tif_trans <- BoxCox(train_df$TIF, tif_lambda)
train_df$TIF <- tif_trans
```

Binning values for `CAR_AGE`, `HOME_VAL` and `TIF`:

```{r}
## put into bins:  TIF; CAR_AGE; HOMEKIDS; HOME_VAL
q <- quantile(train_df$CAR_AGE)
q <- c(-1,  1,  8, 12, 28)
train_df <- train_df %>% mutate(CAR_AGE_BIN = cut(CAR_AGE, breaks=q, labels=FALSE))

q <- quantile(train_df$HOME_VAL)
q[1] <- -1
train_df <- train_df%>% mutate(HOME_VAL_BIN = cut(HOME_VAL, breaks=q, labels=FALSE))

q <- quantile(train_df$TIF); 
q[1] <- -1
train_df <- train_df %>% mutate(TIF_BIN = cut(TIF, breaks=q, labels=FALSE))

#print(summary(transformed_df))

```

Creating dummy variables for factors with two levels:

```{r dummy_vars}
## Create Dummy Variables for factors with two levels
dummy_vars <- function(df){
  df %>%
    mutate(
      MALE = factor(ifelse(SEX=="M", 1, 0)), 
      MARRIED = factor(ifelse(MSTATUS=="Yes", 1, 0)),
      LIC_REVOKED = factor(ifelse(REVOKED =="Yes", 1, 0)),
      CAR_RED = factor(ifelse(RED_CAR =="Yes", 1, 0)),
      PRIVATE_USE = factor(ifelse(CAR_USE =="Private", 1, 0)),
      SINGLE_PARENT = factor(ifelse(PARENT1 =="Yes", 1, 0)),
      URBAN = factor(ifelse(URBANICITY =="Highly Urban/ Urban", 1, 0))
    ) %>% 
    dplyr::select(-c(SEX, MSTATUS, REVOKED, RED_CAR, CAR_USE, PARENT1,URBANICITY))
      }

train_df <- dummy_vars(train_df)
```


### 2.4 Visualizations

```{r visualizations}

## Create temporary CAR_CRASH variable from the target_flag for visualization
vis_df <- train_df %>% mutate(CAR_CRASH = ifelse(TARGET_FLAG == 1, 'Yes', 'No'))

colnames(vis_df)

a <- vis_df %>% select_if(is.numeric) %>% dplyr::select(!(c(TARGET_AMT))) %>% data.table::melt()
a %>% ggplot(aes(x=value))+ geom_density(alpha=.2,fill="#FF6666") + facet_wrap(~variable, scales='free')
```

```{r}
## Mosaic plots to view any relationship to the TARGET_FLAG variable for binary factor variables
## PARENT1
plot_parent1 <- ggplot(data = vis_df) +
  geom_mosaic(aes(x = SINGLE_PARENT, fill=CAR_CRASH), offset = 0.005) +
  labs(y="", x="", title="Single Parent", margin=margin(b=10,unit="pt")) +
    theme(panel.background = element_blank(),
          axis.text.y = element_blank(),
          axis.ticks.y= element_blank(),
          axis.ticks.x= element_blank(),
          axis.text.x = element_text(angle = 0, margin = margin(t = -6, unit="mm")),
          plot.title = element_text(hjust = 0.5),
          axis.title.x = element_blank(),
          legend.position = "right")

## Sex
plot_sex <- ggplot(data = vis_df) +
  geom_mosaic(aes(x = MALE, fill=CAR_CRASH), offset = 0.005) +
  labs(y="", x="", title="Sex", margin=margin(b=10,unit="pt")) +
    theme(panel.background = element_blank(),
          axis.text.y = element_blank(),
          axis.ticks.y= element_blank(),
          axis.ticks.x= element_blank(),
          axis.text.x = element_text(angle = 0, margin = margin(t = -6, unit="mm")),
          plot.title = element_text(hjust = 0.5),
          axis.title.x = element_blank(),
          legend.position = "right")

## Marriage status
plot_mstatus <- ggplot(data = vis_df) +
  geom_mosaic(aes(x = MARRIED, fill=CAR_CRASH), offset = 0.005) +
  labs(y="", x="", title="Marriage Status", margin=margin(b=10,unit="pt")) +
    theme(panel.background = element_blank(),
          axis.text.y = element_blank(),
          axis.ticks.y= element_blank(),
          axis.ticks.x= element_blank(),
          axis.text.x = element_text(angle = 0, margin = margin(t = -6, unit="mm")),
          plot.title = element_text(hjust = 0.5),
          axis.title.x = element_blank(),
          legend.position = "right")

## Car use
plot_car_use <- ggplot(data = vis_df) +
  geom_mosaic(aes(x = PRIVATE_USE, fill=CAR_CRASH), offset = 0.005) +
  labs(y="", x="", title="Car Use", margin=margin(b=10,unit="pt")) +
    theme(panel.background = element_blank(),
          axis.text.y = element_blank(),
          axis.ticks.y= element_blank(),
          axis.ticks.x= element_blank(),
          axis.text.x = element_text(angle = 0, margin = margin(t = -6, unit="mm")),
          plot.title = element_text(hjust = 0.5),
          axis.title.x = element_blank(),
          legend.position = "right")

## Red car
plot_red_car <- ggplot(data = vis_df) +
  geom_mosaic(aes(x = CAR_RED, fill=CAR_CRASH), offset = 0.005) +
  labs(y="", x="", title="Red Car?", margin=margin(b=10,unit="pt")) +
    theme(panel.background = element_blank(),
          axis.text.y = element_blank(),
          axis.ticks.y= element_blank(),
          axis.ticks.x= element_blank(),
          axis.text.x = element_text(angle = 0, margin = margin(t = -6, unit="mm")),
          plot.title = element_text(hjust = 0.5),
          axis.title.x = element_blank(),
          legend.position = "right")

## license revoked
plot_revoked <- ggplot(data = vis_df) +
  geom_mosaic(aes(x = LIC_REVOKED, fill=CAR_CRASH), offset = 0.005) +
  labs(y="", x="", title="License Revoked (Past 7 Years)", margin=margin(b=10,unit="pt")) +
    theme(panel.background = element_blank(),
          axis.text.y = element_blank(),
          axis.ticks.y= element_blank(),
          axis.ticks.x= element_blank(),
          axis.text.x = element_text(angle = 0, margin = margin(t = -6, unit="mm")),
          plot.title = element_text(hjust = 0.5),
          axis.title.x = element_blank(),
          legend.position = "right")

## urban or rural area
plot_urbanicity <- ggplot(data = vis_df) +
  geom_mosaic(aes(x = URBAN, fill=CAR_CRASH), offset = 0.005) +
  labs(y="", x="", title="Home / Work Area", margin=margin(b=10,unit="pt")) +
    theme(panel.background = element_blank(),
          axis.text.y = element_blank(),
          axis.ticks.y= element_blank(),
          axis.ticks.x= element_blank(),
          axis.text.x = element_text(angle = 0, margin = margin(t = -6, unit="mm")),
          plot.title = element_text(hjust = 0.5),
          axis.title.x = element_blank(),
          legend.position = "right")

plot_parent1 + plot_sex + plot_mstatus + plot_car_use + plot_red_car + plot_revoked + plot_urbanicity + plot_layout(nrow = 4)

```

```{r}
## Mosaic plots to view any relationship to the TARGET_FLAG variable for multi-level factors 
plot_education <- ggplot(data = vis_df) +
  geom_mosaic(aes(x = EDUCATION, fill=CAR_CRASH), offset = 0.005) +
  labs(y="", x="", title="Education", margin=margin(b=10,unit="pt")) +
    theme(panel.background = element_blank(),
          axis.text.y = element_blank(),
          axis.ticks.y= element_blank(),
          axis.ticks.x= element_blank(),
          axis.text.x = element_text(angle = 90, margin = margin(t = -0.5, unit="pt")),
          plot.title = element_text(hjust = 0.5),
          axis.title.x = element_blank(),
          legend.position = "right")

plot_job <- ggplot(data = vis_df) +
  geom_mosaic(aes(x = JOB, fill=CAR_CRASH), offset = 0.005) +
  labs(y="", x="", title="Job Category", margin=margin(b=10,unit="pt")) +
    theme(panel.background = element_blank(),
          axis.text.y = element_blank(),
          axis.ticks.y= element_blank(),
          axis.ticks.x= element_blank(),
          axis.text.x = element_text(angle = 90, margin = margin(t = -0.5, unit="pt")),
          plot.title = element_text(hjust = 0.5),
          axis.title.x = element_blank(),
          legend.position = "right")

plot_car_type <- ggplot(data = vis_df) +
  geom_mosaic(aes(x = CAR_TYPE, fill=CAR_CRASH), offset = 0.005) +
  labs(y="", x="", title="Car Type", margin=margin(b=10,unit="pt")) +
    theme(panel.background = element_blank(),
          axis.text.y = element_blank(),
          axis.ticks.y= element_blank(),
          axis.ticks.x= element_blank(),
          axis.text.x = element_text(angle = 90, margin = margin(t = -0.5, unit="pt")),
          plot.title = element_text(hjust = 0.5),
          axis.title.x = element_blank(),
          legend.position = "right")

plot_education + plot_job + plot_car_type + plot_layout(nrow = 2)
```


## 2.x Training and Validation Sets

We'll split our traning data into train (80%) and validaton (20%) datasets.

```{r}
set.seed(1233)

sample <- sample.split(train_df$TARGET_FLAG, SplitRatio = 0.8)
train_data  <- subset(train_df, sample == TRUE)
valid_data   <- subset(train_df, sample == FALSE)

```


## 3. Multiple linear regression


### 3.1 Model 1 - Lasso

```{r}

t_df <- train_data %>% dplyr::select(-c(TARGET_FLAG)) %>% drop_na()


# build X matrix and Y vector
X <- model.matrix(TARGET_AMT ~ ., data=t_df)[,-1]
Y <- t_df[,"TARGET_AMT"] 

```



```{r}


lasso.lm.model<- cv.glmnet(x=X,y=Y,
                       family = "gaussian",
                       #link = "logit",
                       standardize = TRUE,                       #standardize  
                       nfold = 5,
                       alpha=1)                                  #alpha=1 is lasso

l.min <- lasso.lm.model$lambda.min
l.1se <- lasso.lm.model$lambda.1se
coef(lasso.lm.model, s = "lambda.min" )
coef(lasso.lm.model, s = "lambda.1se" )
lasso.lm.model


```



```{r}

par(mfrow=c(2,2))

plot(lasso.lm.model)
plot(lasso.lm.model$glmnet.fit, xvar="lambda", label=TRUE)
plot(lasso.lm.model$glmnet.fit, xvar='dev', label=TRUE)

#rocs <- roc.glmnet(lasso.model, newx = X, newy = Y )
#plot(rocs,type="l")  

```


```{r}
assess.glmnet(lasso.lm.model,           
              newx = X,              
              newy = Y,
              family = 'gaussian')    

print(glmnet_cv_aicc(lasso.lm.model, 'lambda.min'))
print(glmnet_cv_aicc(lasso.lm.model, 'lambda.1se'))

```



```{r}
coef(lasso.lm.model, s = "lambda.min" )
vip(lasso.lm.model, num_features=20 ,geom = "col", include_type=TRUE, lambda = "lambda.min")
```


#### Model Performance

```{r}

# 
t0_df <- valid_data %>% dplyr::select(-c(TARGET_FLAG)) %>% drop_na()


# build X matrix and Y vector
X_test <- model.matrix(TARGET_AMT ~ ., data=t0_df)[,-1]
Y_test <- t0_df[,"TARGET_AMT"] 

# predict using coefficients at lambda.min
lassoPred <- predict(lasso.lm.model, newx = X_test, type = "response", s = 'lambda.min')

pred_ln_df <- valid_data %>% drop_na()
pred_ln_df$TARGET_AMT_PRED <- lassoPred[,1]
#pred_df$target_pred <- ifelse(lassoPred > 0.5, 1, 0)[,1]

```


```{r}

n <- nrow(t0_df)
x <- t0_df$TARGET_AMT
e <- lassoPred - t0_df$TARGET_AMT

plot(x, e,  
     xlab = "cost", 
     ylab = "residuals", 
     bg = "steelblue", 
     col = "darkgray", cex = 1.5, pch = 21, frame = FALSE)
abline(h = 0, lwd = 2)
for (i in 1 : n) 
  lines(c(x[i], x[i]), c(e[i], 0), col = "red" , lwd = 1)

```





```{r}

multi_metric <- metric_set(mape, smape, mase, mpe, rmse)
pred_ln_df %>% multi_metric(truth=TARGET_AMT, estimate=TARGET_AMT_PRED)

```


### 3.2 Model 2 - 


### 3.3 Model 3 - 

### 3.4 Model selection



## 4. Binary logistic regression


### 4.1 Model 1 - Lasso



```{r}

t_df <- train_data %>% dplyr::select(-c(TARGET_AMT)) %>% drop_na()


# build X matrix and Y vector
X <- model.matrix(TARGET_FLAG ~ ., data=t_df)[,-1]
Y <- t_df[,"TARGET_FLAG"] 

```



```{r}


lasso.log.model<- cv.glmnet(x=X,y=Y,
                       family = "binomial",
                       link = "logit",
                       standardize = TRUE,                       #standardize  
                       nfold = 5,
                       alpha=1)                                  #alpha=1 is lasso

l.min <- lasso.log.model$lambda.min
l.1se <- lasso.log.model$lambda.1se
coef(lasso.log.model, s = "lambda.min" )
coef(lasso.log.model, s = "lambda.1se" )
lasso.log.model


```



```{r}

par(mfrow=c(2,2))

plot(lasso.log.model)
plot(lasso.log.model$glmnet.fit, xvar="lambda", label=TRUE)
plot(lasso.log.model$glmnet.fit, xvar='dev', label=TRUE)

rocs <- roc.glmnet(lasso.log.model, newx = X, newy = Y )
plot(rocs,type="l")  

```


```{r}
assess.glmnet(lasso.log.model,           
                newx = X,              
                newy = Y,
                family = "binomial",
                s = 'lambda.min'
              )    

print(glmnet_cv_aicc(lasso.log.model, 'lambda.min'))
print(glmnet_cv_aicc(lasso.log.model, 'lambda.1se'))

```



```{r}
coef(lasso.log.model, s = "lambda.min" )
vip(lasso.log.model, num_features=20 ,geom = "col", include_type=TRUE, lambda = "lambda.min")
```


#### Model Performance

```{r}

# 
t0_df <- valid_data %>% dplyr::select(-c(TARGET_AMT)) %>% drop_na()


# build X matrix and Y vector
X_test <- model.matrix(TARGET_FLAG ~ ., data=t0_df)[,-1]
Y_test <- t0_df[,"TARGET_FLAG"] 

# predict using coefficients at lambda.min
lassoPred <- predict(lasso.log.model, newx = X_test, type = "response", s = 'lambda.min')

pred_log_df <- valid_data %>% drop_na()
pred_log_df$TARGET_FLAG_PROB <- lassoPred[,1]
pred_log_df$TARGET_FLAG_PRED <- ifelse(lassoPred > 0.5, 1, 0)[,1]

```




```{r}

confusion.glmnet(lasso.log.model, newx = X_test, newy = Y_test, s = 'lambda.min')

```





#### Checking Model Assumptions


```{r}

pred_log_df <- pred_log_df %>%
  mutate(logit = log(TARGET_FLAG_PROB/(1-TARGET_FLAG_PROB))) 

m_df <- pred_log_df %>% dplyr::select(where(is.numeric)) %>% pivot_longer(!c(TARGET_AMT,TARGET_FLAG_PROB,TARGET_FLAG_PRED,logit), 
                                 names_to='variable' , values_to = 'value')

#Scatter plot
ggplot(m_df, aes(logit, value))+
  geom_point(size = 0.5, alpha = 0.5) +
  geom_smooth(method = "loess") + 
  theme_bw() + 
  facet_wrap(~variable, scales = "free_y")

```

The lasso regression solve multicollinearity issue by selecting the variable with the largest coefficient while setting the rest to (nearly) zero. 



```{r}

pred_log_df$index <- as.numeric(rownames(pred_log_df))
pred_log_df$resid <- pred_log_df$TARGET_FLAG_PROB - pred_log_df$TARGET_FLAG


ggplot(pred_log_df, aes(index, resid)) + 
  geom_point(aes(color = TARGET_FLAG), alpha = .5) +
  theme_bw()

```




### 4.2 Model 2 - 


### 4.3 Model 3 - 


### 4.4 Model selection


```{r}


results_lm_tbl <- tibble(
                      Model = character(),
                      mape = numeric(), 
                      smape = numeric(), 
                      mase = numeric(), 
                      mpe = numeric(), 
                      rmse = numeric(),
                      AIC = numeric()
                )

results_log_tbl <- tibble(
                      Model = character(),
                      Accuracy = numeric(), 
                      "Classification error rate" = numeric(),
                      F1 = numeric(),
                      Deviance= numeric(), 
                      R2 = numeric(),
                      Sensitivity = numeric(),
                      Specificity = numeric(),
                      Precision = numeric(),
                      AIC= numeric()
                )

```



** Model 3.1: Lasso Linear Regression **


```{r}

# build X matrix and Y vector
t0_df <- valid_data %>% dplyr::select(-c(TARGET_FLAG)) %>% drop_na()
X_test <- model.matrix(TARGET_AMT ~ ., data=t0_df)[,-1]
Y_test <- t0_df[,"TARGET_AMT"] 



lasso.r1 <- assess.glmnet(lasso.lm.model,
                                newx = X_test,
                                newy = Y_test )
lasso.r2 <- glmnet_cv_aicc(lasso.lm.model, 'lambda.min')


multi_metric <- metric_set(mape, smape, mase, mpe, rmse)
m_df <- pred_ln_df %>% multi_metric(truth=TARGET_AMT, estimate=TARGET_AMT_PRED)



results_lm_tbl <- results_lm_tbl %>% add_row(tibble_row(
                      Model = "M4.1:Lasso Linear",
                      mape = m_df[[1,3]],
                      smape = m_df[[2,3]],
                      mase = m_df[[3,3]],
                      mpe = m_df[[4,3]],
                      rmse = m_df[[5,3]],
                      AIC = lasso.r2$AICc)
                     )


```





** Model 4.1: Lasso Logistic Regression **

```{r}

conf_matrix_1 = confusionMatrix(factor(pred_log_df$TARGET_FLAG_PRED),factor(pred_log_df$TARGET_FLAG), "1")
conf_matrix_1$byClass["F1"]



# build X matrix and Y vector
t0_df <- valid_data %>% dplyr::select(-c(TARGET_AMT)) %>% drop_na()
X_test <- model.matrix(TARGET_FLAG ~ ., data=t0_df)[,-1]
Y_test <- t0_df[,"TARGET_FLAG"] 


lasso.r1 <- assess.glmnet(lasso.log.model,           
                                newx = X_test,              
                                newy = Y_test,
                                family = "binomial",
                                s = 'lambda.min'
                          )   
lasso.r2 <- glmnet_cv_aicc(lasso.log.model, 'lambda.min')



results_log_tbl <- results_log_tbl %>% add_row(tibble_row(
                      Model = "M4.1:Lasso Logistic", 
                      Accuracy=lasso.r1$auc[1], 
                      "Classification error rate" = 1 - lasso.r1$auc[1],
                      F1 = conf_matrix_1$byClass["F1"],
                      Deviance=lasso.r1$deviance[[1]], 
                      R2 = NA,
                      Sensitivity = conf_matrix_1$byClass["Sensitivity"],
                      Specificity = conf_matrix_1$byClass["Specificity"],
                      Precision = conf_matrix_1$byClass["Precision"],
                      AIC= lasso.r2$AICc)
                     )



```










Regression

```{r}

#t1 <- as_tibble(cbind(variables = names(results_tbl), t(results_tbl)))

kable(results_lm_tbl, digits=4) %>% 
  kable_styling(bootstrap_options = "basic", position = "center")

```




Logistic

```{r}


#t1 <- as_tibble(cbind(variables = names(results_tbl), t(results_tbl)))

kable(results_log_tbl, digits=4) %>% 
  kable_styling(bootstrap_options = "basic", position = "center")

```



## 5. Predictions


## 6. Conclusion



## 7. References


## Appendix: R code


