---
title: 'Homework #4:  Multiple linear regression and Binary logistic regression'
subtitle: 'Critical Thinking Group 1'
author: 'Ben Inbar, Cliff Lee, Daria Dubovskaia, David Simbandumwe, Jeff Parks'
output:
  html_document:
    toc: yes
    toc_float: yes
    theme: united
  pdf_document:
    toc: yes
editor_options:
  chunk_output_type: console
  markdown: 
    wrap: sentence
---

```{r setup, include=FALSE}
# chunks
knitr::opts_chunk$set(echo=FALSE, eval=TRUE, include=TRUE, 
message=FALSE, warning=FALSE, fig.height=5)

# libraries
library(yardstick)
library(tidyverse)
library(janitor)
library(mice)
library(readr)
library(skimr)
library(ggplot2)
library(tidyverse)
library(reshape2)
library(MASS)
library(forecast)
library(kableExtra)
library(ggpubr)
library(fastDummies)
library(jtools)
library(performance)

## To load the below libraries you might have to do the following in the console first:
####  install.packages("devtools")
####  devtools::install_github("haleyjeppson/ggmosaic")
####  devtools::install_github("thomasp85/patchwork")

library(ggmosaic)
library(patchwork)

library(caTools)
library(corrplot)
library(Hmisc)
library(glmnet)
library(vip)
library(caret)

# ggplot
theme_set(theme_bw())
```

```{r common functions}

#' glmnet_cv_aicc
#'
#' @param fit
#' @param lambda
#'
#' @return
#' @export
#'
#' @examples
glmnet_cv_aicc <- function(fit, lambda = 'lambda.1se'){
  whlm <- which(fit$lambda == fit[[lambda]])
  with(fit$glmnet.fit,
       {
         tLL <- nulldev - nulldev * (1 - dev.ratio)[whlm]
         k <- df[whlm]
         n <- nobs
         return(list('AICc' = - tLL + 2 * k + 2 * k * (k + 1) / (n - k - 1),
                     'BIC' = log(n) * k - tLL))
       })
}


#' coeff2dt
#'
#' @param fitobject 
#' @param s 
#'
#' @return
#' @export
#'
#' @examples
coeff2dt <- function(fitobject, s) {
  coeffs <- coef(fitobject, s) 
  coeffs.dt <- data.frame(name = coeffs@Dimnames[[1]][coeffs@i + 1], coefficient = coeffs@x) 

  # reorder the variables in term of coefficients
  return(coeffs.dt[order(coeffs.dt$coefficient, decreasing = T),])
}

```

```{r data}
# read train and evaluation datasets
train_df <- read.csv("https://raw.githubusercontent.com/cliftonleesps/data621_group1/main/hw4/insurance_training_data.csv")
eval_df <- read.csv("https://raw.githubusercontent.com/cliftonleesps/data621_group1/main/hw4/insurance-evaluation-data.csv")
```

## Overview

In this homework assignment, we will explore, analyze and model a data set containing approximately 8000 records representing customers at an auto insurance company.
We will build multiple linear regression models on the continuous variable `TARGET_AMT` and binary logistic regression model on the boolean variable `TARGET_FLAG` to predict the probability that a person will crash their car, and to predict the associated costs.

We are going to build several models using different techniques and variable selection.
In order to best assess our predictive models, we will create a validation set within our training data along an 80/20 training/testing proportion, before applying the finalized models to a separate evaluation dataset that does not contain the target.

## 1. Data Exploration

The insurance training dataset contains 8161 observations of 26 variables, each record represents a customer.
The evaluation dataset contains 2141 observations of 26 variables.
These include demographic measures such as age and gender, socioeconomic measures such as education and household income, and vehicle-specific metrics such as car model, age and assessed value.

<img src="https://raw.githubusercontent.com/cliftonleesps/data621_group1/main/hw4/hw4_variables.png" width="1200" style="display: block; margin-left: auto; margin-right: auto; width: 100%;"/>


### 1.1 Summary Statistics

Each record also has two response variables.
The first response variable, `TARGET_FLAG`, is a boolean where "1" means that the person was in a car crash.
The second response variable, `TARGET_AMT` is a numeric indicating the (positive) cost if a car crash occurred; this value is zero if the person did not crash their car.

We can explore a sample of the training data here, and make some initial observations:

-   Some currency variables are strings with '\$' symbols instead of numerics (e.g.`OLDCLAIM`, `BLUEBOOK`).
-   Some character variables include a prefix `z_` that could be removed for readability (`EDUCATION`, `JOB`).

```{r summary1}
DT::datatable(
      train_df[1:25,],
      extensions = c('Scroller'),
      options = list(scrollY = 350,
                     scrollX = 500,
                     deferRender = TRUE,
                     scroller = TRUE,
                     dom = 'lBfrtip',
                     fixedColumns = TRUE, 
                     searching = FALSE), 
      rownames = FALSE) 
```

The table below provides valuable descriptive statistics about the training data.

Based on this summary table and exploration of the data, we can make the following observations:

-   14 variables are categorical, 12 are numeric.
-   There is no missing data for character variables.
-   Numeric variables with missing values include `YOJ` (6%), `INCOME` (5%), `HOME_VAL` (6%), `CAR_AGE` (6%), and `AGE` (1%).
-   Most of the numeric variables have a minimum of zero.
-   One variable, `CAR_AGE` has a negative value of -3, which doesn't make intuitive sense. We'll handle this below.
-   Some of the variables are character though they should be numeric and vice-versa.

```{r summary}
skim_without_charts(train_df)
```

```{r prep}
# Drop INDEX
train_df <- train_df %>% dplyr::select(-INDEX)
eval_df <- eval_df %>% dplyr::select(-INDEX)

## Remove z_ from character class values
z_vars <- c("MSTATUS","SEX","JOB","CAR_TYPE","URBANICITY","EDUCATION")
for (v in z_vars) {
  train_df <- train_df %>% mutate(!!v := str_replace(get(v),"z_",""))
  eval_df <- eval_df %>% mutate(!!v := str_replace(get(v),"z_",""))
}

# Update RED_CAR, replace [no,yes] values with [No, Yes] values
train_df <- train_df %>% mutate( RED_CAR = ifelse(RED_CAR == "no","No","Yes"))
eval_df <- eval_df %>% mutate( RED_CAR = ifelse(RED_CAR == "no","No","Yes"))

# Instead of level "" will be "Unknown"
train_df$JOB[train_df$JOB==""] <- "Unknown"
eval_df$JOB[eval_df$JOB==""] <- "Unknown"

# Convert currency columns from character class to integer
dollar_vars <- c("INCOME","HOME_VAL","BLUEBOOK","OLDCLAIM")
for (v in dollar_vars) {
  train_df <- train_df %>% mutate(!!v := str_replace(get(v),"[\\$,]",""))
  eval_df <- eval_df %>% mutate(!!v := str_replace(get(v),"[\\$,]",""))
}

currency_vars <- c("INCOME","HOME_VAL","BLUEBOOK","OLDCLAIM")

for (v in currency_vars) {
  train_df <- train_df %>% mutate(!!v := parse_number(get(v)))
  eval_df <- eval_df %>% mutate(!!v := parse_number(get(v)))
}

```



### 1.2 Distributions

Before building a model, we need to make sure that we have both classes equally represented in our `TARGET_FLAG` variable.
Class `1` takes 26% and class `0` takes 74% of the target variable.
As a result, we have unbalanced class distribution for our target variable that we have to deal with, we have to take some additional steps (bootstrapping, etc) before using logistic regression.

```{r balance}
prop.table(table(train_df$TARGET_FLAG)) %>% 
  kable(label = "Distribution of Target Flag", 
        caption = "Distribution of Target Flag", 
        col.names = c("Value", "%"), 
        digits = 2) %>% 
    kable_styling(full_width = F) 
```

Many of these distributions for the numeric variables seem highly skewed and non-normal.
As part of our data preparation we'll use power transformations to find whether transforming variables to more normal distributions improves our models' efficacy.

```{r, cache=TRUE}
dist_df <- train_df %>% dplyr::select(where(is.numeric)) %>%
  pivot_longer(!c(TARGET_FLAG,TARGET_AMT), 
               names_to = 'variable', 
               values_to = 'value') %>% 
  drop_na()

dist_df %>% ggplot(aes(x=value, group=TARGET_FLAG, fill=TARGET_FLAG)) +
  geom_density(color='#023020') + 
  facet_wrap(~variable, scales = 'free',  ncol = 4) + 
  theme_bw()
```

By checking the frequency of the variables that have very few unique values (`HOMEKIDS`, `KIDSDRIV`, `CLM_FREQ`), we can assume that these variables better be transformed to binary (1 if the value is greater than 0). As the see the percentage below, more than 50% of the data for these variables is 0, the rest some over few values.
```{r}
train_df[,c("HOMEKIDS","KIDSDRIV", "CLM_FREQ")] %>%
gather("variable","value") %>%
group_by(variable) %>%
count(value) %>%
mutate(value = factor(value,levels=5:0)) %>%
mutate(percent = n*100/8161) %>%
ggplot(.,
aes(variable,percent)) +
geom_bar(stat = "identity", aes(fill = value)) +
xlab("Variable") +
ylab("Percent of records") +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
scale_fill_manual(values = rev(c("#999999","#E69F00", "#56B4E9", "#009E73", "#F0E442","#CC79A7")))
```


```{r}
train_df[,c("Commercial_vehicle","Married","Single_parent","Red_car","Revoked","Sex_male","Urban_not_rural","TARGET_FLAG")] %>%
gather("variable","value") %>%
group_by(variable) %>%
count(value) %>%
mutate(value = factor(value)) %>%
mutate(percent = n*100/8161) %>%
ggplot(.,
aes(variable,percent)) +
geom_bar(stat = "identity", aes(fill = value)) +
xlab("Variable") +
ylab("Percent of records") +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
```



### 1.3 Box Plots

This is born out in the box plots, in which we do see quite a variable range for `INCOME`, `KIDSDRIV`, and `OLDCLAIM`among others.

```{r boxplot, cache=TRUE}
dist_df %>% ggplot(aes(x=value, group=TARGET_FLAG, fill=TARGET_FLAG)) + 
  geom_boxplot(color='#023020') + 
  facet_wrap(~variable, scales = 'free',  ncol = 4) + 
  theme_bw()
```

### 1.4 Scatter Plot

Interestingly, none of our predictors appear to have strong linear relationships to our `TARGET_AMT` response variable, which is a primary assumption of linear regression.
This suggests that alternative methods might be more successful in modeling the relationships.

```{r scatterplot, cache=TRUE}
dist_df %>% drop_na() %>% ggplot(aes(x=value, y=TARGET_AMT)) + 
  geom_smooth(method='glm', se=TRUE, na.rm=TRUE) +
  geom_point(color='#023020') + 
  facet_wrap(~variable, scales = 'free',  ncol = 4) + 
  theme_bw()
```

### 1.5 Correlation Matrix

We see this also born out in the correlation matrix, in which the highest coefficient is 0.58, with others averaging much lower.

```{r corr, cache=TRUE}
rcore <- rcorr(as.matrix(train_df %>% dplyr::select(where(is.numeric))))
coeff <- rcore$r
corrplot(coeff, tl.cex = .5, tl.col="black", method = 'color', addCoef.col = "black",
         type="upper", order="hclust",
         diag=FALSE)
```

## 2. Data preparation

### 2.1 Data types

In order to work with our training dataset, we'll need to first convert some variables to more useful data types:

-   Convert currency columns from character to integer: `INCOME`,`HOME_VAL`,`BLUEBOOK` and `OLDCLAIM`.
-   Convert character columns to factors: `TARGET_FLAG`, `CAR_TYPE`, `CAR_USE`, `EDUCATION`, `JOB`, `MSTATUS`, `PARENT1`, `RED_CAR`, `REVOKED`, `SEX` and `URBANICITY`.

```{r}
# Other changes applied in Section 1.

factor_vars <- c("TARGET_FLAG", "PARENT1","CAR_TYPE","JOB","CAR_USE",
                 "URBANICITY","RED_CAR","REVOKED","MSTATUS","EDUCATION","SEX")



for (v in factor_vars) {
  train_df <- train_df %>% mutate(!!v := factor(get(v)))
}

```


```{r}
colnames(train_df) <- plyr::mapvalues(colnames(insurance),
	from=c("HOMEKIDS","KIDSDRIV"),
	to=c("Kids","Driving_kids"))
colnames(evaluation) <- plyr::mapvalues(colnames(evaluation),
	from=c("HOMEKIDS","KIDSDRIV"),
	to=c("Kids","Driving_kids"))

for(var in c("Kids","Driving_kids"))
{
train_df[,var] <- ifelse(train_df[,var] > 0,1,0)
eval_df[,var] <- ifelse(eval_df[,var] > 0,1,0)
}
```


### 2.2 Transformations, Outliers and Missing Values

Before we go further, we need to identify and handle any missing, NA or negative data values so we can perform log transformations and regression.

First, we'll apply transformations to clean up and align formatting of our variables:

-   Drop the `INDEX` variable.
-   Remove "z\_" from all character class values (`MSTATUS`, `SEX`, `JOB`, `CAR_TYPE`, `URBANICITY`, `EDUCATION`)
-   Remove "\$" from the numeric values (`INCOME`, `HOME_VAL`, `BLUEBOOK`, `OLDCLAIM`).
-   Update RED_CAR, replace [no,yes] values with [No, Yes] values.
-   Replace `JOB` blank values with 'Unknown'.
-   Transform variables `HOMEKIDS`, `KIDSDRIV`, `CLM_FREQ` to binary, the value is TRUE if the original value is greater than 0.

Next, we'll manually adjust two special cases of missing or outlier values.

-   In cases where `YOJ` is zero and `INCOME` is NA, we'll set `INCOME` to zero to avoid imputing new values over legitimate instances of non-employment.
-   There is also at least one value of `CAR_AGE` that is less than zero - we'll assume this is a data collection error and set it to zero (representing a brand-new car.)

```{r}
## Check JOB values where YOJ==0 and INCOME is NA
# train_df %>% filter(YOJ == 0 & is.na(INCOME)) %>% dplyr::select(JOB) %>% unique()

# Manual correction of missing/outlier values
train_df <- train_df %>%
  mutate(CAR_AGE = ifelse(CAR_AGE < 0, 0, CAR_AGE)) %>% 
  mutate(INCOME = ifelse(YOJ == 0 & is.na(INCOME), 0, INCOME))
```

```{r}


for(var in c("EDUCATION","CAR_TYPE","JOB"))
{
frequency_per_level <- data.frame(table(train_df[,var]))
colnames(frequency_per_level) <- c("value","n")
print(ggplot(frequency_per_level,
aes(value,n*100/8161)) +
geom_bar(stat = "identity",col="black",fill="darkgrey") +
xlab("") +
ylab("Percent of records") +
ggtitle(var) +
theme(axis.text.x = element_text(angle = 90, hjust = 1)))
}
```

We'll use MICE to impute our remaining variables with missing values - `AGE`, `YOJ`, `CAR_AGE`, `INCOME` and `HOME_VALUE`:

-   We might reasonably assume that relationships exist between these variables (older, more years on the job may correlate with higher income and home value). Taking simple means or medians might suppress those features, but MICE should provide a better imputation.

```{r impute, cache=TRUE}
# MICE imputation of missing values
imputed_Data <- mice(train_df, m=5, maxit = 20, method = 'pmm', seed = 500, printFlag=FALSE)
train_df <- complete(imputed_Data)
```


To give our models more variables to work with, we'll engineer some additional features:

-   Create bin values for `CAR_AGE`, `HOME_VAL` and `TIF`.
-   Create dummy variables for two-level factors, `MALE`, `MARRIED`, `LIC_REVOKED`, `CAR_RED`, `PRIVATE_USE`, `SINGLE_PARENT` and `URBAN`.

```{r dummy_vars}
## Create Dummy Variables for factors with two levels
dummy_vars <- function(df){
  df %>%
    mutate(
      MALE = factor(ifelse(SEX=="M", 1, 0)), 
      MARRIED = factor(ifelse(MSTATUS=="Yes", 1, 0)),
      LIC_REVOKED = factor(ifelse(REVOKED =="Yes", 1, 0)),
      CAR_RED = factor(ifelse(RED_CAR =="Yes", 1, 0)),
      PRIVATE_USE = factor(ifelse(CAR_USE =="Private", 1, 0)),
      SINGLE_PARENT = factor(ifelse(PARENT1 =="Yes", 1, 0)),
      URBAN = factor(ifelse(URBANICITY =="Highly Urban/ Urban", 1, 0))
    ) %>% 
    dplyr::select(-c(SEX, MSTATUS, REVOKED, RED_CAR, CAR_USE, PARENT1, URBANICITY))
      }

train_df <- dummy_vars(train_df)
```


```{r}
train_df[,c("MALE", "MARRIED", "LIC_REVOKED", "CAR_RED", "PRIVATE_USE","SINGLE_PARENT", "URBAN","TARGET_FLAG")] %>%
gather("variable","value") %>%
group_by(variable) %>%
count(value) %>%
mutate(value = factor(value)) %>%
mutate(percent = n*100/8161) %>%
ggplot(.,
aes(variable,percent)) +
geom_bar(stat = "identity", aes(fill = value)) +
xlab("Variable") +
ylab("Percent of records") +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

```{r}
## put into bins:  TIF; CAR_AGE; HOMEKIDS; HOME_VAL
q <- quantile(transformed_df$CAR_AGE)
q <- c(-1,  1,  8, 12, 28)
transformed_df <- transformed_df %>% mutate(CAR_AGE_BIN = cut(CAR_AGE, breaks=q, labels=FALSE))

q <- quantile(transformed_df$HOME_VAL)
q[1] <- -1
transformed_df <- transformed_df%>% mutate(HOME_VAL_BIN = cut(HOME_VAL, breaks=q, labels=FALSE))

q <- quantile(transformed_df$TIF); 
q[1] <- -1
transformed_df <- transformed_df %>% mutate(TIF_BIN = cut(TIF, breaks=q, labels=FALSE))
```




Next we'll want to consider any power transformations for variables that have skewed distributions.
For example, our numeric response variable `TARGET_AMT` is a good candidate for transformation as its distribution is very highly skewed, and the assumption of normality is required in order to apply linear regression.

-   Log transformation will be applied to variables `TARGET_AMT`, `OLDCLAIM` to transform their distributions from right-skewed to normally distributed.
-   Similarly, BoxCox transformation will be applied to variables `BLUEBOOK`, `TRAVTIME`, `TIF`, so they also are more normally distributed.

```{r boxcox, fig.keep="none", cache=TRUE}

transformed_df <- train_df

## Use basic log transformation on `TARGET_AMT`, `OLDCLAIM`
transformed_df$TARGET_AMT <- log(transformed_df$TARGET_AMT + 1)
transformed_df$OLDCLAIM <- log(transformed_df$OLDCLAIM + 1)


# use Box-Cox on `BLUEBOOK`, `TRAVTIME`,  `TIF`
bluebook_boxcox <- boxcox(lm(transformed_df$BLUEBOOK ~ 1))
bluebook_lambda <- bluebook_boxcox$x[which.max(bluebook_boxcox$y)]
bluebook_trans <- BoxCox(transformed_df$BLUEBOOK, bluebook_lambda)
transformed_df$BLUEBOOK <- bluebook_trans

travtime_boxcox <- boxcox(lm(transformed_df$TRAVTIME ~ 1))
travtime_lambda <- travtime_boxcox$x[which.max(travtime_boxcox$y)]
travtime_trans <- BoxCox(transformed_df$TRAVTIME, travtime_lambda)
transformed_df$TRAVTIME <- travtime_trans

tif_boxcox <- boxcox(lm(transformed_df$TIF ~ 1))
tif_lambda <- tif_boxcox$x[which.max(tif_boxcox$y)]
tif_trans <- BoxCox(transformed_df$TIF, tif_lambda)
transformed_df$TIF <- tif_trans
```





We can examine our final, transformed training dataset and distributions below (with a temporary numeric variable `CAR_CRASH` to represent the response variable for visualization purposes.)

```{r}
skim_without_charts(transformed_df)
```

```{r}
## Create temporary CAR_CRASH variable from the target_flag for visualization
vis_df <- transformed_df %>% mutate(CAR_CRASH = ifelse(TARGET_FLAG == 1, 'Yes', 'No'))

a <- vis_df %>% 
  select_if(is.numeric) %>% 
  dplyr::select(!(c(TARGET_AMT))) %>%
  data.table::melt()

a %>% ggplot(aes(x=value)) + 
  geom_density(alpha=1, fill='#47A0F5') + 
  facet_wrap(~variable, scales='free')

vis_df$SINGLE_PARENT
```


### 2.5 Training and Validation Sets

To proceed with modeling, we'll split our training data into train (80%) and validation (20%) datasets.

```{r}
set.seed(1233)
sample <- sample.split(transformed_df$TARGET_FLAG, SplitRatio = 0.8)
train_data  <- subset(transformed_df, sample == TRUE)
valid_data   <- subset(transformed_df, sample == FALSE)


train_original <- createDataPartition(train_df$TARGET_AMT, p=0.8, list = FALSE)
original_train <- train_df[train_original, ] %>% dplyr::select(-c(TARGET_FLAG))
original_test <- train_df[-train_original, ] %>% dplyr::select(-c(TARGET_FLAG))

log_original <- createDataPartition(train_df$TARGET_FLAG, p=0.8, list = FALSE)
log_train <- train_df[log_original, ] %>% dplyr::select(-c(TARGET_AMT))
log_test <- train_df[-log_original, ] %>% dplyr::select(-c(TARGET_AMT))

train_linear <- createDataPartition(transformed_df$TARGET_AMT, p=0.8, list = FALSE)
train_log <- createDataPartition(transformed_df$TARGET_FLAG, p=0.8, list = FALSE)

amt_train <- transformed_df[train_linear, ] %>% dplyr::select(-c(TARGET_FLAG))
amt_test <- transformed_df[-train_linear, ] %>% dplyr::select(-c(TARGET_FLAG))

flag_train <- transformed_df[train_log,] %>% dplyr::select(-c(TARGET_AMT))
flag_test <- transformed_df[-train_log,] %>% dplyr::select(-c(TARGET_AMT))

```

## 3. Multiple Linear Regression

We'll use Multiple Linear Regression to model the `TARGET_AMT` response variable, the estimated cost of a crash for a given observation.

### 3.1 Model 1 - Original data

As the first step, we are going to build the linear model based on the original training dataset with some variables transformed to categorical/numeric if needed, with no missing values. First, we include all variables.

```{r model_1}
# build model
model_1 <- lm(TARGET_AMT ~ ., data = original_train)

```

With stepAIC, we can try to see what variables should be included in our model to increase the performance. 
Based on the the results of the function, we will include `KIDSDRIV`, `INCOME`, `PARENT1`,  `MSTATUS`,  `SEX`, `JOB`, `TRAVTIME`, `CAR_USE`, `BLUEBOOK`, `TIF`, `CAR_TYPE`, `OLDCLAIM`, `CLM_FREQ`, `REVOKED`, `MVR_PTS`, `CAR_AGE`, and `URBANICITY` variables.
```{r model_1_aic}

# select features and refit by stepwise selection (AIC)
model_1_aic <- model_1 %>% stepAIC(trace = FALSE)
summ(model_1_aic, digits = getOption("jtools-digits", 4))

```

#### Model Performance
The F-statistic is 16.87, the adjusted R-squared is 0.064, and out of the 27 variables, 17 have statistically significant p-values.

The adjusted R2 indicates that only 6.4% of the variance in the response variable can be explained by the predictor variables.
The F-statistic is low and the model's p-value is not statistically significant.
We should probably look at other models for better performance.

```{r}
# summary table
cap <- paste('Adjusted R2:',round(summary(model_1_aic)$adj.r.squared,5))

model_1_aic %>% tidy() %>% 
  dplyr::select(term, estimate, `p.value`) %>%
  arrange(p.value, desc(abs(estimate))) %>% 
  # mutate(est_orig = get_boxcox_exp(estimate, lm_bc_lambda)) %>%
  # relocate(est_orig, .after=estimate) %>%
  kable(digits = 3, caption=cap)
```

```{r}
# validate and calculate RMSE
model_1_aic.valid <- predict(model_1_aic, newdata = original_test)
model_1_aic.eval <- bind_cols(target = original_test$TARGET_AMT, predicted=model_1_aic.valid)
model_1_aic.rmse <- sqrt(mean((model_1_aic.eval$target - model_1_aic.eval$predicted)^2)) 

# plot targets vs predicted
model_1_aic.eval %>%
  ggplot(aes(x = target, y = predicted)) +
  geom_point(alpha = .3) +
  geom_smooth(method="lm", color='grey', alpha=.3, se=FALSE) +
  labs(title=paste('RMSE:',round(model_1_aic.rmse,1)))
```

#### Model Assumptions

We evaluate the linear modeling assumptions using standard diagnostic plots, the Breusch--Pagan Test for Heteroscedasticity and Variance Inflation Factor (VIF) to assess colinearity.

The initial review of the diagnostic plots for this model shows some deviation from linear modeling assumptions.
The Q-Q plot shows some deviations from the normal distribution at the ends.
The residuals-fitted and standardized residuals-fitted plots show a curve in the main cluster, which indicates that we do not have constant variance.

```{r check_lm1}
check_model(model_1_aic, check=c('ncv','qq','homogeneity','outliers'))
```



### 3.2 Model 2 - Stepwise Feature Selection

```{r model_2}
# load data
t_df <- train_data %>% dplyr::select(-c(TARGET_FLAG))
v_df <- valid_data %>% dplyr::select(-c(TARGET_FLAG))

# build model
model_2 <- lm(TARGET_AMT ~ ., data = t_df)

```


```{r model_2_aic}

# select features and refit by stepwise selection (AIC)
model_2_aic <- model_2 %>% stepAIC(trace = FALSE)
```

#### Model Performance

```{r}
# summary table
cap <- paste('Adjusted R2:',round(summary(model_2_aic)$adj.r.squared,5))

model_2_aic %>% tidy() %>% 
  dplyr::select(term, estimate, `p.value`) %>%
  arrange(p.value, desc(abs(estimate))) %>% 
  # mutate(est_orig = get_boxcox_exp(estimate, lm_bc_lambda)) %>%
  # relocate(est_orig, .after=estimate) %>%
  kable(digits = 3, caption=cap)
```

```{r}
# validate and calculate RMSE
stepwise.lm.valid <- predict(model_2_aic, newdata = v_df)
stepwise.lm.eval <- bind_cols(target = v_df$TARGET_AMT, predicted=stepwise.lm.valid)
stepwise.lm.rmse <- sqrt(mean((stepwise.lm.eval$target - stepwise.lm.eval$predicted)^2)) 

# plot targets vs predicted
stepwise.lm.eval %>%
  ggplot(aes(x = target, y = predicted)) +
  geom_point(alpha = .3) +
  geom_smooth(method="lm", color='grey', alpha=.3, se=FALSE) +
  labs(title=paste('RMSE:',round(stepwise.lm.rmse,1)))
```

The resulting model is much more parsimonious than the first, with statistically significant results for three predictors, **bluebook**, **mvr_pts** and **mstatus_yes**.

The Adjusted R-Squared is better than Model 1 but still very low (0.0167) meaning this model only explains about 1.7% of the total variance in the response variable **target_amt**. However, an examination of the residuals indicates most of the key assumptions for linear regression are met - the Residuals vs Fitted plot shows a more constant variability of the residuals, and the Q-Q plot indicates a greater level of normality.

The summary table includes the estimate transformed to original scale for easier interpretation.In this case, the 'base' *target_amt* would be estimated at \$3,434.76 with an increase in 1% per each dollar of bluebook value, a 1.07% increase if the driver were male, and 0.9% decrease if the driver were married.

#### Model Assumptions

```{r}
# residual plots
plot(model_2_aic)
```



### 3.3 Model 3 - Lasso Regression

The `cv.glmnet()` function was used to perform k-fold cross-validation with variable selection using lasso regularization.
The following attribute settings were selected for the model:

-   type.measure = "mse" - The type.measure is set to minimize the Mean Squared Error for the model.
-   nfold = 10 - Given the size of the dataset we defaulted to 10-fold cross-validation.
-   family = gaussian - For Linear Regression
-   alpha = 1 - The alpha value of 1 sets the variable shrinkage method to lasso.
-   weights = a weight of 0.2638 / n for observation with a 0 `TARGET_AMT` and 0.7362 / n for all observations with all other values of `TARGET_AMT.`
-   standardize = TRUE - Finally, we explicitly set the standardization attribute to TRUE; this will normalize the prediction variables around a mean of zero and a standard deviation of one before modeling.

The resulting model is explored by extracting coefficients at two different values for lambda, lambda.min and lambda.1se respectively.

The coefficients extracted using lambda.min minimizes the mean cross-validated error.
The resulting model includes 33 non-zero coefficients and has an AIC of 60.08.
The coefficients extracted using lambda.1se produce the most regularized model (cross-validated error is within one standard error of the minimum).
For this model there are 25 non-zero coefficients and it has an AIC of 44.23

The coefficients extracted using lambda.1se results in the lowest AIC (highest model performance) with fewer predictor variables.

```{r}
# filter the target data
t_df <- train_data %>% dplyr::select(-c(TARGET_FLAG))

# build X matrix and Y vector
X <- model.matrix(TARGET_AMT ~ ., data=t_df)[,-1]
Y <- t_df[,"TARGET_AMT"] 

# calculate the weights for the target flag
w <- table(train_data$TARGET_FLAG)
n <- length(Y)
prop_0 <- w[1] / (w[1] + w[2])
prop_1 <- w[2] / (w[1] + w[2])

# calculate the weights vector
weights_lm  <- ifelse(train_data$TARGET_FLAG == 0,  prop_1/n, prop_0/n)
```

```{r}
lasso.lm.model <- cv.glmnet(
  x=X,y=Y, # Y already logged in prep
  family = "gaussian",
  type.measure="mse",
  standardize = TRUE, # standardize
  nfold = 10,
  weights = weights_lm,
  alpha=1) # alpha=1 is lasso

lasso.lm.model

l.min <- lasso.lm.model$lambda.min
l.1se <- lasso.lm.model$lambda.1se
```

```{r}
par(mfrow=c(2,2))

plot(lasso.lm.model)
plot(lasso.lm.model$glmnet.fit, xvar="lambda", label=TRUE)
plot(lasso.lm.model$glmnet.fit, xvar='dev', label=TRUE)
```

```{r}
assess.glmnet(lasso.lm.model,           
              newx = X,              
              newy = Y,
              s = 'lambda.min',
              family = 'gaussian')    

print(glmnet_cv_aicc(lasso.lm.model, 'lambda.min'))
print(glmnet_cv_aicc(lasso.lm.model, 'lambda.1se'))
```

A closer look at the remaining 37 non-zero coefficients for the selected lambda value of lambda.min (`r round(lasso.lm.model$lambda.min,3)`) we can observe the top predictor variables `URBANICITY Highly Urban/ Urban` predictor variable has the largest impact on the response variable `TARGET_AMT`.

In the lasso model the coefficient for `URBANICITY Highly Urban/ Urban` home work area is biggest contributor to the cost estimates of a car crash by a factor of 2.

Reviewing the top 5 predictor variables that impact likelihood and cost associated with an accident:

-   `URBANICITY Highly Urban/ Urban` - working or living in an urban neighborhood increase expected cost associated with a crash
-   `JOB Doctor` - being a doctor reduces the expected costs associated with a crash
-   `JOB Manager` - being a manager reduces the expected costs associated with a crash
-   `CAR_TYPE Sports Car` - owning a sports car increases the expected costs associated with a crash
-   `CAR_USE Private` - using a car for private activities reduces the expected costs associated with a crash
-   `REVOKED Yes` - a history of having a revoked license increases the expected costs associated with a crash

Some of the notable coefficients that drop out of the model include:

-   `HOME_VAL`
-   `JOB Professional`
-   `CAR_AGE`
-   `CAR_RED`

```{r}
coef(lasso.lm.model, s = "lambda.min")
```

```{r}
# # test
# tbl <- data.matrix(coef(lasso.lm.model, s = "lambda.min"))
# 
# tbl <- data.matrix(tbl[order(tbl, decreasing=TRUE),])
# 
# kable(
#   list(tbl[1:20,], tbl[21:41,]),
#   booktabs = TRUE
# ) %>% kable_styling()

```

```{r fig.show="hold", out.width="50%", fig.height=6}
vip(lasso.lm.model, num_features=20 ,geom = "col", include_type=TRUE, lambda = "lambda.min")

coeffs.table <- coeff2dt(fitobject = lasso.lm.model, s = "lambda.1se")

coeffs.table %>% mutate(name = fct_reorder(name, desc(coefficient))) %>%
ggplot() +
  geom_col(aes(y = name, x = coefficient, fill = {coefficient > 0})) +
  xlab(label = "") +
  ggtitle(expression(paste("Lasso Coefficients with ", lambda, " = 0.0275"))) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust=0.5),legend.position = "none")
```

As mentioned earlier, the dataset has a high correlation between predictor variables.
The lasso regression approaches this issue by selecting the variable with the highest correlation and shrinking the remaining variables (as can be seen in the plot of coefficients).

#### Model Performance

The lasso model using coefficients extracted at lambda.1se was used to predict the 60,421 test cases and comparing the predicted insurance AMT to the actual cost of a car crash.
The predicted cost of the crash include negative numbers that are effectively 0.
We selected a threshold cost and assigning 0 to all amounts below that threshold value.
Since

In the training data 337.50 was the lowest crash cost included in the dataset.
We used 100 as the measurement threshold and assume that all predicted costs below 100 dollars are effectively 0.

```{r}
t0_df <- valid_data %>% dplyr::select(-c(TARGET_FLAG)) 
X_test <- model.matrix(TARGET_AMT ~ ., data=t0_df)[,-1]
Y_test <- t0_df[,"TARGET_AMT"] 

lassoPred <- predict(
  lasso.lm.model, 
  newx = X_test,
  type = "response",
  s = 'lambda.1se')

lassoPred <- exp(lassoPred) - 0.001

pred_ln_df <- valid_data %>% drop_na()
pred_ln_df$TARGET_AMT_PRED_RAW <- lassoPred[,1]
pred_ln_df$TARGET_AMT_PRED <- ifelse(lassoPred > 100, lassoPred, 0)[,1]
```

Using the yardstick package to measure model performance, mape, smape and mpe return NaNs while the `mase` = 0.578 and `rmse` = 4984.

```{r eval=FALSE}
multi_metric <- metric_set(mape, smape, mase, mpe, rmse)
pred_ln_df %>% multi_metric(truth=TARGET_AMT, estimate=TARGET_AMT_PRED)
```

Analyzing a scatter plot of the prediction errors vs measured costs and a comparative histogram of the predicted and measured costs highlights a shortcoming of the lasso model.
The model consistently predicts crash costs lower than the actual measured crash costs.
The gap is more pronounced when looking at predicted and ??????

```{r fig.show="hold", out.width="50%", fig.height=6}
n <- nrow(t0_df)
x <- t0_df$TARGET_AMT
e <- lassoPred - t0_df$TARGET_AMT

plot(x, e,  
     xlab = "cost", 
     ylab = "residuals", 
     bg = "steelblue", 
     col = "darkgray", cex = 1.5, pch = 21, frame = FALSE)
abline(h = 0, lwd = 2)
for (i in 1 : n) 
  lines(c(x[i], x[i]), c(e[i], 0), col = "red" , lwd = 1)

# comparative histogram
t0_df <- pred_ln_df %>% dplyr::select(TARGET_AMT,TARGET_AMT_PRED) %>% pivot_longer(c(TARGET_AMT,TARGET_AMT_PRED),
                                 names_to='variable' , values_to = 'value')

t0_df %>% dplyr::filter(value <= 1000) %>% 
  ggplot(aes(x=value, fill=variable)) +geom_histogram(bins=5) 

t0_df %>% dplyr::filter(value > 1000) %>% 
  ggplot(aes(x=value, fill=variable)) +geom_histogram(bins=100) 
```

#### Model Assumptions

To reduce multicollinearity we can use regularization that means to keep all the features but reducing the magnitude of the coefficients of the model.
This is a good solution when each predictor contributes to predict the dependent variable.

The Standardized Residuals plot shows increasing variance at higher values of the response variable.

```{r}
pred_ln_df$PER_ERR <- (pred_ln_df$TARGET_AMT_PRED - pred_ln_df$TARGET_AMT)
```

The lasso regression solves the multicollinearity issue by selecting the variable with the largest coefficient while setting the rest to (nearly) zero.

```{r}
pred_ln_df %>% ggplot(aes(x=TARGET_AMT, colour=TARGET_AMT_PRED, y=PER_ERR)) +
  geom_point()
```


### 3.4 Model selection

## 4. Binary Logistic Regression

We'll use Binary Logistic Regression to classify our response variable `TARGET_FLAG`, the probability of a car crash for a given observation.

### 4.1 Model 1 - Original data

As the first step, we are going to build the generalized linear model based on the original training dataset with some variables transformed to categorical/numeric if needed, with no missing values. Since our dependent variable takes only two values (0 and 1), we will use logistic regression. To do so, the function `glm()` with `family=binomial` is used. At the beginning, all variables are included.

```{r}
log_train <- log_train %>% mutate(TARGET_FLAG = as.factor(TARGET_FLAG))
log_test <- log_test %>% mutate(TARGET_FLAG = as.factor(TARGET_FLAG))

```

```{r log_model1, warning=FALSE, message=FALSE}
log_model_1 <- glm(log_train, formula = TARGET_FLAG ~., family = binomial(link = "logit"))
```


With stepAIC, we can try to see what variables should be included in our model to increase the performance. 
Based on the the results of the function, we will include the variables below.

```{r log_model1_aic, warning=FALSE, message=FALSE}

log_model_1_aic <- log_model_1 %>% stepAIC(trace = FALSE)
summary(log_model_1_aic)
```

The p-value associated with this Chi-Square statistic is below. The lower the p-value, the better the model is able to fit the dataset compared to a model with just an intercept term.
```{r chi_model_1}
1-pchisq(log_model_1_aic$null.deviance-log_model_1_aic$deviance, log_model_1_aic$df.null- log_model_1_aic$df.residual)
```

#### Model Performance

The `null deviance` of `7496.5` defines how well the target variable can be predicted by a model with only an intercept term.

The `residual deviance` of `5843.4` defines how well the target variable can be predicted by our AIC model that we fit with  predictor variables mentioned above. The lower the value, the better the model is able to predict the value of the response variable.

The p-value associated with this `Chi-Square statistic` is 0 which is less than .05, the model can be useful.

The Akaike information criterion (`AIC`) is `5907.4`. We will use this metric to compare the fit of different models.  The lower the value, the better the regression model is able to fit the data.

#### Model Assumptions

```{r check_log1}
par(mfrow=c(2,2))
plot(log_model_1_aic)

```


### 4.2 Model 2 - Stepwise Feature Selection
Next, we can try to build the generalized linear model based on the dataset with transformations and additional variables.

```{r general_box, warning=FALSE, message=FALSE}
log_model_2 <- glm(flag_train, formula = TARGET_FLAG ~ ., family = binomial(link = "logit"))
```

The stepAIC function tells us to include once again the variables below.
```{r log_model2_aic, warning=FALSE, message=FALSE}
log_model_2_aic <- log_model_2 %>% stepAIC(trace = FALSE)
summary(log_model_2_aic)
```

The p-value associated with this Chi-Square statistic is below. 
```{r chi_model_2}
1-pchisq(log_model_2_aic$null.deviance-log_model_2_aic$deviance, log_model_2_aic$df.null- log_model_2_aic$df.residual)
```


#### Model Performance


#### Model Assumptions



### 4.3 Model 3 - Lasso

Lasso Regression may be a good candidate for this dataset, since we are dealing with a large number of complex variables.
Lasso helps identify the most important variables and reduces the model complexity.

The cv.glmnet() function was also used as logistic regression model.
Similar to the regression model k-fold cross-validation was performed with variable selection using lasso regularization.
The following attribute settings were selected for the model:

-   type.measure = "class" - The type.measure is set to class to minimize the mis-classification errors of the model since the accurate classification of the validation data set is the desired outcome.
-   nfold = 10 - Given the size of the training dataset, we opted for 10-fold cross-validation as a default.
-   family = binomial - For Logistic Regression, the family attribute of the function is set to binomial.
-   link = logit - For this model, we choose the default link function for a logistic model.
-   alpha =1 - The alpha value of 1 sets the variable shrinkage method to lasso.
-   weights = a weight of 0.2638 / n for observation with a 0 `TARGET_FLAG` and 0.7362 / n observations with a 1 value of `TARGET_FLAG`.
-   standardize = TRUE - Finally, we explicitly set the standardization attribute to TRUE; this will normalize the prediction variables around a mean of zero and a standard deviation of one before modeling.

The resulting model is explored by extracting coefficients at two different values for lambda, lambda.min and lambda.1se respectively.

-   The coefficients extracted using lambda.min minimizes the mean cross-validated error. The resulting model includes 35 non zero coefficients and has an AIC of -1605.418.
-   The coefficients extracted using lambda.1se produce the most regularized model (cross-validated error is within one standard error of the minimum). The resulting model includes 25 no zero coefficients and has an AIC of -1503.695,

The coefficients extracted using lambda.min results in the lowest AIC and highest performance model.

```{r}
t_df <- train_data %>% dplyr::select(-c(TARGET_AMT)) %>% drop_na()

# build X matrix and Y vector
X <- model.matrix(TARGET_FLAG ~ ., data=t_df)[,-1]
Y <- t_df[,"TARGET_FLAG"] 

# calculate weights
w <- table(train_data$TARGET_FLAG)
n <- length(Y)
prop_0 <- w[1] / (w[1] + w[2])
prop_1 <- w[2] / (w[1] + w[2])
  
# calculate weight vector 
weights_log  <- ifelse(train_data$TARGET_FLAG == 0,  prop_1 / n, prop_0 / n)
```

```{r}
lasso.log.model<- cv.glmnet(
  x=X,y=Y,
  family = "binomial",
  link = "logit",
  standardize = TRUE,
  nfold = 5,
  alpha=1) # alpha=1 is lasso

lasso.log.model

l.min <- lasso.log.model$lambda.min
l.1se <- lasso.log.model$lambda.1se
```

```{r}
par(mfrow=c(2,2))

plot(lasso.log.model)
plot(lasso.log.model$glmnet.fit, xvar="lambda", label=TRUE)
plot(lasso.log.model$glmnet.fit, xvar='dev', label=TRUE)

rocs <- roc.glmnet(lasso.log.model, newx = X, newy = Y )
plot(rocs,type="l")  
```

```{r}
assess.glmnet(lasso.log.model,           
                newx = X,              
                newy = Y,
                family = "binomial",
                s = 'lambda.min'
              )    

print(glmnet_cv_aicc(lasso.log.model, 'lambda.min'))
print(glmnet_cv_aicc(lasso.log.model, 'lambda.1se'))
```

A closer look at the remaining 36 non-zero coefficients for the selected lambda value of lambda.min we can observe the `URBANICITY Highly Urban/ Urban` predictor variable has the largest impact on the prediction of a car crash by a factor of three.
Reviewing the top 5 predictor variables that impact likelihood and cost associated with an accident:

-   `URBANICITY Highly Urban/ Urban` - working or living in an urban neighborhood increase expected cost associated with a crash
-   `CAR_USE Private` - using a car for private activities reduces
-   `JOB Manager` - being a manager reduces the expected costs associated with a crash
-   `REVOKED Yes` - a history of having a revoked license increases the expected costs associated with a crash
-   `JOB Doctor` - being a doctor reduces the expected costs associated with a crash

The JOBStudent coefficient is the only predictor variable that drops out, however several variable including INCOME, HOME_VAL and OLDCLAIM are srunk substantially.

```{r}
coef(lasso.log.model, s = "lambda.min" )
```

```{r fig.show="hold", out.width="50%", fig.height=6}
vip(lasso.log.model, num_features=20 ,geom = "col", include_type=TRUE, lambda = "lambda.min")

coeffs.table <- coeff2dt(fitobject = lasso.log.model, s = "lambda.min")

coeffs.table %>% mutate(name = fct_reorder(name, desc(coefficient))) %>%
ggplot() +
  geom_col(aes(y = name, x = coefficient, fill = {coefficient > 0})) +
  xlab(label = "") +
  ggtitle(expression(paste("Lasso Coefficients with ", lambda, " = 0.0275"))) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust=0.5),legend.position = "none")
```

#### Model Performance

The coefficients extracted at the lambda.min value are used to predict the likelihood of an accident.
The confusion matrix highlights an accuracy of 73.7%.

```{r}
# drop TARGET_AMT
t0_df <- valid_data %>% dplyr::select(-c(TARGET_AMT))

# build X matrix and Y vector
X_test <- model.matrix(TARGET_FLAG ~ ., data=t0_df)[,-1]
Y_test <- t0_df[,"TARGET_FLAG"] 

# predict using coefficients at lambda.min
lassoPred <- predict(lasso.log.model, newx = X_test, type = "response", s = 'lambda.min')

pred_log_df <- valid_data %>% drop_na()
pred_log_df$TARGET_FLAG_PROB <- lassoPred[,1]
pred_log_df$TARGET_FLAG_PRED <- ifelse(lassoPred > 0.5, 1, 0)[,1]
```

```{r}
confusion.glmnet(lasso.log.model, newx = X_test, newy = Y_test, s = 'lambda.min')
```

#### Model Assumptions

Again we check linear relationship between independent variables and the Logit of the target variable.
Visually inspecting the results there is a linear trend in the relationship but there are deviations from the straight line in all variables.
The lasso regression solves the multicollinearity issue by selecting the variable with the largest coefficient while setting the rest to (nearly) zero.

```{r}
pred_log_df <- pred_log_df %>%
  mutate(logit = log(TARGET_FLAG_PROB/(1-TARGET_FLAG_PROB))) 

m_df <- pred_log_df %>% dplyr::select(where(is.numeric)) %>% pivot_longer(!c(TARGET_AMT,TARGET_FLAG_PROB,TARGET_FLAG_PRED,logit), 
                                 names_to='variable' , values_to = 'value')

# Scatter plot
ggplot(m_df, aes(logit, value))+
  geom_point(size = 0.5, alpha = 0.5) +
  geom_smooth(method = "loess") + 
  theme_bw() + 
  facet_wrap(~variable, scales = "free_y")

# Scatter plot 2
pred_log_df$index <- as.numeric(rownames(pred_log_df))
pred_log_df$resid <- pred_log_df$TARGET_FLAG_PROB - pred_log_df$TARGET_FLAG    

ggplot(pred_log_df, aes(index, resid)) + 
  geom_point(aes(color = TARGET_FLAG), alpha = .5) +
  theme_bw()
```



### 4.4 Model selection

```{r}


results_lm_tbl <- tibble(
                      Model = character(),
                      mape = numeric(), 
                      smape = numeric(), 
                      mase = numeric(), 
                      mpe = numeric(), 
                      rmse = numeric(),
                      AIC = numeric()
                )

results_log_tbl <- tibble(
                      Model = character(),
                      Accuracy = numeric(), 
                      "Classification error rate" = numeric(),
                      F1 = numeric(),
                      Deviance= numeric(), 
                      R2 = numeric(),
                      Sensitivity = numeric(),
                      Specificity = numeric(),
                      Precision = numeric(),
                      AIC= numeric()
                )

```

\*\* Model 3.1: Lasso Linear Regression \*\*

```{r}

# build X matrix and Y vector
t0_df <- valid_data %>% dplyr::select(-c(TARGET_FLAG)) %>% drop_na()
X_test <- model.matrix(TARGET_AMT ~ ., data=t0_df)[,-1]
Y_test <- t0_df[,"TARGET_AMT"] 



lasso.r1 <- assess.glmnet(lasso.lm.model,
                                newx = X_test,
                                newy = Y_test )
lasso.r2 <- glmnet_cv_aicc(lasso.lm.model, 'lambda.min')

```

```{r eval=FALSE}
multi_metric <- metric_set(mape, smape, mase, mpe, rmse)
m_df <- pred_ln_df %>% multi_metric(truth=TARGET_AMT, estimate=TARGET_AMT_PRED)


```

```{r}
results_lm_tbl <- results_lm_tbl %>% add_row(tibble_row(
                      Model = "M4.1:Lasso Linear",
                      mape = m_df[[1,3]],
                      smape = m_df[[2,3]],
                      mase = m_df[[3,3]],
                      mpe = m_df[[4,3]],
                      rmse = m_df[[5,3]],
                      AIC = lasso.r2$AICc)
                     )


```

\*\* Model 4.1: Lasso Logistic Regression \*\*

```{r}

conf_matrix_1 = confusionMatrix(factor(pred_log_df$TARGET_FLAG_PRED),factor(pred_log_df$TARGET_FLAG), "1")
conf_matrix_1$byClass["F1"]



# build X matrix and Y vector
t0_df <- valid_data %>% dplyr::select(-c(TARGET_AMT)) %>% drop_na()
X_test <- model.matrix(TARGET_FLAG ~ ., data=t0_df)[,-1]
Y_test <- t0_df[,"TARGET_FLAG"] 


lasso.r1 <- assess.glmnet(lasso.log.model,           
                                newx = X_test,              
                                newy = Y_test,
                                family = "binomial",
                                s = 'lambda.min'
                          )   
lasso.r2 <- glmnet_cv_aicc(lasso.log.model, 'lambda.min')



results_log_tbl <- results_log_tbl %>% add_row(tibble_row(
                      Model = "M4.1:Lasso Logistic", 
                      Accuracy=lasso.r1$auc[1], 
                      "Classification error rate" = 1 - lasso.r1$auc[1],
                      F1 = conf_matrix_1$byClass["F1"],
                      Deviance=lasso.r1$deviance[[1]], 
                      R2 = NA,
                      Sensitivity = conf_matrix_1$byClass["Sensitivity"],
                      Specificity = conf_matrix_1$byClass["Specificity"],
                      Precision = conf_matrix_1$byClass["Precision"],
                      AIC= lasso.r2$AICc)
                     )



```

Regression

```{r}

#t1 <- as_tibble(cbind(variables = names(results_tbl), t(results_tbl)))

kable(results_lm_tbl, digits=4) %>% 
  kable_styling(bootstrap_options = "basic", position = "center")

```

Logistic

```{r}


#t1 <- as_tibble(cbind(variables = names(results_tbl), t(results_tbl)))

kable(results_log_tbl, digits=4) %>% 
  kable_styling(bootstrap_options = "basic", position = "center")

```

## 5. Predictions

## 6. Conclusion

## 7. References

## Appendix: R code