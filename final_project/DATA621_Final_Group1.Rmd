---
title: 'Final: Texas Higher Education Opportunity Project (THEOP)'
subtitle: 'Critical Thinking Group 1'
author: 'Ben Inbar, Cliff Lee, Daria Dubovskaia, David Simbandumwe, Jeff Parks'
output:
  html_document:
    toc: yes
    toc_float: yes
    theme: united
  pdf_document:
    toc: yes
editor_options:
  chunk_output_type: console
  markdown: 
    wrap: sentence
---

```{r setup, include=FALSE}
# chunks
knitr::opts_chunk$set(echo=FALSE, eval=TRUE, include=TRUE, 
message=FALSE, warning=FALSE, fig.height=5, fig.align='center')

# libraries
library(tidyverse)
library(kableExtra)
library(MASS) # glm.nb()
library(mice)
library(pscl) # zeroinfl()
library(skimr)
library(sjPlot)
library(mpath)
library(yardstick)
library(labelled)
library(haven)
library(corrplot)
library(Hmisc)
library(jtools)
library(caret)
library(broom)

 # library
library(treemap)
library(ggplot2)
library(hrbrthemes)
library(viridis)
library(vip)

library(ggpubr)



# ggplot
theme_set(theme_light())


# random seed
set.seed(42)
```




```{r common functions}

#' nice_table
#' 
#' @param df
#' @param fw
nice_table <- function(df, cap=NULL, cols=NULL, dig=3, fw=F){
  if (is.null(cols)) {c <- colnames(df)} else {c <- cols}
  table <- df %>% 
    kable(caption=cap, col.names=c, digits=dig) %>% 
    kable_styling(
      bootstrap_options = c("striped", "hover", "condensed"),
      html_font = 'monospace',
      full_width = fw)
  return(table)
}

model_diag <- function(model){
  model_sum <- summary(model)
  aic <- AIC(model)
  ar2 <- model_sum$adj.r.squared
  disp <- sum(resid(model,'pearson')^2)/model$df.residual
  loglik <- logLik(model)
  
  vec <- c(ifelse(is.null(aic), NA, aic),
           ifelse(is.null(ar2), NA, ar2),
           ifelse(is.null(disp), NA, disp),
           ifelse(is.null(loglik), NA, loglik))
  
  names(vec) <- c('AIC','Adj R2','Dispersion','Log-Lik')
  return(vec)
}



factor_haven <- function(df, col_lst) {
  for (c in col_lst) {
    df[c] <- as_factor(zap_missing(df[c]))
  }
  return(df)
}


#' Title
#'
#' @param fit
#' @param lambda
#'
#' @return
#' @export
#'
#' @examples
glmnet_cv_aicc <- function(fit, lambda = 'lambda.1se'){
  whlm <- which(fit$lambda == fit[[lambda]])
  with(fit$glmnet.fit,
       {
         tLL <- nulldev - nulldev * (1 - dev.ratio)[whlm]
         k <- df[whlm]
         n <- nobs
         return(list('AICc' = - tLL + 2 * k + 2 * k * (k + 1) / (n - k - 1),
                     'BIC' = log(n) * k - tLL))
       })
}



```

# Abstract 

<i>This work examines changes in the racial and ethnic composition of admissions at Texas A&M college following the 1996 Hopwood case, which put a court prohibition on affirmative action. We estimate the extent to which these universities used affirmative action before the ban, and we examine how admissions officers at these universities adjusted the relative weights given to key applicant criteria throughout the suspension. We model the extent to which these new regulations succeeded in preserving minority admission rates at pre-Hopwood levels after examining whether changes in relative weights favored minority applicants. We discovered that most colleges followed the Hopwood rule, so that direct advantages offered to black and Hispanic candidates vanished (and, in some circumstances, became disadvantages). While there is evidence that universities changed the weights they placed on applicant characteristics other than race and ethnicity in ways that aided underrepresented minority applicants, these changes in the admissions process were unable to maintain the share of admitted students held by black and Hispanic applicants. As a result, these alternative admissions procedures have not been a reliable proxy for race and ethnicity.

</i>


# Introduction

Despite some claims, people have acknowledged a gap in access to higher education still exists, especially for minority students. However, many have objected to affirmative action policies as a solution, and in 1996 in Texas, the Fifth Court Circuit of Appeals outlawed the use of race for college admissions in the state. After a noticeable drop in minority student enrollments, the governor of Texas signed a new bill as a compromise:  any high school student finishing in top decile received a guaranteed admission to Texas public university (Texas A&M University). Instead of competing with applicants from the entire state, students only had to compete with their immediate classmates to access higher education.

# Literature review
<i>Discuss how other researchers have addressed similar problems, what their achievements are, and what the advantage and drawbacks of each reviewed approach are. Explain how your investigation is similar or different to the state-of-the-art. Please cite the relevant papers where appropriate. </i>

# Methodology 
<i>Discuss the key aspects of your problem, data set and regression model(s). Given that you are working on real-world data, explain at a high-level your exploratory data analysis, how you prepared the data for regression modeling, your process for building regression models, and your model selection.</i> 



# Data

```{r}

# change to your local data dir outside the repo
#local_data_dir <- 'C:/Users/daria/Documents/theop'
local_data_dir <- '../../data/theop'


# # load application and transactions data frames
load(paste0(local_data_dir, '/data_model/df_applications.RData'))
load(paste0(local_data_dir, '/data_model/df_transcripts.RData'))

```






```{r}
# clean application data and remove labels
app_df  <- df_applications

col_lst <- c("termdes","male","ethnic","citizenship","restype","satR","actR","testscoreR","decileR","quartile","major_field","hsprivate","hstypeR","hsinstate","hseconstatus","hslos","hscentury","admit","admit_prov","enroll","gradyear","studentid_uniq","univ","termapp","sat_not_recenteredR","admit_ut_summer")

app_df <- factor_haven(app_df,col_lst)

# clean transcripts and remove labels
transcript_df <- df_transcripts

col_lst <- c("term","hrearn","term_major_dept","term_major_field")

transcript_df <- factor_haven(transcript_df,col_lst)

```


```{r}

## Choose Texas A&M college and compare the admissions policy at a university before and after top 10%. Make variables categorical if needed.
texas_applications <- filter(app_df, univ == "am") %>% dplyr::select(!c(termapp, sat_not_recenteredR, admit_ut_summer, univ)) %>% 
        drop_na(satR,decileR)
#texas_transcripts <- filter(transcript_df, univ == "am") %>% dplyr::select(!univ)
```


```{r}

## Dummy Variables for factors with two levels
dummy_vars <- function(df){
  df %>%
    mutate(
      male = factor(ifelse(male == "Male", 1, 0)), 
      US_Citizen = factor(ifelse(citizenship == "US Citizen",  1, 0)),
      Texas_resident = factor(ifelse(restype == "Texas Resident",  1, 0)),
      admit = factor(ifelse(admit == "Yes",  1, 0)),
      admit_prov = factor(ifelse(admit_prov == "Yes",  1, 0)),
      enroll = factor(ifelse(enroll == "Yes",  1, 0))
    ) %>% 
    dplyr::select(-c(citizenship,restype))
      }

texas_applications <- dummy_vars(texas_applications)


texas_applications$satR <- as.numeric(texas_applications$satR)
texas_applications$actR <- as.numeric(texas_applications$actR)
texas_applications$testscoreR <- as.numeric(texas_applications$testscoreR)
texas_applications$gradyear <- as.numeric(texas_applications$gradyear)

```



We employ administrative records from a Texas A&M University (1992-2022) that include info about admissions selectivity, public/private status, and the ethno-racial composition of their student body for this analysis. Importantly, the time period for the public organization comprises years prior to and following the judicial ban on affirmative action. This is significant because, while the judicial restriction applied to all schools in the 5th Circuit District, the top 10% policy was restricted to public colleges and universities. 
These records contain a plethora of information about the applicant pool, and have been standardized where necessary, and checked for consistency.

The **application** dataset contained 163,027 observations of 24 predictor variables and **transcripts** dataset contained 637,028 observations of 10 predictor variables, where each record represented an individual applicant.
These variables describe items typically found on a college admission application such as the year and term an applicant desired to enroll, applicant 
demographics, applicant academic characteristics, and high school characteristics.

Unfortunately, the data does not often include information regarding a student's high school academics or application essays. In evaluating the results, we take great note of these data constraints.



<img src="https://raw.githubusercontent.com/cliftonleesps/data621_group1/main/final_project/project_data.png" width="1200" style="display: block; margin-left: auto; margin-right: auto; width: 100%;"/>

Each record in the application dataset included a response variable "admit" (Institutionâ€™s admission decision), was a boolean where "1" indicated the person was admitted.

A sample of the data appears in the following table:

```{r}
DT::datatable(
      texas_applications[1:25,],
      extensions = c('Scroller'),
      options = list(scrollY = 350,
                     scrollX = 500,
                     deferRender = TRUE,
                     scroller = TRUE,
                     dom = 'lBfrtip',
                     fixedColumns = TRUE, 
                     searching = FALSE), 
      rownames = FALSE) 
```


While exploring this data, we made the following observations for the **applications data**:

-   21 variables were categorical, and 3 were numeric.
-   `actR`, `gradyear`, `hscentury`, `hslos` variables had more that 50% of the missing data.


```{r}

texas_applications %>% 
  skim() %>%
  dplyr::select(skim_variable, complete_rate, n_missing, 
                numeric.p0, numeric.p100) %>%
  dplyr::rename(variable=skim_variable, min=numeric.p0, max=numeric.p100) %>%
  mutate(complete_rate=round(complete_rate,2), 
         min=round(min,2), max=round(max,2)) %>%
  arrange(variable) %>%
  nice_table()
```


```{r}
#remove data with NA more than 50%
texas_applications <- texas_applications %>% dplyr::select(-c(actR,gradyear,hscentury,hseconstatus,hslos))

```



76% of all the students were admitted to the university.
```{r}
texas_applications%>%
  ggplot() +
  geom_bar(aes(x=admit), colour='black', size=0.2) 
```

By checking the frequency of the variables with few unique values ("male","ethnic", "citizenship", "restype","decileR","major_field"), we checked the frequency of each value.
As demonstrated by the graph below, more than 50% of the data for the `citizenship`, `restype` and `ethnic` variables was `US Citizen`, `Texas Resident` and `White, Non-Hispanic` accordingly:

```{r discrete_plot}
texas_applications[,c("admit","male", "US_Citizen", "Texas_resident","admit_prov","enroll")] %>%
  gather("variable","value") %>%
  group_by(variable) %>%
  count(value) %>%
  mutate(value = factor(value,levels=2:0)) %>%
  mutate(percent = n*100/134020) %>%
  ggplot(.,
  aes(variable,percent)) +
  geom_bar(stat = "identity", aes(fill = value)) +
  xlab("Variable") +
  ylab("Percentage") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  scale_fill_manual(values = rev(c("#003f5c","#58508d", "#bc5090")))
```

```{r eval=FALSE}
# a <- ggplot(texas_transcripts, aes(x = cgpa)) +
#   geom_histogram(binwidth = 1, fill = "skyblue", color = "black") +
#   labs(x = "Per semester cumulative gpa", y = "Count") 
# 
# 
# b <- ggplot(texas_transcripts, aes(x = semgpa)) +
#   geom_histogram(binwidth = 1, fill = "skyblue", color = "black") +
#   labs(x = "Semester gpa", y = "Count")
# 
# plot<- ggarrange(a, b, ncol=2, nrow=1)
# 
# annotate_figure(plot, top = text_grob("Distribution of GPA", 
#                color = "black", face = "bold", size = 14))
```

```{r eval=FALSE}
# m_df <- texas_transcripts %>% dplyr::select(c(semgpa,cgpa)) %>% melt() 
# 
# m_df %>% ggplot(aes(x= value)) + 
# geom_histogram(color='#023020', fill='gray') + 
#   facet_wrap(~variable, scales = 'free',  ncol = 4) + theme_bw()

```



```{r}
a <- ggplot(texas_applications, aes(x = quartile)) +
  geom_bar(binwidth = 1, fill = "skyblue", color = "black") +
  labs(x = "Quartile", y = "Count")+ theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

b <- ggplot(texas_applications, aes(x = decileR)) +
  geom_bar(binwidth = 1, fill = "skyblue", color = "black") +
  labs(x = "Decile", y = "Count") + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))


plot<- ggarrange(a, b, ncol=2, nrow=1)

annotate_figure(plot, top = text_grob("Distribution of Student HS class rank", 
               color = "black", face = "bold", size = 14))
```



# Transform Data
Students with missing values for `ethnic` variable are combined with  "White, Non-Hispanic" students, resulting in cautious estimates of policy effects.

```{r message=FALSE, warning=FALSE,}

levels(texas_applications$hsprivate) <- c(levels(texas_applications$hsprivate),"None")
texas_applications$hsprivate[is.na(texas_applications$hsprivate)] <- "None"

levels(texas_applications$hsinstate) <- c(levels(texas_applications$hsinstate),"None")
texas_applications$hsinstate[is.na(texas_applications$hsinstate)] <- "None"

texas_applications <- texas_applications %>% mutate(ethnic = ifelse(is.na(ethnic), "5", ethnic))
texas_applications$ethnic <- as.factor(texas_applications$ethnic)

# set values, dummy variable if top 10% or not
texas_applications <- texas_applications %>% mutate(top10 = ifelse(decileR == "Top 10%", 1, 0))
texas_applications$top10 <- as.factor(texas_applications$top10)

levels(texas_applications$decileR) <- c(levels(texas_applications$decileR),"None")
texas_applications$decileR[is.na(texas_applications$decileR)] <- "None"


levels(texas_applications$quartile) <- c(levels(texas_applications$quartile),"None")
texas_applications$decileR[is.na(texas_applications$decileR)] <- "None"


#remove 52 observations where male=NA
texas_applications <- texas_applications %>%                                        
  filter(!is.na(male))

```



```{r message=FALSE, warning=FALSE,}
# filter 
attr_str <- c('admit', 'termdes', 'male','ethnic','US_Citizen','Texas_resident','satR','testscoreR',
              'top10','hsprivate','hsinstate','yeardes', 'enroll')

am_df <- texas_applications %>% dplyr::select(attr_str)


```



```{r message=FALSE, warning=FALSE,}

am_df %>% 
  skim() %>%
  dplyr::select(skim_variable, complete_rate, n_missing, 
                numeric.p0, numeric.p100) %>%
  dplyr::rename(variable=skim_variable, min=numeric.p0, 
                max=numeric.p100) %>%
  mutate(complete_rate=round(complete_rate,2), 
         min=round(min,2), max=round(max,2)) %>%
  arrange(variable) %>%
  nice_table()

```


```{r message=FALSE, warning=FALSE,}
#m_df <- merge_df %>% slice_sample(n=2000) %>%
m_df <- am_df %>%
  dplyr::select(where(is.numeric) & !c('admit','enroll')) %>% 
  pivot_longer(!c('satR'), names_to='variable' , values_to = 'value') %>% 
  drop_na()

m_df %>% ggplot(aes(x=value)) + 
#m_df %>% ggplot(aes(x=value, group=avg_gpa, fill=avg_gpa)) + 
geom_density(color='#023020') + facet_wrap(~variable, scales = 'free',  ncol = 4) + theme_bw()

```


```{r message=FALSE, warning=FALSE,}
m_df <- am_df %>%
  dplyr::select((where(is.numeric) | c('admit','enroll')) & !c(yeardes)) %>% 
  pivot_longer(!c('admit','enroll'), names_to='variable' , values_to = 'value') %>% 
  drop_na()

m_df %>% ggplot(aes(y=value, x=admit, fill=enroll)) + 
#m_df %>% ggplot(aes(x=value, group=TARGET_FLAG, fill=TARGET_FLAG)) + 
geom_boxplot(color='#023020') + facet_wrap(~variable, scales = 'free',  ncol = 4) + theme_bw()


# m_df %>% ggplot(aes(y=value, x=enroll)) + 
# #m_df %>% ggplot(aes(x=value, group=TARGET_FLAG, fill=TARGET_FLAG)) + 
# geom_boxplot(color='#023020') + facet_wrap(~variable, scales = 'free',  ncol = 4) + theme_bw()

```


```{r message=FALSE, warning=FALSE,}

#m_df <- am_df 

m_df <- am_df %>%
  group_by(admit, ethnic, male) %>%
  summarise(n = n())

#treemap(m_df, index=c("ethnic","admit"), vSize="n", type="index")
treemap(m_df, index=c("admit","ethnic"), vSize="n", type="index",
      title="My Treemap",                      # Customize your title
      fontsize.title=12,
      align.labels=list(
        c("center", "center"), 
        c("right", "bottom")
        ),                                   # Where to place labels in the rectangle?
    overlap.labels=0.5,                      # 
    inflate.labels=F, )


```

```{r message=FALSE, warning=FALSE,}

m_df <- am_df %>%
  group_by(admit, ethnic, male) %>%
  summarise(n = n())

# plot
ggplot(m_df, aes(fill=ethnic, y=n, x=admit)) + 
  geom_bar(position="stack", stat="identity") + 
  scale_fill_viridis(discrete=TRUE, name="") +
  facet_wrap(~admit, scales = 'free',  ncol = 4)
  theme_ipsum() +
  ylab("Money input") + 
  xlab("Month")



```

# Models
As the first step, we built the generalized linear model based on the dataset with some variables transformed to categorical from numeric for the time before Fall, 1998.

```{r}
#no termdes=Fall or summer II only
pre_df <- am_df %>% dplyr::filter(yeardes < 1998)
b <- am_df %>% dplyr::filter(yeardes == 1998 & termdes=="Summer II")

pre_df <- rbind(pre_df,b)
```

```{r}
pre_df %>%
  #dplyr::select(!c(studentid)) %>%
  skim() %>%
  dplyr::select(skim_variable, complete_rate, n_missing, 
                numeric.p0, numeric.p100) %>%
  rename(variable=skim_variable, min=numeric.p0, max=numeric.p100) %>%
  mutate(complete_rate=round(complete_rate,2), 
         min=round(min,2), max=round(max,2)) %>%
  arrange(variable) %>%
  nice_table()

```

Since our dependent variable is binary (0 and 1), we used logistic regression. To do so, the function `glm()` with `family=binomial` was used. At the beginning, variable below were included.
```{r}

am_model_1 <- glm(pre_df, formula = admit ~ . -enroll -yeardes -termdes, family = binomial(link = "probit"))
summary(am_model_1)

```
Using StepAic() function helped to improve the model performance.
```{r}

am_model_1_aic <- am_model_1 %>% stepAIC(trace = FALSE)
summ(am_model_1_aic)

```


## Model Results


As it appeared, `ethnic` variable had the most negative effect for the university acceptance. While being Texas resident or being in top 10% could help to be admitted.

The `null deviance` of `r round(am_model_1_aic$null.deviance,2)` defined how well the target variable could be predicted by a model with only an intercept term.

The `residual deviance` of `r round(am_model_1_aic$deviance,2)` defined how well the target variable could be predicted by the AIC model that we fit with the predictor variables listed above.
The lower the value, the better the model's predictions of the response variable.

The p-value associated with this `Chi-Square Statistic` was 0 (less than .05), so the model could be useful.

The Akaike information criterion (`AIC`) was `r round(am_model_1_aic$aic,2)`.
The lower the AIC value, the better the model's ability to fit the data.


#### Checking Model Assumptions

The resulting plots show us similar to model 1 picture: under-fitting at lower predicted values, with the predicted proportions being larger than the observed proportions; over-fitting for a couple of predictor patterns at higher predicted values with the predicted values are much larger than the predicted proportions.
The Standardized Pearson Residuals plot shows an approximate Standard Normal distribution if the model fits.  The model seems good in the middle range but there are extremes on the right and left sides. Some normality of the residuals for the binomial logistic regression models  is just an evidence of a decent fitting model.

```{r check_lm2}
par(mfrow=c(2,2))
plot(am_model_1_aic)
```

By checking the linearity assumptions, we see that only `rm` seem like linear relation with the logit results.
```{r model_2_linearity}
probabilities <- predict(am_model_1_aic, type = "response")
predicted.classes <- ifelse(probabilities > 0.5, "1", "0")
head(predicted.classes)

#Only numeric predictors
data <- pre_df %>%
  dplyr::select_if(is.numeric) 
predictors <- colnames(data)

# Bind the logit and tidying the data for plot
data <- data %>%
  mutate(logit = log(probabilities/(1-probabilities))) %>%
  gather(key = "predictors", value = "predictor.value", -logit)

```

The Standardized Residuals plot seems to have a constant variance though there are some outliers.
```{r model_2_residuals}
am_model_1_aic.data <- augment(am_model_1_aic) %>% 
  mutate(index = 1:n())

ggplot(am_model_1_aic.data, aes(index, .std.resid)) + 
  geom_point(aes(color = admit), alpha = .5) +
  theme_bw()
```

The marginal model plots below show reasonable agreement across the two sets of fits indicating that model_2_aic is a valid model.

```{r warning=FALSE, message=FALSE}
car::mmps(am_model_1_aic, span = 3/4, layout = c(2, 2))
```

In terms of multicollinearity, all variables have a VIF less than 5. As a result, multicollinearity shouldn't be a problem for our model.
```{r model_2_vif}
car::vif(am_model_1_aic)
```



# Predict

Once the uniform admission law was fully in force, fall, 1998.
```{r}

am1998_df <- am_df %>% dplyr::filter(yeardes == 1998 & termdes=="Fall")
```

```{r}

am1998_pred <- predict.glm(am_model_1_aic, am1998_df, "response")

am1998_df$prob_admit <- am1998_pred
am1998_df$pred_admit <- ifelse(am1998_pred >= 0.5, 1, 0)


```


```{r}

log_matrix_1 <- confusionMatrix(factor(am1998_df$pred_admit), 
                                factor(am1998_df$admit), "1")

log_matrix_1


```

1999
```{r}


am1999_df <- am_df %>% dplyr::filter(yeardes == 1999)

am1999_pred <- predict.glm(am_model_1_aic, am1999_df, "response")

am1999_df$prob_admit <- am1999_pred
am1999_df$pred_admit <- ifelse(am1999_pred >= 0.5, 1, 0)


```


```{r}
log_matrix_2 <- confusionMatrix(factor(am1999_df$pred_admit), 
                                factor(am1999_df$admit), '1')

log_matrix_2
```


2000
```{r}


am2000_df <- am_df %>% dplyr::filter(yeardes == 2000)

am2000_pred <- predict.glm(am_model_1_aic, am2000_df, "response")

am2000_df$prob_admit <- am2000_pred
am2000_df$pred_admit <- ifelse(am2000_pred >= 0.5, 1, 0)


```


```{r}
log_matrix_3 <- confusionMatrix(factor(am2000_df$pred_admit), 
                                factor(am2000_df$admit), '1')

log_matrix_3
```


2001

```{r}
am2001_df <- am_df %>% dplyr::filter(yeardes == 2001)

am2001_pred <- predict.glm(am_model_1_aic, am2001_df, "response")

am2001_df$prob_admit <- am2001_pred
am2001_df$pred_admit <- ifelse(am2001_pred >= 0.5, 1, 0)


```


```{r}
log_matrix_4 <- confusionMatrix(factor(am2001_df$pred_admit), 
                                factor(am2001_df$admit), '1')

log_matrix_4
```

2002
```{r}
am2002_df <- am_df %>% dplyr::filter(yeardes == 2002)

am2002_pred <- predict.glm(am_model_1_aic, am2002_df, "response")

am2002_df$prob_admit <- am2002_pred
am2002_df$pred_admit <- ifelse(am2002_pred >= 0.5, 1, 0)


```


```{r}
log_matrix_5 <- confusionMatrix(factor(am2002_df$pred_admit), 
                                factor(am2002_df$admit), '1')

log_matrix_5
```


# Lasso???


```{r}
# set seed for consistancy 
set.seed(42)

# build X matrix and Y vector
X <- model.matrix(admit ~ . -enroll -yeardes -termdes , data=pre_df)[,-1]
Y <- pre_df[,"admit"] 

```


```{r}

lasso.model<- cv.glmnet(x=X,y=Y,
                       family = "binomial", 
                       link = "logit",
                       standardize = TRUE,                       #standardize  
                       nfold = 5,
                       alpha=1)                                  #alpha=1 is lasso

l.min <- lasso.model$lambda.min
l.1se <- lasso.model$lambda.1se
coef(lasso.model, s = "lambda.min" )
coef(lasso.model, s = "lambda.1se" )
lasso.model

```


```{r}

par(mfrow=c(2,2))

plot(lasso.model)
plot(lasso.model$glmnet.fit, xvar="lambda", label=TRUE)
plot(lasso.model$glmnet.fit, xvar='dev', label=TRUE)

rocs <- roc.glmnet(lasso.model, newx = X, newy = Y )
plot(rocs,type="l")  

```


```{r}

```


```{r}
assess.glmnet(lasso.model,           
              newx = X,              
              newy = Y )    

print(glmnet_cv_aicc(lasso.model, 'lambda.min'))
print(glmnet_cv_aicc(lasso.model, 'lambda.1se'))

```


```{r}
coef(lasso.model, s = "lambda.min" )
vip(lasso.model, num_features=20 ,geom = "col", include_type=TRUE, lambda = "lambda.min")
```

#### Model Results


The coefficients extracted at the lambda.min value are used to predict the relative crime rate for the testing data set. The confusion matrix highlights an accuracy of 91%.
```{r eval=FALSE}
# Create matrix new data
#X <- model.matrix(admit ~ . -enroll -yeardes -termdes , data=pre_df)[,-1]
X_test <- model.matrix(admit ~ . -enroll -yeardes -termdes, data=am1998_df)[,-1]
Y_test <- am1998_df[,"admit"] 


# predict using coefficients at lambda.min
lassoPred <- predict(lasso.model, newx = X_test, type = "response", s = 'lambda.min')

pred_df <- am1998_df
pred_df$admit_prob <- lassoPred[,1]
pred_df$admit_pred <- ifelse(lassoPred >= 0.5, 1, 0)[,1]

```


```{r eval=FALSE}

confusion.glmnet(lasso.model, newx = X_test, newy = Y_test, s = 'lambda.min')

```
















```{r eval=FALSE}
### Merge dataframes.

# Check for duplicates in applications and transcripts.
# For applications there were duplicates for the desired year of admission, so we took the minimum desired year.
# For transcripts, make sure there are no duplicates for the same semester.
# We also limited to enrollment at the applied school to track progress only at schools of enrollment.

#we are working with applications data only, we don't need transcripts

#tabyl(app_df$termdes)
#dupes <- get_dupes(app_df, studentid_uniq, enroll)

#app_df <- app_df |>
 # mutate(termdes_num = case_when(termdes=='Spring' ~ 1
                            #     , termdes=='Summer I' ~ 2
                             #    , termdes=='Summer II' ~ 3
                              #   , termdes=='Fall' ~ 4)) |>
 # group_by(studentid_uniq) |>
 # filter(yeardes==min(yeardes)) |>
 # filter(termdes_num==min(termdes_num)) |>
 # filter(enroll=="Yes") |>
 # ungroup()
  
#dupes <- get_dupes(transcript_df, studentid_uniq, year, term)

#transcript_df <- transcript_df |>
  #distinct()

# Join them together
#all <- app_df |>
 # left_join(transcript_df, by=c('studentid_uniq', 'univ'))
  
# tabyl(all$ethnic)
# tabyl(all$univ.x)
# tabyl(all$decileR)
# tabyl(all$gradyear)
# table(all$univ.y, all$admit)
# table(all$univ.x, all$univ.y)
# tabyl(all$admit_prov)
# tabyl(all$year)

#rm(transcript_df, app_df)

```





```{r eval=FALSE}



### Cleaning

# Since the goal was to determine whether the 10% rule was predictive of retention (total number of semesters), and performance (cumulative GPA), to make this easier we widened the data and removed the semester-based tabulation in favor of cumulative.
# 
# 1.  Create cumulative fields for `hrearn` and total numbers of semesters.
# 2.  Drop `gpahrs` since this was only available for one school, Texas A&M Kingsville.
# 3.  Drop several other variables:
#     -   `studentid` since we generated a unique id with studentid and school abbreviation.
#     -   `yeardes` year of admission desired, since intuitively this would be very hard to interpret as a predictor.
#     -   `satR` and `actR` since these were already coalesced in the `testscoreR` column.
#     -   `termapp` application term since we wanted to be timing-agnostic.
#     -   `termdes_num` since we already had the variable represented as `termdes`
#     -   `semgpa` semester by semester GPA, but we were only interested in cumulative.
#     -   `gradyear` year of graduation as again, we wanted to be timing-agnostic.

# 
# cleaned <- all |>
#   dplyr::select(-c(studentid.x, studentid.y, yeardes, gpahrs, satR, actR, termapp, termdes_num, semgpa)) |>
#   distinct() |>
#   mutate(termnum = case_when(term=='Spring' ~ 1
#                                  , term=='Summer I' ~ 2
#                                  , term=='Summer II' ~ 3
#                                  , term=='Fall' ~ 4)) |>
#   group_by(studentid_uniq) |>
#   mutate(cum_hrs = sum(as.numeric(hrearn)) - n(),
#          cum_terms = n(),
#          enrolled_pre_98 = case_when(year<=1998 ~ "Pre"
#                             , year>1998 ~ "Post"
#                             , T ~ "Unknown")) |>
#   filter(year==max(year)) |>    # To get maximum enrollment
#   filter(termnum==max(termnum)) |>  # ... and maximum cgpa.
#   filter(!(enrolled_pre_98=="Unknown")) |>  # Drop cases without term years as we do not know when this would be as it relates to the TX law.
#   ungroup() |>
#   dplyr::select(-c(year, term, hrearn, termnum)) |>
#   dplyr::select(studentid_uniq, everything())  # Just move studentid_uniq to left index.
# 
# 
# skim(cleaned)
# 
# cleaned <- as.data.frame(cleaned)
# cleaned.m <- melt(cleaned)
# 
# cleaned.m %>% ggplot(aes(x=value)) +
#   geom_density(color='#023020') +
#   facet_wrap(~variable, scales = 'free') +
#   theme_bw()

```


# Discussion and Conclusions 

<i>Conclude your findings, limitations, and suggest areas for future work. </i>

```{r}
knitr::knit_exit()
```


