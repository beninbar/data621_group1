---
title: 'Homework #3: Logistic Regression'
subtitle: 'Critical Thinking Group 1'
author: 'Ben Inbar, Cliff Lee, Daria Dubovskaia, David Simbandumwe, Jeff Parks, Nick Oliver'
output:
  html_document:
    toc: yes
    toc_float: yes
    theme: united
  pdf_document:
    toc: yes
editor_options:
  chunk_output_type: console
  markdown: 
    wrap: sentence
---


```{r setup, include=FALSE}
library(tidyverse)
library(dplyr)
library(reshape2)
library(ggplot2)
library(glmnet)
library(psych)
library(caret)
library(pROC)
library(skimr)
library(lemon)
library(DT)
library(kableExtra)
library(forecast)
library(corrplot)
library(ggpubr)
library(Hmisc)
library(caTools)
library(performance)
library(jtools)
library(regclass)
library(MASS)
library(PerformanceAnalytics)
library(leaps)
source("logistic_functions.R")

knitr::opts_chunk$set(echo = FALSE)
```


```{r}
set.seed(1233)
```




```{r data}

train_df <- read.csv("https://raw.githubusercontent.com/cliftonleesps/data621_group1/main/hw3/crime-training-data_modified.csv")
eval_df <- read.csv("https://raw.githubusercontent.com/cliftonleesps/data621_group1/main/hw3/crime-evaluation-data_modified.csv")

```

## Overview
People's quality of life consists many factors: available housing, economic viability, and geography among many others. One of the most import factors is security, which is directly related to the prevalence of crime. In large cities, crime negatively affects citizens' livelihood. Criminal activity prevents people from safely raising families, running businesses, owning property and providing tax revenue among other effects. And once crime appears in a locality, it becomes expensive and difficult to reverse its influence; city planners and public officials still do not agree on the best approaches despite many attempts and experiments.

This study focuses on crime detection and prediction. Specifically, using publicly data gathered from various Boston neighborhoods to predict whether each area has a crime rate above or below below the city median. The predictor variables describe an area's zoning, housing characteristics, and proximity to critical resources (e.g. employment or highways), among others.

In order to best assess our predictive model, we created a test set within our training data, and split it along an 80/20 training/testing proportion, before applying the finalized model to a separate evaluation dataset that did not contain the target.

After investigating a few models, a binary logistic regression performed the best with an AIC score of -321 when incorporating eight predictors, out of a possible twelve. The model achieved an accuracy rate of 86%. With this and similar models, city councils can devote resources and attention to neighborhoods with a higher crime risk as well as better plan future development.


<br></br>


## 1. Data Exploration

The crime training dataset contains 466 observations of 13 variables that are collected in attempts to evaluate the crime within the Boston neighborhoods. The evaluation dataset contains 40 observations of 12 variables.
The descriptions of each column are below.

<img src="https://raw.githubusercontent.com/cliftonleesps/data621_group1/main/hw3/data_variables.png" width="1200" style="display: block; margin-left: auto; margin-right: auto; width: 100%;"/>

### 1.1 Summary Statistics

First, let's take a look at the train data.
Our `target` variable takes two values: 0 an 1.

```{r summary1}
DT::datatable(
      train_df,
      extensions = c('Scroller'),
      options = list(scrollY = 350,
                     scrollX = 500,
                     deferRender = TRUE,
                     scroller = TRUE,
                     dom = 'lBfrtip',
                     fixedColumns = TRUE, 
                     searching = FALSE), 
      rownames = FALSE) 
```

The table below provides the mean for each variable, the standard deviation and some other descriptive statistics.
As we see below, `zn`, `indus`, `age`, and `lstat` are proportions. `chas` is a dummy variable. The rest are integers and fortunately, there are no missing values.

```{r summary}
skim(train_df)
```
<br></br>

### 1.2 Distribution
Next, we will check the density plots. It will help us to start thinking about which variables can be included in our model and how they are distributed relative to our target variable, so we graph both the total density and density per target category below.
Some distributions are skewed: left-skewed `age`, `ptratio` and right-skewed `dis`, `lstat`, `zn`. We may consider some transformations for these variables to deal with the linear regression assumptions.
The variables `tax` and `rad` appear binomial, and `chas` is binary and takes mostly the value of `0`. Likewise, `zn` also takes mostly the value of `0`, and thus may be worth removing.


```{r density_plot}

m_df <- train_df %>% pivot_longer(!target, names_to='variable' , values_to = 'value')
m_df %>% ggplot(aes(x=value, group=target, fill=target)) + 
geom_density(color='#023020') + facet_wrap(~variable, scales = 'free',  ncol = 4) + theme_bw()

m_df %>% ggplot(aes(x= value)) + 
geom_density(color='#023020', fill='gray') + facet_wrap(~variable, scales = 'free',  ncol = 4) + theme_bw()

```

Box plots indicate some outliers that we may wish to address.

```{r box_plot}

m_df %>% ggplot(aes(x=target, y=value, group=target)) + 
geom_boxplot(color='#023020', fill='gray') + facet_wrap(~variable, scales = 'free',  ncol = 4) +
  stat_summary(fun = "mean",  geom = "point", shape = 8, size = 2, color = "steelblue") + 
  stat_summary(fun = "median",  geom = "point", shape = 8, size = 2, color = "red") + theme_bw()

```

Before building a model, we need to make sure that we have both classes equally presented in out target variable. This appears to be the case, as class `1` takes 49% and class `0` takes 51% of the target variable. As a result, we don't have unbalanced class distribution for our target variable that we have to deal with. Otherwise, we had to take some additional steps (bootstrapping, etc) before using logistic regression.

```{r balance}
proportion <- colMeans(train_df['target'])
proportion
```

### 1.3 Correlation Matrix
Plotting the correlations between the `target` and the variables, we can see that some variables correlate *positively* with the target, such as `indus` (proportion of non-retail business acres), `nox` (nitrogen oxides concentration), `age` (proportion of units built prior to 1940), `rad` (accessibility to highways), `tax` (property tax rate per $10,000), `lstat` (lower status of population i.e. a socioeconomic measure), and *negatively* with `zn` (proportion of residential land zoned for large lots) and `dis` (mean distance to 5 Boston employment centers). Intuitively and based on well-studied socioeconomic principles, these make sense, barring perhaps the tax rate, which one would expect to be lower in higher crime-prone neighborhoods.

Variables with correlations close to zero are unlikely to offer significant insights into the factors contributing to a crime rate. 

To avoid multicollinearity, we should exclude some variables from the model that have high degrees of correlation with each other.
We can see potential cases of multicollinearity for the variables `rad`/`tax`, `indus`/`tax`, `age`/`nox`, `indus`/`nox`, `rm`/`medv`, `lstat`/`medv`, `nox`/`dis`, `indus`/`dis`, and `age`/`dis`.

We will carefully examine the VIF for these later on.

```{r corr}

rcore <- rcorr(as.matrix(train_df %>% dplyr::select(where(is.numeric))))
coeff <- rcore$r
corrplot(coeff, tl.cex = .7, tl.col="black", method = 'color', addCoef.col = "black",
         type="upper", order="hclust",
         diag=FALSE)

```

```{r corr_numbers}
tst <- train_df
tst <- tst[,13]
kable(cor(drop_na(train_df))[,13], "html", escape = F, col.names = c('Coefficient')) %>%
  kable_styling("striped", full_width = F) %>%
  column_spec(1, bold = T)
```



## 2. Data preparation

To build our model, we need to transform categorical variables `chas`, `target` and `rad` from numeric (default by the data) to factors. `rad` in particular is an ordinal categorical index.

```{r factor}
transformed_train_df <- train_df
transformed_train_df$chas <- as.factor(transformed_train_df$chas)
transformed_train_df$target <- as.factor(transformed_train_df$target)
transformed_train_df$rad <- as.factor(transformed_train_df$rad)
```


As mentioned above, the variable `zn` has 0 value as the most occurred. It appears 339 times out of total 466 (72%), and may cause overdispersion, so we will remove it.
```{r count_zn}
count(train_df,zn)

transformed_train_df <- subset(transformed_train_df, select = -c(zn))
```

```{r eval=FALSE}
# transformed_train_df$zn <- ifelse(transformed_train_df$zn == 0, 0, 1) 
# count(transformed_train_df,zn)
# transformed_train_df$zn <- as.factor(transformed_train_df$zn)
```


```{r log, warning=FALSE, message=FALSE}
# log_train_df <- transformed_train_df
# log_train_df$dis <- log(log_train_df$dis+1)
# log_train_df$medv <- log(log_train_df$medv+1)
# 
# log_train_df %>% dplyr::select(-c(chas, target, rad)) %>% skewness(na.rm=FALSE)
```

```{r log_hists, warning=FALSE, message=FALSE}
# m_df_box <- log_train_df %>% dplyr::select(c(dis, medv)) %>% melt() 
# 
# a <- m_df_box %>% ggplot(aes(x= value)) + 
# geom_density(color='#023020', fill='gray') + facet_wrap(~variable, scales = 'free',  ncol = 1) + theme_bw()
# 
# m_df <- transformed_train_df %>% dplyr::select(c(dis, medv)) %>% melt() 
# 
# b <- m_df %>% ggplot(aes(x= value)) + 
# geom_density(color='#023020', fill='gray') + facet_wrap(~variable, scales = 'free',  ncol = 1) + theme_bw()
# 
# ggarrange(a, b + rremove("x.text"), 
#           labels = c("Transformed", "Original"),
#           ncol = 2, nrow = 1)
```


Next, let's use BoxCox transformation to normalize our variables - we will look at the distributions for all: `age`, `dis`, `lstat`, `nox`, `ptratio`, `tax`, `medv`, and `indus`.
```{r boxcox}
# Create new dataframe to work with
box_train_df <- transformed_train_df


age_boxcox <- boxcox(lm(box_train_df$age ~ 1))
age_lambda <- age_boxcox$x[which.max(age_boxcox$y)]
box_train_df$age <- BoxCox(box_train_df$age, age_lambda)

dis_boxcox <- boxcox(lm(box_train_df$dis ~ 1))
dis_lambda <- dis_boxcox$x[which.max(dis_boxcox$y)]
box_train_df$dis <- BoxCox(box_train_df$dis, dis_lambda)

lstat_boxcox <- boxcox(lm(box_train_df$lstat ~ 1))
lstat_lambda <- lstat_boxcox$x[which.max(lstat_boxcox$y)]
box_train_df$lstat <- BoxCox(box_train_df$lstat, lstat_lambda)

nox_boxcox <- boxcox(lm(box_train_df$nox ~ 1))
nox_lambda <- lstat_boxcox$x[which.max(nox_boxcox$y)]
box_train_df$nox <- BoxCox(box_train_df$nox, nox_lambda)

ptratio_boxcox <- boxcox(lm(box_train_df$ptratio ~ 1))
ptratio_lambda <- lstat_boxcox$x[which.max(ptratio_boxcox$y)]
box_train_df$ptratio <- BoxCox(box_train_df$ptratio, ptratio_lambda)

tax_boxcox <- boxcox(lm(box_train_df$tax ~ 1))
tax_lambda <- tax_boxcox$x[which.max(tax_boxcox$y)]
box_train_df$tax <- BoxCox(box_train_df$tax, tax_lambda)

indus_boxcox <- boxcox(lm(box_train_df$indus ~ 1))
indus_lambda <- indus_boxcox$x[which.max(indus_boxcox$y)]
box_train_df$indus <- BoxCox(box_train_df$indus, indus_lambda)

medv_boxcox <- boxcox(lm(box_train_df$medv ~ 1))
medv_lambda <- medv_boxcox$x[which.max(medv_boxcox$y)]
box_train_df$medv <- BoxCox(box_train_df$medv, medv_lambda)

``` 


Compare the original distributions with transformed.

```{r box_hists, warning=FALSE, message=FALSE}
m_df_box <- box_train_df %>% dplyr::select(c(age, dis, lstat, nox, ptratio, tax, indus, medv)) %>% melt() 

a <- m_df_box %>% ggplot(aes(x= value)) + 
geom_density(color='#023020', fill='gray') + facet_wrap(~variable, scales = 'free',  ncol = 1) + theme_bw()

m_df <- transformed_train_df %>% dplyr::select(c(age, dis, lstat, nox, ptratio, tax, indus, medv)) %>% melt() 

b <- m_df %>% ggplot(aes(x= value)) + 
geom_density(color='#023020', fill='gray') + facet_wrap(~variable, scales = 'free',  ncol = 1) + theme_bw()

ggarrange(a, b + rremove("x.text"), 
          labels = c("Transformed", "Original"))
```


The glmnet() function uses a vector for the explanatory variable and a matrix for the predictor variables. The training and validation data frame are transformed into vectors and the corresponding matrix. Since the glmnet() function supports the standardization and transformation of the predictor variables, we do not need to create individual variable sets for the different transformations.  

```{r echo=TRUE}

sample <- sample.split(train_df$target, SplitRatio = 0.8)
train_data  <- subset(train_df, sample == TRUE)
test_data   <- subset(train_df, sample == FALSE)


# build X matrix and Y vector
X <- model.matrix(target ~ ., data=train_data)[,-1]
Y <- train_data[,"target"] 

```

**BEFORE BUILIDNG THE MODELS WE MAY NEED TO DEAL WITH OVERDISPERSION OR multicollinearity** **SHOULD BE DONE ON THE MODELS**
**We can also deal with variable selection again after running the models by using Cp Mallow's statistic. Template code example commented here**

```{r}
# # Run subsets of variables for the model multiple times and save to regfit.full. Default only goes up to subsets of size 8 variables.
# regfit.full=regsubsets(Salary~., data=Hitters)
# summary(regfit.full)
# 
# # Increase number of variables.
# regfit.full=regsubsets(Salary~., data=Hitters, nvmax=19)
# summary <- summary(regfit.full)
# 
# # Plot the number of variables versus the 'Cp' (Mallow's Cp) statistic which gives an idea of predictive power per number of variables. Cp value should always be less than the number of predictor variables + 1.
# plot(summary$cp, xlab="Number of Variables", ylab="Cp")
# which.min(summary$cp)
# # Way to view all the specific regsubset variables with Cps
# plot(regfit.full, scale="Cp")
# # Coefficients for model 10
# coef(regfit.full, 10)
```

## 3. Build Models

### 3.1 Model 1 - Logit model

#### Dataset - transformed_train_df

As the first step, we are going to build the generalized linear model based on the original training dataset with some variables transformed to categorical from numeric. Since our dependent variable takes only two values (0 and 1), we will use logistic regression. To do so, the function `glm()` with `family=binomial` is used. At the beginning, all variables are included
```{r general, warning=FALSE, message=FALSE}
model_1 <- glm(transformed_train_df, formula = target ~., family = binomial(link = "logit"))
summ(model_1)
```
With stepAIC, we can try to see what variables should be included in our model to increase the performance. 
Based on the the results of the function, we will include `zn`, `indus`, `nox`, `rm`, `age`, `dis`, `rad`, `tax` and `medv` variables.

**MAYBE WE SHOULD USE SOMETHING DIFFERENT FOR VARIABLE SELECTION, Figure 7.12 Flow chart for multiple linear regression SHOS US WHAT TO USE FOR VARIABLE SELECTION, BOOK "A Modern Approach to Regression with R", CHAPTER 7.4, PAGE 252**

```{r model1_aic, warning=FALSE, message=FALSE}
model_1_aic <- model_1 %>% stepAIC(trace = FALSE)
summary(model_1_aic)
```


#### Model Results

**ADD TEXT ABOUT THE MODEL**

#### Checking Model Assumptions

```{r check_lm1}
par(mfrow=c(2,2))
plot(model_1_aic)
```

### 3.2 Model 2 - 

#### Dataset - log_train_df


**Next, we can try to build the generalized linear model based on the dataset with log transformation.**
```{r general_log, warning=FALSE, message=FALSE}
#model_2 <- glm(log_train_df, formula = target ~., family = binomial(link = "logit"))
#summary(model_2)
```

**The stepAIC function tells us to include `zn`, `indus`, `nox`, `dis`, `rad`, `lstat` and `medv` variables.**
```{r model2_aic, warning=FALSE, message=FALSE}
#model_2_aic <- model_2 %>% stepAIC(trace = FALSE)
#summary(model_2_aic)
```

#### Model Results

**ADD TEXT ABOUT THE MODEL**

#### Checking Model Assumptions

```{r check_lm2}
#par(mfrow=c(2,2))
#plot(model_2_aic)
```


### 3.3 Model 3 - 

#### Dataset - box_train_df 


Next, we can try to build the generalized linear model based on the dataset with boxcox transformation.
```{r general_box, warning=FALSE, message=FALSE}
model_3 <- glm(box_train_df, formula = target ~., family = binomial(link = "logit"))
summary(model_3)
```

The stepAIC function tells us to include `zn`, `indus`, `nox`, `rm`, `age`, `dis`, `rad`, and `medv` variables.
```{r model3_aic, warning=FALSE, message=FALSE}
model_3_aic <- model_3 %>% stepAIC(trace = FALSE)
summary(model_3_aic)
```


#### Model Results

**ADD TEXT ABOUT THE MODEL**

#### Checking Model Assumptions

```{r check_lm3}
par(mfrow=c(2,2))
plot(model_3_aic)
```


### 3.4 Lasso Cross Validation

The cv.glmnet() function was used to perform k-fold cross-validation with variable selection using lasso regularization. The following attribute settings were selected for the model:

- type.measure = "class" - The type.measure is set to class to minimize the misclassification errors of the model since the accurate classification of the validation data set is the desired outcome.  
- nfold = 5 - Through our exploration, we did not uncover any hard and fast rules governing the optimal number of folds for n-fold cross-validation. Given the size of the training dataset, we opted for 5-fold cross-validation. 
- family = binomial - For Logistic Regression, the family attribute of the function is set to binomial.
- link = logit - For this model, we choose the default link function for a logistic model. 
- alpha =1 - The alpha value of 1 sets the variable shrinkage method to lasso. 
- standardize = TRUE - Finally, we explicitly set the standardization attribute to TRUE; this will normalize the prediction variables around a mean of zero and a standard deviation of one before modeling. 

```{r}

lasso.model<- cv.glmnet(x=X,y=Y,
                       family = "binomial", 
                       link = "probit",
                       standardize = TRUE,                       #standardize  
                       nfold = 10,
                       alpha=1)                                  #alpha=1 is lasso

l.min <- lasso.model$lambda.min
l.1se <- lasso.model$lambda.1se
coef(lasso.model, s = "lambda.min" )
coef(lasso.model, s = "lambda.1se" )
lasso.model

```


```{r}

par(mfrow=c(2,2))

plot(lasso.model)
plot(lasso.model$glmnet.fit, xvar="lambda", label=TRUE)
plot(lasso.model$glmnet.fit, xvar='dev', label=TRUE)

rocs <- roc.glmnet(lasso.model, newx = X, newy = Y )
plot(rocs,type="l")  

```


```{r}
assess.glmnet(lasso.model,           
              newx = X,              
              newy = Y )    

print(glmnet_cv_aicc(lasso.model, 'lambda.min'))
print(glmnet_cv_aicc(lasso.model, 'lambda.1se'))

```




## 4. Select Model

In analyzing the resulting model, we explored two different sets of coefficients that were extracted using lambda.min and lambda.1se respectively.

- The coefficients extracted using lambda.min minimizes the mean cross-validated error. The resulting model drops one predictor variable rm and has an AIC of -427.0. 
- The coefficients extracted using lambda.1se produce the most regularized model (cross-validated error is within one standard error of the minimum). For this model the dis and lstat predictor variables dropout, generating an AIC of -393.2.

Based on the lower AIC we select the coefficients extracted using the lambda.min. 

```{r}
# Create matrix new data
X_new <- model.matrix(target ~  zn + indus + chas + nox + rm + 
                      age + dis + rad + tax + ptratio + 
                      lstat + medv,
                      data=test_data)[,-1]


# predict using coefficients at lambda.min
lassoPred <- predict(lasso.model, newx = X_new, type = "response", s = l.min)

pred_df <- test_data
pred_df$target_prob <- lassoPred[,1]
pred_df$target_pred <- ifelse(lassoPred > 0.5, 1, 0)[,1]
Y_new <- pred_df$target

```


```{r}

confusion.glmnet(lasso.model, newx = X_new, newy = Y_new, s = l.min)

```

## 5. Conclusion



## References


## Appendix: R code


