---
title: 'Homework #3: Logistic Regression'
subtitle: 'Critical Thinking Group 1'
author: 'Ben Inbar, Cliff Lee, Daria Dubovskaia, David Simbandumwe, Jeff Parks'
output:
  html_document:
    toc: yes
    toc_float: yes
    theme: united
  pdf_document:
    toc: yes
editor_options:
  chunk_output_type: console
  markdown: 
    wrap: sentence
---


```{r setup, include=FALSE}
library(tidyverse)
library(dplyr)
library(reshape2)
library(ggplot2)
library(glmnet)
library(psych)
library(caret)
library(pROC)
library(skimr)
library(lemon)
library(DT)
library(kableExtra)
library(forecast)
library(corrplot)
library(ggpubr)
library(Hmisc)
library(caTools)
library(performance)
library(jtools)
library(regclass)
library(MASS)
library(PerformanceAnalytics)
library(leaps)
library(broom)
library(vip)

source("logistic_functions.R")

knitr::opts_chunk$set(echo = FALSE)
```


```{r}
set.seed(1233)
```




```{r data}
# read train and evaluation datasets
train_df <- read.csv("https://raw.githubusercontent.com/cliftonleesps/data621_group1/main/hw3/crime-training-data_modified.csv")
eval_df <- read.csv("https://raw.githubusercontent.com/cliftonleesps/data621_group1/main/hw3/crime-evaluation-data_modified.csv")

```

## Overview
People's quality of life consists many factors: available housing, economic viability, and geography among many others. One of the most import factors is security, which is directly related to the prevalence of crime. In large cities, crime negatively affects citizens' livelihood. Criminal activity prevents people from safely raising families, running businesses, owning property and providing tax revenue among other effects. And once crime appears in a locality, it becomes expensive and difficult to reverse its influence; city planners and public officials still do not agree on the best approaches despite many attempts and experiments.

This study focuses on crime detection and prediction. Specifically, using publicly data gathered from various Boston neighborhoods to predict whether each area has a crime rate above or below below the city median. The predictor variables describe an area's zoning, housing characteristics, and proximity to critical resources (e.g. employment or highways), among others.

We are going to build several models using different techniques and variable selection. In order to best assess our predictive model, we created a test set within our training data, and split it along an 80/20 training/testing proportion, before applying the finalized model to a separate evaluation dataset that did not contain the target.


<br></br>


## 1. Data Exploration

The crime training dataset contains 466 observations of 13 variables that are collected in attempts to evaluate the crime within the Boston neighborhoods. The evaluation dataset contains 40 observations of 12 variables.
The descriptions of each column are below.

<img src="https://raw.githubusercontent.com/cliftonleesps/data621_group1/main/hw3/data_variables.png" width="1200" style="display: block; margin-left: auto; margin-right: auto; width: 100%;"/>

### 1.1 Summary Statistics

The training data can be previewed below. The `target` column is the binary dependent variable denoting if a neighborhood had a crime rate above (target = 1) or below (target = 0) the city median.

```{r summary1}
DT::datatable(
      train_df,
      extensions = c('Scroller'),
      options = list(scrollY = 350,
                     scrollX = 500,
                     deferRender = TRUE,
                     scroller = TRUE,
                     dom = 'lBfrtip',
                     fixedColumns = TRUE, 
                     searching = FALSE), 
      rownames = FALSE) 
```

The table below provides the mean for each variable, the standard deviation and some other descriptive statistics.
As we see below, `zn`, `indus`, `age`, and `lstat` are proportions. `Chas` is a dummy variable. The rest are integers and fortunately, there are no missing values.

```{r summary}
skim(train_df)
```
<br></br>

### 1.2 Distributions
Next, we will check the density plots. It will help us to start thinking about which variables can be included in our model and how they are distributed relative to our target variable, so we graph both the total density and density per target category below.
</br>
</br>
Both types of skewness appears in several variables; `age`, `ptratio` are left-skewed, while  `dis`, `lstat`, `zn` are right-skewed. We may consider some transformations for these variables to satisfy linear regression assumptions.
</br>
</br>
The variables `tax` and `rad` appear binomial, and `chas` is binary and takes mostly the value of `0`. Likewise, `zn` also takes mostly the value of `0`, and thus may be worth removing.


```{r density_plot}

m_df <- train_df %>% pivot_longer(!target, names_to='variable' , values_to = 'value')
m_df %>% ggplot(aes(x=value, group=target, fill=target)) + 
geom_density(color='#023020') + facet_wrap(~variable, scales = 'free',  ncol = 4) + theme_bw()

m_df %>% ggplot(aes(x= value)) + 
geom_density(color='#023020', fill='gray') + facet_wrap(~variable, scales = 'free',  ncol = 4) + theme_bw()

```

Box plots indicate some outliers that we may wish to address.

```{r box_plot}

m_df %>% ggplot(aes(x=target, y=value, group=target)) + 
geom_boxplot(color='#023020', fill='gray') + facet_wrap(~variable, scales = 'free',  ncol = 4) +
  stat_summary(fun = "mean",  geom = "point", shape = 8, size = 2, color = "steelblue") + 
  stat_summary(fun = "median",  geom = "point", shape = 8, size = 2, color = "red") + theme_bw()

```

Before building a model, we need to make sure that we have both classes equally presented in out target variable. This appears to be the case, as class `1` takes 49% and class `0` takes 51% of the target variable. As a result, we don't have unbalanced class distribution for our target variable that we have to deal with. Otherwise, we had to take some additional steps (bootstrapping, etc) before using logistic regression.

```{r balance}
proportion <- colMeans(train_df['target'])
proportion
```

### 1.3 Correlation Matrix
Plotting the correlations between the `target` and the variables, we can see that some variables correlate *positively* with the target, such as `indus` (proportion of non-retail business acres), `nox` (nitrogen oxides concentration), `age` (proportion of units built prior to 1940), `rad` (accessibility to highways), `tax` (property tax rate per $10,000), `lstat` (lower status of population i.e. a socioeconomic measure), and *negatively* with `zn` (proportion of residential land zoned for large lots) and `dis` (mean distance to 5 Boston employment centers). Intuitively and based on well-studied socioeconomic principles, these make sense, barring perhaps the tax rate, which one would expect to be lower in higher crime-prone neighborhoods.

Variables with correlations close to zero are unlikely to offer significant insights into the factors contributing to a crime rate. 

To avoid multicollinearity, we should exclude some variables from the model that have high degrees of correlation with each other.
We can see potential cases of multicollinearity for the variables `rad`/`tax`, `indus`/`tax`, `age`/`nox`, `indus`/`nox`, `rm`/`medv`, `lstat`/`medv`, `nox`/`dis`, `indus`/`dis`, and `age`/`dis`.

We will carefully examine the VIF for these later on.

```{r corr}

rcore <- rcorr(as.matrix(train_df %>% dplyr::select(where(is.numeric))))
coeff <- rcore$r
corrplot(coeff, tl.cex = .7, tl.col="black", method = 'color', addCoef.col = "black",
         type="upper", order="hclust",
         diag=FALSE)

```

```{r corr_numbers}
tst <- train_df
tst <- tst[,13]
kable(cor(drop_na(train_df))[,13], "html", escape = F, col.names = c('Coefficient')) %>%
  kable_styling("striped", full_width = F) %>%
  column_spec(1, bold = T)
```


## 2. Data preparation

To build our model, we need to transform categorical variables `chas`, `target` and `rad` from numeric (default by the data) to factors. `rad` in particular is an ordinal categorical index. The data will be split for training and testing.

```{r factor}
transformed_train_df <- train_df
transformed_train_df$chas <- as.factor(transformed_train_df$chas)
transformed_train_df$target <- as.factor(transformed_train_df$target)
transformed_train_df$rad <- as.factor(transformed_train_df$rad)
```

As mentioned above, the variable `zn` has 0 value as the most occurred. It appears 339 times out of total 466 (72%), and may cause overdispersion, so we will remove it.
```{r count_zn}
count(train_df,zn)

transformed_train_df <- subset(transformed_train_df, select = -c(zn))
```


```{r split_data_1}
#split data for training and testing
sample <- sample.split(transformed_train_df$target, SplitRatio = 0.8)
transformed_training  <- subset(transformed_train_df, sample == TRUE)
transformed_testing   <- subset(transformed_train_df, sample == FALSE)
```


Next, let's use BoxCox transformation to normalize our variables - we will look at the distributions for all: `age`, `dis`, `lstat`, `nox`, `ptratio`, `tax`, `medv`, and `indus`.
```{r boxcox, fig.keep="none"}
# Create new dataframe to work with
box_train_df <- transformed_train_df


age_boxcox <- boxcox(lm(box_train_df$age ~ 1))
age_lambda <- age_boxcox$x[which.max(age_boxcox$y)]
box_train_df$age <- BoxCox(box_train_df$age, age_lambda)

dis_boxcox <- boxcox(lm(box_train_df$dis ~ 1))
dis_lambda <- dis_boxcox$x[which.max(dis_boxcox$y)]
box_train_df$dis <- BoxCox(box_train_df$dis, dis_lambda)

lstat_boxcox <- boxcox(lm(box_train_df$lstat ~ 1))
lstat_lambda <- lstat_boxcox$x[which.max(lstat_boxcox$y)]
box_train_df$lstat <- BoxCox(box_train_df$lstat, lstat_lambda)

nox_boxcox <- boxcox(lm(box_train_df$nox ~ 1))
nox_lambda <- lstat_boxcox$x[which.max(nox_boxcox$y)]
box_train_df$nox <- BoxCox(box_train_df$nox, nox_lambda)

ptratio_boxcox <- boxcox(lm(box_train_df$ptratio ~ 1))
ptratio_lambda <- lstat_boxcox$x[which.max(ptratio_boxcox$y)]
box_train_df$ptratio <- BoxCox(box_train_df$ptratio, ptratio_lambda)

tax_boxcox <- boxcox(lm(box_train_df$tax ~ 1))
tax_lambda <- tax_boxcox$x[which.max(tax_boxcox$y)]
box_train_df$tax <- BoxCox(box_train_df$tax, tax_lambda)

indus_boxcox <- boxcox(lm(box_train_df$indus ~ 1))
indus_lambda <- indus_boxcox$x[which.max(indus_boxcox$y)]
box_train_df$indus <- BoxCox(box_train_df$indus, indus_lambda)

medv_boxcox <- boxcox(lm(box_train_df$medv ~ 1))
medv_lambda <- medv_boxcox$x[which.max(medv_boxcox$y)]
box_train_df$medv <- BoxCox(box_train_df$medv, medv_lambda)

``` 


Compare the original distributions with transformed. 
As we see on the graphs below, distributions of the transformed variables became closer to normal.

```{r box_hists, warning=FALSE, message=FALSE}
m_df_box <- box_train_df %>% dplyr::select(c(age, dis, lstat, nox, ptratio, tax, indus, medv)) %>% melt() 

a <- m_df_box %>% ggplot(aes(x= value)) + 
geom_density(color='#023020', fill='gray') + facet_wrap(~variable, scales = 'free',  ncol = 1) + theme_bw()

m_df <- transformed_train_df %>% dplyr::select(c(age, dis, lstat, nox, ptratio, tax, indus, medv)) %>% melt() 

b <- m_df %>% ggplot(aes(x= value)) + 
geom_density(color='#023020', fill='gray') + facet_wrap(~variable, scales = 'free',  ncol = 1) + theme_bw()

ggarrange(a, b + rremove("x.text"), 
          labels = c("Transformed", "Original"))
```

## 3. Build Models

The glmnet() function (will be used later) uses a vector for the explanatory variable and a matrix for the predictor variables. The training and validation data frame are transformed into vectors and the corresponding matrix (X, Y). Since the glmnet() function supports the standardization and transformation of the predictor variables, we do not need to create individual variable sets for the different transformations.

However, we will also split the regularly BoxCox transformed training set, to be able to compare those transformations against the glmnet() transforms and results.

```{r split_data_2, echo=TRUE}
# Split data for training and testing
sample <- sample.split(box_train_df$target, SplitRatio = 0.8)
box_training  <- subset(box_train_df, sample == TRUE)
box_testing   <- subset(box_train_df, sample == FALSE)
```

```{r echo=TRUE}
# set seed for consistancy 
set.seed(1233)

# 80/20 split of the training data set
sample <- sample.split(train_df$target, SplitRatio = 0.8)
train_data  <- subset(train_df, sample == TRUE)
test_data   <- subset(train_df, sample == FALSE)


# build X matrix and Y vector
X <- model.matrix(target ~ ., data=train_data)[,-1]
Y <- train_data[,"target"] 

```


### 3.1 Model 1 - Logit model, original data

As the first step, we are going to build the generalized linear model based on the original training dataset (but without `zn` column) with some variables transformed to categorical from numeric. Since our dependent variable takes only two values (0 and 1), we will use logistic regression. To do so, the function `glm()` with `family=binomial` is used. At the beginning, all variables are included.

```{r general, warning=FALSE, message=FALSE}
model_1 <- glm(transformed_training, formula = target ~., family = binomial(link = "logit"))
summ(model_1)
```


With stepAIC, we can try to see what variables should be included in our model to increase the performance. 
Based on the the results of the function, we will include `nox`, `rm`, `age`, `rad`, `tax`, `ptratio` and `medv` variables.

```{r model1_aic, warning=FALSE, message=FALSE}
model_1_aic <- model_1 %>% stepAIC(trace = FALSE)
summary(model_1_aic)
```

The p-value associated with this Chi-Square statistic is below. The lower the p-value, the better the model is able to fit the dataset compared to a model with just an intercept term.
```{r chi_model_1}
1-pchisq(model_1_aic$null.deviance-model_1_aic$deviance, model_1_aic$df.null- model_1_aic$df.residual)
```

#### Model Results

Only `nox`, `age`, `tax`, `ptratio` and `medv` have regression coefficients in model that are highly significant at the 5% level. The coefficients of `nox` (nitrogen oxides concentration), `age` (proportion of owner-occupied units built prior to 1940), `ptratio` (pupil-teacher ratio by town) and `medv` (median value of owner-occupied homes in 1000s dollars) increase the chance of crime while `tax` (full-value property-tax rate per $10,000) decreases as expected. 

The `null deviance` of `516.96` defines how well the target variable can be predicted by a model with only an intercept term.

The `residual deviance` of `97.06` defines how well the target variable can be predicted by our AIC model that we fit with  predictor variables mentioned above. The lower the value, the better the model is able to predict the value of the response variable.

The p-value associated with this Chi-Square statistic is 0 which is less than .05, the model can be useful.

The Akaike information criterion (`AIC`) is `127.06`. We will use this metric to compare the fit of different models.  The lower the value, the better the regression model is able to fit the data.

#### Checking Model Assumptions

We evaluate the modeling assumptions using standard diagnostic plots, marginal model plots, Variance Inflation Factor (VIF) to assess collinearity.

**ADD TEXT ABOUT GRAPHS BELOW**

```{r check_lm1}
par(mfrow=c(2,2))
plot(model_1_aic)
```

To check the linearity assumptions (if there is a linear relationship between independent variables and the Logit of the target variable). We select only numeric variables from our data, add the logit results based on the probability from the predictions and build a scatter plot. We are going check the linearity assumptions visually.
Only `rm`, `lstat`, `medv`, `nox` seem like linear relation with the logit results.
```{r model_1_linearity}
probabilities <- predict(model_1_aic, type = "response")
predicted.classes <- ifelse(probabilities > 0.5, "pos", "neg")
head(predicted.classes)

#Only numeric predictors
data <- transformed_training %>%
  dplyr::select_if(is.numeric) 
predictors <- colnames(data)

# Bind the logit and tidying the data for plot
data <- data %>%
  mutate(logit = log(probabilities/(1-probabilities))) %>%
  gather(key = "predictors", value = "predictor.value", -logit)

#Scatter plot
ggplot(data, aes(logit, predictor.value))+
  geom_point(size = 0.5, alpha = 0.5) +
  geom_smooth(method = "loess") + 
  theme_bw() + 
  facet_wrap(~predictors, scales = "free_y")
```

The Standardized Residuals plot seems to have a constant variance though there are some outliers.
```{r model_1_residuals}
model_1_aic.data <- augment(model_1_aic) %>% 
  mutate(index = 1:n())

ggplot(model_1_aic.data, aes(index, .std.resid)) + 
  geom_point(aes(color = target), alpha = .5) +
  theme_bw()
```

The marginal model plot is a very useful graphical method for deciding if a logistic regression model is adequate or not. The marginal model plots below show reasonable agreement across the two sets of fits indicating that model_1_aic is a valid model.

```{r warning=FALSE, message=FALSE}
car::mmps(model_1_aic, span = 3/4, layout = c(2, 2))
```

In terms of multicollinearity, all variables have a VIF less than 5. As a result, multicollinearity shouldn't be a problem for our model.
```{r model_1_vif}
car::vif(model_1_aic)
```


```{r}
# **BEFORE BUILIDNG THE MODELS WE MAY NEED TO DEAL WITH OVERDISPERSION OR multicollinearity** **SHOULD BE DONE ON THE MODELS**
# **We can also deal with variable selection again after running the models by using Cp Mallow's statistic. Template code example commented here**
# # Run subsets of variables for the model multiple times and save to regfit.full. Default only goes up to subsets of size 8 variables.
# regfit.full=regsubsets(Salary~., data=Hitters)
# summary(regfit.full)
# 
# # Increase number of variables.
# regfit.full=regsubsets(Salary~., data=Hitters, nvmax=19)
# summary <- summary(regfit.full)
# 
# # Plot the number of variables versus the 'Cp' (Mallow's Cp) statistic which gives an idea of predictive power per number of variables. Cp value should always be less than the number of predictor variables + 1.
# plot(summary$cp, xlab="Number of Variables", ylab="Cp")
# which.min(summary$cp)
# # Way to view all the specific regsubset variables with Cps
# plot(regfit.full, scale="Cp")
# # Coefficients for model 10
# coef(regfit.full, 10)
```

### 3.2 Model 2 - Logit model, BoxCox transformation

Next, we can try to build the generalized linear model based on the dataset with boxcox transformation.
```{r general_box, warning=FALSE, message=FALSE}
model_2 <- glm(box_training, formula = target ~., family = binomial(link = "logit"))
summary(model_2)
```

The stepAIC function tells us to include once again the variables `nox`, `rm`, `age`, `dis`, `rad` and `medv`.
```{r model2_aic, warning=FALSE, message=FALSE}
model_2_aic <- model_2 %>% stepAIC(trace = FALSE)
summary(model_2_aic)
```

The p-value associated with this Chi-Square statistic is below. 
```{r chi_model_2}
1-pchisq(model_2_aic$null.deviance-model_2_aic$deviance, model_2_aic$df.null- model_2_aic$df.residual)
```

#### Model Results

Only `nox`, `rm`, `age`, `dis`, and `medv` have regression coefficients in model that are significant. The coefficients of `nox` (nitrogen oxides concentration), `medv` (median value of owner-occupied homes in 1000s dollars), `age` (proportion of owner-occupied units built prior to 1940), `dis` (weighted mean of distances to five Boston employment centers) increase the chance of crime while `rm` (average number of rooms per dwelling) decreases. 

The `residual deviance` is `113.97`.

The p-value associated with this Chi-Square statistic is 0 which is less than .05, the model can be useful.

The Akaike information criterion (`AIC`) is `139.97`. 

#### Checking Model Assumptions


**ADD TEXT ABOUT GRAPHS BELOW**

```{r check_lm2}
par(mfrow=c(2,2))
plot(model_2_aic)
```

By checking the linearity assumptions, we see that only `rm` seem like linear relation with the logit results.
```{r model_2_linearity}
probabilities <- predict(model_2_aic, type = "response")
predicted.classes <- ifelse(probabilities > 0.5, "pos", "neg")
head(predicted.classes)

#Only numeric predictors
data <- box_training %>%
  dplyr::select_if(is.numeric) 
predictors <- colnames(data)

# Bind the logit and tidying the data for plot
data <- data %>%
  mutate(logit = log(probabilities/(1-probabilities))) %>%
  gather(key = "predictors", value = "predictor.value", -logit)

#Scatter plot
ggplot(data, aes(logit, predictor.value))+
  geom_point(size = 0.5, alpha = 0.5) +
  geom_smooth(method = "loess") + 
  theme_bw() + 
  facet_wrap(~predictors, scales = "free_y")
```

The Standardized Residuals plot seems to have a constant variance though there are some outliers.
```{r model_2_residuals}
model_2_aic.data <- augment(model_2_aic) %>% 
  mutate(index = 1:n())

ggplot(model_2_aic.data, aes(index, .std.resid)) + 
  geom_point(aes(color = target), alpha = .5) +
  theme_bw()
```

The marginal model plots below show reasonable agreement across the two sets of fits indicating that model_2_aic is a valid model.

```{r warning=FALSE, message=FALSE}
car::mmps(model_2_aic, span = 3/4, layout = c(2, 2))
```

In terms of multicollinearity, all variables have a VIF less than 5. As a result, multicollinearity shouldn't be a problem for our model.
```{r model_2_vif}
car::vif(model_2_aic)
```



### 3.3 Model 3 - Lasso Cross Validation

The cv.glmnet() function was used to perform k-fold cross-validation with variable selection using lasso regularization. The following attribute settings were selected for the model:

- type.measure = "class" - The type.measure is set to class to minimize the misclassification errors of the model since the accurate classification of the validation data set is the desired outcome.  
- nfold = 5 - Through our exploration, we did not uncover any hard and fast rules governing the optimal number of folds for n-fold cross-validation. Given the size of the training dataset, we opted for 5-fold cross-validation. 
- family = binomial - For Logistic Regression, the family attribute of the function is set to binomial.
- link = logit - For this model, we choose the default link function for a logistic model. 
- alpha =1 - The alpha value of 1 sets the variable shrinkage method to lasso. 
- standardize = TRUE - Finally, we explicitly set the standardization attribute to TRUE; this will normalize the prediction variables around a mean of zero and a standard deviation of one before modeling. 



The resulting model is explored by extractng coefficients at two different values for lambda, lambda.min and lambda.1se respectively.

- The coefficients extracted using lambda.min minimizes the mean cross-validated error. The resulting model drops one predictor variable rm and has an AIC of -352.0. 
- The coefficients extracted using lambda.1se produce the most regularized model (cross-validated error is within one standard error of the minimum). For this model the 'rm' and 'tax' predictor variables dropout, generating an AIC of -336.8.

Even though the coefficients extracted using the lambda.min have the lower AIC value, selecting the coefficients at lambda.1se generates a more parsimonious model.


```{r}

lasso.model<- cv.glmnet(x=X,y=Y,
                       family = "binomial", 
                       link = "logit",
                       standardize = TRUE,                       #standardize  
                       nfold = 5,
                       alpha=1)                                  #alpha=1 is lasso

l.min <- lasso.model$lambda.min
l.1se <- lasso.model$lambda.1se
coef(lasso.model, s = "lambda.min" )
coef(lasso.model, s = "lambda.1se" )
lasso.model

```


```{r}

par(mfrow=c(2,2))

plot(lasso.model)
plot(lasso.model$glmnet.fit, xvar="lambda", label=TRUE)
plot(lasso.model$glmnet.fit, xvar='dev', label=TRUE)

rocs <- roc.glmnet(lasso.model, newx = X, newy = Y )
plot(rocs,type="l")  

```


```{r}
assess.glmnet(lasso.model,           
              newx = X,              
              newy = Y )    

print(glmnet_cv_aicc(lasso.model, 'lambda.min'))
print(glmnet_cv_aicc(lasso.model, 'lambda.1se'))

```



Looking closer at the remaining coefficients for the selected lambda values, the `nox` predictor variable has the largest coefficient, and several variables including `lstat`,`age`,`zn`,and `indus` have coefficients close to zero. As mentioned earlier, the dataset has a high correlation between predictor variables. The lasso regression approaches this issue by selecting the variable with the highest correlation (in this case`nox`) and shrinking the remaining variables (as can be seen in the plot of coefficients). 
```{r}
coef(lasso.model, s = "lambda.1se" )
vip(lasso.model, num_features=20 ,geom = "col", include_type=TRUE, lambda = "lambda.1se")
```


#### Model Results


The coefficients extracted at the lambda.1se value are used to predict the relative crime rate for the testing data set. The confusion matrix highlights an accuracy of 83.9%.

```{r}
# Create matrix new data
X_test <- model.matrix(target ~ ., data=test_data)[,-1]
Y_test <- test_data[,"target"] 


# predict using coefficients at lambda.min
lassoPred <- predict(lasso.model, newx = X_test, type = "response", s = 'lambda.1se')

pred_df <- test_data
pred_df$target_prob <- lassoPred[,1]
pred_df$target_pred <- ifelse(lassoPred > 0.5, 1, 0)[,1]

```


```{r}

confusion.glmnet(lasso.model, newx = X_test, newy = Y_test, s = 'lambda.1se')

```




#### Checking Model Assumptions


Again we check linear relationship between independent variables and the Logit of the target variable. Visually inspecting the results there is a linear trend in the relationship but there are deviations from the straight line in all variables with the exception of `nox` and `ptratio`.


```{r}

pred_df <- pred_df %>%
  mutate(logit = log(target_prob/(1-target_prob))) 

m_df <- pred_df %>% pivot_longer(!c(target,target_prob,target_pred,logit), 
                                 names_to='variable' , values_to = 'value')

#Scatter plot
ggplot(m_df, aes(logit, value))+
  geom_point(size = 0.5, alpha = 0.5) +
  geom_smooth(method = "loess") + 
  theme_bw() + 
  facet_wrap(~variable, scales = "free_y")

```


The Standardized Residuals plot seems to have a constant variance though there are some outliers.

```{r}
pred_df$index <- as.numeric(rownames(pred_df))
pred_df$resid <- pred_df$target_prob - pred_df$target


ggplot(pred_df, aes(index, resid)) + 
  geom_point(aes(color = target), alpha = .5) +
  theme_bw()

```


The lasso regression solve multicollinearity issue by selecting the variable with the largest coefficient while setting the rest to (nearly) zero. 


## 4. Select Model

### 4.1 Confusion matrixes, ROC curve
To assess model performance, we will use the testing data that we split originally for each model. Then print out the confusion matrix (Classification Error Rate, Precision, Sensitivity, Specificity, F1 Score) and ROC curve.

**ADD CONF MATRIX AND ROC CURVE FOR MODEL 3**

**MAYBE MAKE A BETTER conf_matrix_1**
```{r model_1_confusionMatrix, include=FALSE}

transformed_testing$model_1 <- ifelse(predict.glm(model_1_aic, transformed_testing, "response") >= 0.5, 1, 0)
conf_matrix_1 <- confusionMatrix(factor(transformed_testing$model_1), factor(transformed_testing$target), "1")


results <- tibble(Model = "Model #1", Accuracy=conf_matrix_1$byClass[11], F1 = conf_matrix_1$byClass[7],
                  Deviance= model_1_aic$deviance, 
                  R2 = 1 - model_1_aic$deviance / model_1_aic$null.deviance,
                  AIC= model_1_aic$aic)

conf_matrix_1


```


```{r model_2_confusionMatrix, include=FALSE}

box_testing$model_2 <- ifelse(predict.glm(model_2_aic, box_testing, "response") >= 0.5, 1, 0)
conf_matrix_2 <- confusionMatrix(factor(box_testing$model_2), factor(box_testing$target), "1")


results2 <- rbind(results,tibble(Model = "Model #2", Accuracy=conf_matrix_2$byClass[11], F1 = conf_matrix_2$byClass[7],
                  Deviance=model_2_aic$deviance, 
                  R2 = 1 - model_2_aic$deviance / model_2_aic$null.deviance,
                  AIC= model_2_aic$aic))
conf_matrix_2
```

## Confusion Matrixes

**Model 1**
```{r show_confusion_matrix_1}
conf_matrix_1$table
paste0(" Percent Correct: ", round((conf_matrix_1$table[1,1] + conf_matrix_1$table[2,2])  /(sum(conf_matrix_1$table[,1]) + sum(conf_matrix_1$table[,2])), 4))
```

**Model 2**
```{r show_confusion_matrix_2}
conf_matrix_2$table
paste0(" Percent Correct: ", round((conf_matrix_2$table[1,1] + conf_matrix_2$table[2,2])  /(sum(conf_matrix_1$table[,1]) + sum(conf_matrix_1$table[,2])), 4))

```

**Model 3**
```{r show_confusion_matrix_3}
confusion.glmnet(lasso.model, newx = X_test, newy = Y_test, s = l.min)
```

```{r roc_model_2, warning=FALSE, message=FALSE, include=FALSE}
par(mfrow=c(2,2))
rocs <- roc.glmnet(lasso.model, newx = X_test, newy = Y_test )
plot(rocs,type="l")  
```

```{r}

lasso.r1 <- assess.glmnet(lasso.model,           
                                newx = X_test,              
                                newy = Y_test )   

lasso.r2 <- glmnet_cv_aicc(lasso.model, 'lambda.1se')


results2 <- rbind(results2,tibble(Model = "Model #3", 
                            Accuracy=lasso.r1$auc[1], 
                            F1 = NA,
                            Deviance=lasso.r1$deviance[[1]], 
                            R2 = NA,
                            AIC= lasso.r2$AICc))

```






```{r}
kable(results2, digits=4) %>% 
  kable_styling(bootstrap_options = "basic", position = "center")
```


### 4.2 Predictions

We are going to follow transformations that we did for the training data.

**ONE OF THE DATA BELOW, DEPENS ON WHAT MODEL WE USE**

For teh first model, we will transform variables `chas` and `rad` to categorical. Also, we remove `zn` column as it had 0 value as the most occurred.
```{r factor_eval}
transformed_eval_df <- eval_df
transformed_eval_df$chas <- as.factor(transformed_eval_df$chas)
transformed_eval_df$rad <- as.factor(transformed_eval_df$rad)

transformed_eval_df <- subset(transformed_eval_df, select = -c(zn))
```

BoxCox transformations:
```{r boxcox_eval, fig.keep = "none"}
# Create new dataframe to work with
box_eval_df <- transformed_eval_df


age_boxcox <- boxcox(lm(box_eval_df$age ~ 1))
age_lambda <- age_boxcox$x[which.max(age_boxcox$y)]
box_eval_df$age <- BoxCox(box_eval_df$age, age_lambda)

dis_boxcox <- boxcox(lm(box_eval_df$dis ~ 1))
dis_lambda <- dis_boxcox$x[which.max(dis_boxcox$y)]
box_eval_df$dis <- BoxCox(box_eval_df$dis, dis_lambda)

lstat_boxcox <- boxcox(lm(box_eval_df$lstat ~ 1))
lstat_lambda <- lstat_boxcox$x[which.max(lstat_boxcox$y)]
box_eval_df$lstat <- BoxCox(box_eval_df$lstat, lstat_lambda)

nox_boxcox <- boxcox(lm(box_eval_df$nox ~ 1))
nox_lambda <- lstat_boxcox$x[which.max(nox_boxcox$y)]
box_eval_df$nox <- BoxCox(box_eval_df$nox, nox_lambda)

ptratio_boxcox <- boxcox(lm(box_eval_df$ptratio ~ 1))
ptratio_lambda <- lstat_boxcox$x[which.max(ptratio_boxcox$y)]
box_eval_df$ptratio <- BoxCox(box_eval_df$ptratio, ptratio_lambda)

tax_boxcox <- boxcox(lm(box_eval_df$tax ~ 1))
tax_lambda <- tax_boxcox$x[which.max(tax_boxcox$y)]
box_eval_df$tax <- BoxCox(box_eval_df$tax, tax_lambda)

indus_boxcox <- boxcox(lm(box_eval_df$indus ~ 1))
indus_lambda <- indus_boxcox$x[which.max(indus_boxcox$y)]
box_eval_df$indus <- BoxCox(box_eval_df$indus, indus_lambda)

medv_boxcox <- boxcox(lm(box_eval_df$medv ~ 1))
medv_lambda <- medv_boxcox$x[which.max(medv_boxcox$y)]
box_eval_df$medv <- BoxCox(box_eval_df$medv, medv_lambda)

``` 

**CHANGE BASED ON THE MODEL OF CHOICE, CHANGE SELECTED VARIABLES**
```{r predictions}
eval_data <-  subset(box_eval_df, select = (c(nox, rm, age, dis, rad, ptratio, medv)))
predictions <- ifelse(predict(model_2_aic, eval_data, type = "link") > 0.5, 1, 0)
eval_df['target'] <- predictions

write.csv(eval_df, 'eval_predictions.csv', row.names=F)
predictions
```




```{r}

# build X matrix and Y vector
X_eval <- model.matrix(target ~ ., data=eval_df)[,-1]
Y_eval <- eval_df[,"target"] 


# predict using coefficients at lambda.min
lassoEval <- predict(lasso.model, newx = X_eval, type = "response", s = 'lambda.1se')

eval_pred_df <- eval_df
eval_pred_df$target_prob <- lassoEval[,1]
eval_pred_df$target_pred <- ifelse(lassoEval > 0.5, 1, 0)[,1]

write.csv(eval_pred_df, 'eval_predictions_lasso.csv', row.names=F)

```



## 5. Conclusion

  After investigating a few models, a binary logistic regression performed the best with an AIC score of -321 when incorporating eight predictors, out of a possible twelve. The model achieved an accuracy rate of 86%. With this and similar models, city councils can devote resources and attention to neighborhoods with a higher crime risk as well as better plan future development.
  **ADD MORE BASED ON THE FINAL RESULTS, SOME RESULTING NUMBERS**

## 6. References

1. Faraway, J. J. (2014). _Linear Models with R, Second Edition._ CRC Press.
2. Sheather, S. (2009). _A Modern Approach to Regression with R._ Springer Science & Business Media.
3. _Detecting Multicollinearity Using Variance Inflation Factors_ | STAT 462. (n.d.). https://online.stat.psu.edu/stat462/node/180/
4. Faraway, J. J. (2016). _Extending the Linear Model with R: Generalized Linear, Mixed Effects and Nonparametric Regression Models._ CRC Press.


## Appendix: R code

