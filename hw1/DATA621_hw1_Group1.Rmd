---
title: 'Homework #1: Moneyball'
author: 'Critical Thinking Group 1: Ben Inbar, Cliff Lee, Daria Dubovskaia, David Simbandumwe, Jeff Parks, Nick Oliver'
output:
  html_document:
    toc: yes
    toc_float: yes
    theme: united
  pdf_document:
    toc: yes
editor_options:
  chunk_output_type: console
bibliography: references.bib
---

```{r setup, include=FALSE}
library(tidyverse)
library(reshape2)
library(kableExtra)
library(Matrix)
library(MASS)
library(mice)
library(Hmisc)
library(corrplot)
library(performance)
library(naniar)
library(psych)
library(GGally)
library(campfin)
library(caret)
library(yardstick)
library(summarytools)
library(sjPlot)
library(car)
library(rsample)
library(olsrr)
knitr::opts_chunk$set(echo = F, warning = F, message = F, eval = T , results="asis")
set.seed(1234)
```


```{r echo=FALSE}

st_css()

 st_options(
   plain.ascii = FALSE,
   style = 'grid',
   dfSummary.style ='grid',
   freq.silent  = TRUE,
   headings     = FALSE,
   tmp.img.dir  = "./tmp",
   dfSummary.custom.1 =
     expression(
       paste(
         "Q1 - Q3 :",
         round(
           quantile(column_data, probs = .25, type = 2,
                    names = FALSE, na.rm = TRUE), digits = 1
         ), " - ",
         round(
           quantile(column_data, probs = .75, type = 2,
                    names = FALSE, na.rm = TRUE), digits = 1
         )
       )
     )
 )

source('./hw1/Functions.R', local = knitr::knit_global())
 
```

## Overview
The use of historical statistics to predict future outcomes, particularly wins and losses, and identify opportunities for improving team or individual performance, has gained significant attention in professional sports. The aim of this analysis is to develop several models that can predict a baseball team's wins over a season based on team stats such as homeruns, strikeouts, base hits, and more. We will begin by examining the data for any issues, such as missing data, or outliers, and take the necessary measures to clean the data. We will subsequently create and evaluate three different linear models that forecast seasonal wins using the dataset, which includes both training and evaluation data. We will train the models using the main training data and then evaluate their performance against the evaluation data to determine their effectiveness. Finally, we will choose the best model that balances accuracy and simplicity for predicting seasonal wins.

## 1. Date Exploration
The baseball training dataset contains 2,276 observations of 17 variables detailing various teams' performances per year from 1871 to 2006. The description of the columns is shown below. Due to the relatively long period, we expect to see outliers and missing data as the league modified official game rules; these rule changes undoubtedly caused teams and players to change their tactics in response. Additionally, the number of single base hits is noticeably missing from the columns. However, we will derive this value as the number of other types of hits (doubles, triples, home runs) can be subtracted from total hits. Lastly, other columns representing game number (out of 162), inning number (1-9), and matching opponent columns would have been vastly useful for predictions. One last noticeable omission from the original dataset is of the number of single base hits. However, this value can possibly be calculated as a difference between other types of hits (doubles, triples, home runs) and total hits. 
<p align="center">
  <img src="https://raw.githubusercontent.com/ex-pr/DATA607/Project-1/a1_moneyball.png">
</p>

```{r initialization, include=FALSE}
trainDf <- read.csv("https://raw.githubusercontent.com/cliftonleesps/data621_group1/main/hw1/moneyball-training-data.csv")
evalDf <- read.csv("https://raw.githubusercontent.com/cliftonleesps/data621_group1/main/hw1/moneyball-evaluation-data.csv")
```


### 1.1 Summary Statistics

The table below shows us some valuable descriptive statistics for the training data. The data set contains all integers. We can see that many of the variables have a minimum of 0 but not all. The means and medians of each variable are all relatively close in value for each individual variable. This tells us that most data is free from extreme outliers as they tend to skew the mean relative to the median.

One interesting piece of information is the min/max of the `TARGET_WINS` variable. The minimum is 0 meaning there are teams that did not win a single game. The maximum is 146 which indicates no team in the training dataset had a perfect season, as we know from the data a season consists of 162 games.

Also of note is the number of missing values from certain variables. Most notably the `TEAM_BATTING_HBP` (batters hit by pitch variable). With 91% of the data missing we will remove this variable from our dataset because there simply is not enough information to impute a sensible value. The column `TEAM_BASERUN_CS` (caught stealing) had 34% of the missing data, we may consider removing it later. The missing data for these two columns may be due to a change official rules or tactics before the modern era of baseball.

```{r echo=FALSE, warning=FALSE, message=FALSE}
print(
  dfSummary(trainDf, 
            varnumbers   = TRUE,
            na.col       = TRUE,
            graph.magnif = .8,
            tmp.img.dir  = "/tmp"),
  method = "render"
)

```

### 1.2 Distribution and Box Plots

Next, we'll visually check for normal distributions and box plots in both the dependent and independent variables. The density plot below shows normalcy in most features except for extremely right skewed features such as hits allowed (PITCHING_H) or errors (FIELDING_E). Homeruns by batters (BATTING_HR) and strikeouts by batters (BATTING_SO) variables seem bimodal. It implies the existence of two distinct clusters within the baseball season data, where teams tended to score more in one of the clusters. \
Box plots for these further show a high number of outliers exist outside of the interquartile ranges so their effects should be carefully considered and we may deal with non-unimodal distributions.

```{r density_boxplot, echo=FALSE, warning=FALSE, message=FALSE, fig.height=8, fig.width=10}
m_df <- trainDf %>% dplyr::select(-c(INDEX)) %>% melt() 

m_df %>% ggplot(aes(x= value)) + 
    geom_density(color='#023020', fill='gray') + facet_wrap(~variable, scales = 'free',  ncol = 4) + theme_bw()

m_df %>% ggplot(aes(x = value)) +
  geom_boxplot(outlier.color = 'red', outlier.shape = 1) +
  facet_wrap(vars(variable),scales = "free", ncol = 4)

```

Lastly, the function featurePlot() will show the relationship between independent variables and the target variable `TARGET_WINS`. 
```{r wins_features, echo=FALSE, fig.height=8, fig.width=10}
featurePlot(trainDf[,2:ncol(trainDf)], trainDf[,1], plot = "scatter", type = c("p", "smooth"), span = 1)
```
In general, while our graphs display certain intriguing connections among the variables, they also expose noteworthy problems with the data. For example, the dataset contains a team that has not won any games, which appears improbable. By checking the web data, we found that it actually happened 2 times: 1872 and 1873. Or that the pitching data contains numerous instances of 0's, several teams have 0 strikeouts by their pitchers over the season, which is highly improbable. Also, there is as a team achieving 20,000 strikeouts. There will be further steps to work with outliers and 0's.

### 1.3 Correlation Matrix

Plotting the correlations between `TARGET_WINS` and the variables (excluding `INDEX` and `TEAM_BATTING_HBP`) we can see that very few variables are strongly correlated with the target variable. Columns with correlations close to zero are unlikely to offer significant insights into the factors that contribute to a team's victories.\

To avoid multicolinearity, we should not include features that have strong correlation. Comparing offensive (any column starting with `BATTING` or `BASERUN`) to defensive stats unexpectedly shows some correlation, pointing to potential problems. Qualitatively, the matrix implies some teams or players are exceptional both at hitting (offensive) and fielding (defensive). Furthermore, a typical team's number of batted home runs and allowed home runs has a correlation of nearly 1.0. This is an unexpected correlation but can be explained by noticing most games are decided by a difference of one or two runs (whether the games are high scoring or not). Any final models should include one of these two home run variables. Alternatively, the correlation between a team's hits (`BATTING_H`) and hits allowed (`PITCHING_H`) is around 0.3 which is seems reasonable.

There are some other strong correlations that are less obvious such as Errors (`TEAM_FIELDING_E`) being strongly negatively correlated with walks by batters (`TEAM_BATTING_BB`), strike outs (`TEAM_BATTING_SO`).  All combined together, teams that get a lot of hits do not generally make fielding errors.

Digging a little deeper we can see there is a Pearson correlation coefficient of `r cor(trainDf$TEAM_FIELDING_E, trainDf$TEAM_BATTING_BB)` for errors and walks by batters which indicates a strong negative correlation between the two variables. Looking at errors compared with team pitching hits allowed we see a correlation of `r cor(trainDf$TEAM_FIELDING_E, trainDf$TEAM_PITCHING_H)` which indicates a strong positive correlation. 


```{r correlation_matrix, echo=FALSE}
rcore <- rcorr(as.matrix(trainDf %>% dplyr::select(where(is.numeric) & -INDEX & -TEAM_BATTING_HBP)))
coeff <- rcore$r
corrplot(coeff, tl.cex = .7 , method = 'circle')
```

```{r corr_numbers, echo=FALSE}
tst <- trainDf
tst <- tst[,-1 ]
kable(cor(drop_na(tst))[,1], "html", escape = F, col.names = c('Coefficient')) %>%
  kable_styling("striped", full_width = F) %>%
  column_spec(1, bold = T)
```

Lastly, lets take a closer look at the missing data. We've already determined that the batter hit by pitch (`TEAM_BATTING_HBP`) variable is missing 91% of its data but what of the other variables. We will just drop the column from the further analysis.

Using the plot below we can visualize the missingness of the remaining variables. There are 5 variables that contain varying degrees of missing data. We will use the information to fill in the missing values in our data preparation step.

`TEAM_BASERUN_CS` appears to be missing the second most amount of values but at only 772 missing values out of 2276 this is much less of a concern than the HBP variable we identified earlier. The remaining variables that are missing data have less than 25% of their data missing so should be safe to impute. 

```{r missing_data, echo=FALSE}
gg_miss_var(trainDf)
```


## 2. Data Preparation

### 2.1 Missing Data

<b><i>
Notes / Questions

- BASE_CS is missing 33% of its values is it an issue to fill that many values
- should we include Hits and Singles, Doubles, Triples, HR - this is the same data captured differently
- Slugging Percentage is an interesting measure that we could add https://en.wikipedia.org/wiki/Slugging_percentage @enwiki:1123388426
- Here are some advanced stats that might be interesting https://www.mlb.com/glossary/advanced-stats
- It would be interesting to test Pythagorean Winning Percentage https://www.mlb.com/glossary/advanced-stats/pythagorean-winning-percentage

</i></b>

As discussed above, we will drop the `INDEX` and `TEAM_BATTING_HBP` variables as the `TEAM_BATTING_HBP` variable is missing 91% of its data and the `INDEX` variable is just an identification Variable.

```{r echo=FALSE}
droppedDf <- trainDf %>% dplyr::select(-c(INDEX, TEAM_BATTING_HBP))
```

We'll also derive a new column for single base hits derived from subtracting double, triples and home runs from the total number of hits.

```{r single_base_hits, echo=FALSE}
droppedDf <- droppedDf %>% mutate(TEAM_BATTING_1B = TEAM_BATTING_H - TEAM_BATTING_2B - TEAM_BATTING_3B - TEAM_BATTING_HR)
```
For further work with NA's, we  create flags to suggest if a variable was missing.
```{r echo=FALSE}
columnNames <- names(droppedDf)
droppedDf <- flag_na(droppedDf, columnNames)
columnNames <- names(evalDf)
evalDf <- flag_na(evalDf, columnNames)
```

To to impute the missing values in the `trainDf` data, the `mice` library is used. To utilize MICE, one must make the assumption that the missing values are missing at random, indicating that the missingness can be explained by variables that have complete information. The MICE algorithm then performs several iterations over the data, as suggested by its name, and generates data to complete the missing values.
We check what impute method we use for each column. `pmm` is predictive mean matching, replacing missing data with column means.

Let's also take a look at the density plots pre and post-imputation to make sure densities look similar. Unfortunately, for `TEAM_BASERUN_SB`, `TEAM_BATTING_SO`, and `TEAM_FIELDING DP` they do not. But in the case of `TEAM_BATTING_SO` our distribution becomes roughly more normal, so it may be beneficial. For the `TEAM_BASERUN_SB` and `TEAM_BASERUN_CS` and `TEAM_FIELDING_DP` we may need to consider alternative methods.

```{r echo=FALSE, warning=FALSE, message=FALSE}
imputeDf <- mice(droppedDf, m = 5, maxit = 50, seed = 123, printFlag = F)
cleanDf <- complete(imputeDf)
m_imputed <- melt(cleanDf)
densityplot(imputeDf)

```

### 2.2 Drop Outliers
To identify outliers, we used historical data from the Lahman's Baseball Database. In baseball's early history,  few games were played in a season (less than 20); thus, the calculation used to normalize the statistics to a 162-game season tends to create outliers. We used the min and max from the modern era (post-1900) as a filter to alleviate the issues created by outlier values.

**ADD MORE TEXT WITH EXPLANATIONS ABOUT FUNCTIONS USED TO WORK WITH OUTLIERS**

```{r drop outliers, echo=FALSE, warning=FALSE, message=FALSE}

columnNames <- c('TEAM_BATTING_H','TEAM_BATTING_2B','TEAM_BATTING_3B','TEAM_BATTING_HR',
                 'TEAM_BATTING_BB','TEAM_BATTING_SO','TEAM_BASERUN_SB','TEAM_BASERUN_CS',
                 'TEAM_PITCHING_H','TEAM_PITCHING_HR','TEAM_PITCHING_BB','TEAM_PITCHING_SO',
                 'TEAM_FIELDING_E','TEAM_FIELDING_DP','TEAM_BATTING_1B')

filterDf <- loadLahmanShortData() %>% filter(yearID >= 1900)
droppedDf <- filterOutliers(filterDf, cleanDf, columnNames)
print(
  dfSummary(droppedDf, 
            varnumbers   = TRUE,
            na.col       = TRUE,
            graph.magnif = .8,
            tmp.img.dir  = "/tmp"),
  method = "render"
)

m_df <- droppedDf %>% melt() 

m_df %>% ggplot(aes(x= value)) + 
    geom_density(color='#023020', fill='gray') + facet_wrap(~variable, scales = 'free',  ncol = 4) + theme_bw()

m_df %>% ggplot(aes(x = value)) +
  geom_boxplot(outlier.color = 'red', outlier.shape = 1) +
  facet_wrap(vars(variable),scales = "free", ncol = 4)

```

### 2.2 Transform non-normal variables
We should also try to transform some variables so that they may fit a more normal distribution, particularly `TEAM_BATTING_HR`, `TEAM_BATTING_SO`, `TEAM_PITCHING_HR`, but also `TEAM_PITCHING_H`, `TEAM_PITCHING_BB`, `TEAM_PITCHING_SO`, and `TEAM_FIELDING_E`. Square rooting these variables helps, as does removing the skewness by reducing the low density ranges.
The plots below compare the original distributions of non-normal variables and transformed ones. This is just one of the ways to handle the data that doesn't follow normal distribution. The  other way is to use Box-Cox transformations, we may try it after fitting the model.

```{r echo=FALSE}
# Reminder of our distributions
m_imputed %>% ggplot(aes(x= value)) + 
    geom_density(fill='gray') + facet_wrap(~variable, scales = 'free') +
    theme(strip.text.x = element_text(size = 6, angle = 0))
# Transform some non-normal variables.
transformed_df <- cleanDf |> mutate_each(funs(sqrt),
                   batting_hr_sqrt = TEAM_BATTING_HR,
                   batting_so_sqrt = TEAM_BATTING_SO,
                   baserun_cs_sqrt = TEAM_BASERUN_CS,
                   pitching_hr_sqrt = TEAM_PITCHING_HR) 

visualize_transformed <- transformed_df |> melt()

# Plot.
visualize_transformed %>% ggplot(aes(x= value)) + 
    geom_density(fill='gray') + facet_wrap(~variable, scales = 'free') +
    theme(strip.text.x = element_text(size = 6, angle = 0))
```


Our current dataset is devoid of any missing data values,the irrelevant `INDEX` and the `TEAM_BATTING_HBP` variables, the outliers. As shown in the table below, no missing data values exist anymore, and we can analyze how the summary statistics may have altered with the imputed data.

```{r echo=FALSE, warning=FALSE, message=FALSE}
print(
  dfSummary(transformed_df, 
            varnumbers   = TRUE,
            na.col       = TRUE,
            graph.magnif = .8,
            tmp.img.dir  = "/tmp"),
  method = "render")

```

## 3. Building models
<b><i>
Notes / Questions

- we could try building a model that does not include any of the outlier variables values
- there is also saber metrics model that Pythagorean Winning Percentage that is supposed to predict wins https://www.mlb.com/glossary/advanced-stats/pythagorean-winning-percentage

</i></b>

At this juncture, with a thorough comprehension of our dataset and having completed the data cleaning process, we can initiate the construction of our multiple linear regression models. We will build four separate linear models and compare their performance. 
To build the models, we will use 4 data frames: original training dataset (trainDf), dataset without missing values (cleanDf), dataset with no outliers and no NA's (droppedDf), dataset with transformations, without NA's (transformedDf).
First, we decided to split our datasets into a training and testing sets (80% training, 20% testing). Then, using our training dataset, we will run our models and check them using testing data.

### 3.1 Model 1: Baseline

For the first multiple linear regression model, we will select all the variables from the original un-cleaned dataset except for `INDEX` (just a row index) and `TEAM_BATTING_HBP` (91% of missing data). We may use this model as a base model to compare with. The results: F-statistic is 82.1, adjusted R-squared 0.433, out of the 15 variables, 6 have statistically significant p-values. The adjusted R2 indicates that only 43% of the variance in the response variable can be explained by the predictor variables. The F-statistic is low and the model’s p-value are not statistically significant. We should look at other models.

```{r echo=FALSE}
trainDf_1 <- trainDf %>% dplyr::select(-c(INDEX, TEAM_BATTING_HBP))
lm1 <- lm(TARGET_WINS ~ . , data = trainDf_1)
lm1Sum <- summary(lm1)
tab_model(lm1Sum, show.df = FALSE, show.aic = TRUE, show.fstat=TRUE, show.se = TRUE, show.ci=FALSE, show.stat=TRUE, digits.p=4)
```

The linear modeling assumption are evaluated using a diagnostic plot, the Breusch–Pagan Test for Heteroscedasticity and Variance Inflation Factor (VIF) to assess colinearity.  

The initial review of the diagnostic plots for this model shows some deviation from linear modeling assumptions. In the Q-Q plot, we see some deviations from the normal distribution at the ends. The residuals-fitted and standardzied residuals-fitted plots show a curve in the main cluster, which indicates that we do not have constant variance. 

```{r}
check_model(lm1, check=c('ncv','qq','homogeneity','outliers'))
```


### 3.2 Model 2: Removed N/A Values
The second model will be based on the cleaned dataset without missing values. We chose variables `TEAM_PITCHING_BB`, `TEAM_BATTING_2B`, `TEAM_BATTING_3B`, `TEAM_FIELDING_E`, `TEAM_PITCHING_H`, `TEAM_BATTING_HR`, `TEAM_BATTING_H` based on the intuition and the understanding of the data. The results: F-statistic is 119.9, adjusted R-squared 0.268, out of the 7 variables, 7 have statistically significant p-values. The adjusted R2 indicates that only 25% of the variance in the response variable can be explained by the predictor variables, which is less than in the original model. The F-statistic is high and the p-value of the model is close to zero, and the model's diagnostic checks are satisfactory, this suggests that we would dismiss the null hypothesis, which assumes that there is no correlation between the explanatory and response variables.

```{r echo=FALSE}
lm2 <- lm(formula = TARGET_WINS ~ TEAM_PITCHING_BB  + TEAM_BATTING_2B + TEAM_BATTING_3B +
   TEAM_FIELDING_E + TEAM_PITCHING_H + TEAM_BATTING_HR + TEAM_BATTING_H, data = cleanDf)
lm2Sum <- summary(lm2)
tab_model(lm2Sum, show.df = FALSE, show.aic = TRUE, show.fstat=TRUE, show.se = TRUE, show.ci=FALSE, show.stat=TRUE, digits.p=4)
```

The initial review of the diagnostic plots for this model shows some deviation from linear modeling assumptions. The Q-Q plot shows the normal distribution but the residuals-fitted and standardzied residuals-fitted plots  don't have contant variance for the fitted values below 60 and above 95.

```{r}
check_model(lm2, check=c('ncv','qq','homogeneity','outliers'))
```

### 3.3 Model 3: Removed Outliers

The third model will use the cleaned dataset with outliers  omitted. Stepwise variable selection based on the AIC score is used to filter the optimal set of features. The resulting model has a significant p-values for the model and all predictor variables. The results: F-statistic is 117.3, adjusted R-squared 0.417. The adjusted R2 indicates that 41% of the variance in the response variable can be explained by the predictor variables. The F-statistic is high and the p-value of the model is close to zero, so we would dismiss the null hypothesis, which assumes that there is no correlation between the explanatory and response variables.
**DAVID, WHY TEAM_BATTING_H WAS REMOVED? COULD YOU PLEASE ADD A SENTENCE EXPLAINING THE CHOICE?**
```{r echo=FALSE, warning=FALSE, message=FALSE}

lm3 <- lm(TARGET_WINS ~ . -TEAM_BATTING_H -na_flag, data=droppedDf)
lm3step <- stepAIC(lm3, trace = FALSE)
lm3sum <- summary(lm3step)
tab_model(lm3step, show.df = FALSE, show.aic = TRUE, show.fstat=TRUE, show.se = TRUE, show.ci=FALSE, show.stat=TRUE, digits.p=4)

```

**WAS BEFORE FROM DAVID*
```{r echo=FALSE, eval=FALSE}
#lm3 <- lm(TARGET_WINS ~ . -TEAM_BATTING_H, data=droppedDf, by=na_flag)
#lm3step <- stepAIC(lm3, trace = FALSE)
#lm3sum <- summary(lm3step)
#tab_model(lm3step, show.df = TRUE, show.aic = TRUE, show.fstat=TRUE, show.se = FALSE, digits.p=4)
```

The initial review of the diagnostic plots for this model shows some deviation from linear modeling assumptions at the boundaries of the prediction range, but the overall QQ plot is almost a flat line; the residuals are flat and exhibit homoscedasticity of variance in the  65 - 95 fitted value range.
```{r echo=FALSE}
check_model(lm3step, check=c('ncv','qq','homogeneity','outliers'))
```

The Breusch–Pagan Test for Heteroscedasticity assumes the following Null and Alternate hypothesis. 

- H0 - Residuals are distributed with equal variance (i.e., homoscedasticity)
- H1 - Residuals are distributed with unequal variance (i.e., heteroscedasticity)

For this model iteration, we reject the null hypothesis and conclude that this model violates the homoscedasticity function.   

```{r echo=FALSE, warning=FALSE, message=FALSE}
lmtest::bptest(lm3step)
```

The Variance Inflation Factor (VIF) calculation detects collinearity with the `TEAM_PITCHING_BB`  and `TEAM_BATTING_BB` variables. Both variables have a VIF score over 5 and are 0.93 correlated. Also, variables TEAM_PITCHING_SO, TEAM_BATTING_SO, TEAM_PITCHING_H, TEAM_BATTING_2B have VIF over 5.

```{r echo=FALSE, warning=FALSE, message=FALSE}

vif_values <- vif(lm3step)
vif_values <- rownames_to_column(as.data.frame(vif_values), var = "var")

vif_values %>%
  ggplot(aes(y=vif_values, x=var)) +
  coord_flip() + 
  geom_hline(yintercept=5, linetype="dashed", color = "red") +
  geom_bar(stat = 'identity', width=0.3 ,position=position_dodge()) 

df <- droppedDf[ , vif_values$var] %>% drop_na()
coeff <- cor(df)
corrplot(coeff, tl.cex = .7 , diag = FALSE ,type = 'upper' ,method = 'number')

```

For the refined model we will drop `TEAM_PITCHING_BB`, `TEAM_BATTING_BB`, and `TEAM_PITCHING_SO` from the model. This model has a lower adjusted R2 of 0.37 but together with high F-statistic of 163.9, statistically significant p-values should align better with the linear regression assumptions.

```{r echo=FALSE, warning=FALSE, message=FALSE}
lm3step.final <- update(lm3step, . ~ . -TEAM_PITCHING_BB -TEAM_BATTING_BB -TEAM_PITCHING_SO - -TEAM_BATTING_SO -TEAM_PITCHING_H -TEAM_BATTING_2B)
lm3finalsum <- summary(lm3step.final)
tab_model(lm3step.final, show.df = FALSE, show.aic = TRUE, show.fstat=TRUE, show.se = TRUE, show.ci=FALSE, show.stat=TRUE, digits.p=4)

```

The diagnostic plots for the new model appear to be closer aligned with the linear regression assumptions. For this model iteration, we will again reject the null hypothesis and conclude that this model violates the homoscedasticity function. And finally the Variance Inflation Factor (VIF) calculation does not detects collinearity across the remaining variables. Overall we will reject model 3 because the residuals are not homoscedastic.

```{r echo=FALSE, warning=FALSE, message=FALSE}
check_model(lm3step.final, check=c('ncv','qq','homogeneity','outliers'))
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
lmtest::bptest(lm3step.final)
```
The model with stepAIC seems the nest out of 3 as all p-values are significant, high F-statistic of 94.48, adjusted R-squared is  0.4185.
```{r echo=FALSE, warning=FALSE, message=FALSE}
tab_model(lm3, lm3step, lm3step.final, 
          dv.labels = c('model 3','model 3 (StepAIC)','model 3 (Final)'),
          show.df = FALSE, show.aic = TRUE, show.fstat=TRUE, show.se = TRUE, show.ci=FALSE, show.stat=TRUE, digits.p=4)

```

### 3.4 Model 4: Variable Transformation

For the fourth model we will use the data with no values missing, with some features transformed and without outliers.
The stepwise variable selection based on the AIC score is used again. The resulting model has a significant p-values for the model and all predictor variables. The results: F-statistic is 98.67, adjusted R-squared 0.371. The F-statistic is high and the p-value of the model is close to zero, so we would dismiss the null hypothesis, which assumes that there is no correlation between the explanatory and response variables.
```{r echo=FALSE}
lm4 <- lm(TARGET_WINS~. -TEAM_PITCHING_HR -TEAM_BATTING_SO -TEAM_BASERUN_CS -TEAM_BATTING_HR -TEAM_PITCHING_BB -TEAM_PITCHING_H -pitching_hr_sqrt -na_flag, data=transformed_df)
lm4step <- stepAIC(lm4, trace=FALSE)
lm4sum <- summary(lm4step)
tab_model(lm4step, show.df = FALSE, show.aic = TRUE, show.fstat=TRUE, show.se = TRUE, show.ci=FALSE, show.stat=TRUE, digits.p=4)
```
The review of the diagnostic plots shows that the overall QQ plot is almost a flat line; the residuals are flat and exhibit homoscedasticity of variance in the  60 - 100 fitted value range.
```{r echo=FALSE, warning=FALSE, message=FALSE}
check_model(lm4step, check=c('ncv','qq','homogeneity','outliers'))
```

## 4. Selecting Models
<b><i>
Notes / Questions

- there are NA's in the predsDf tables with the predictions for every model. Don't know why

</i></b>

The table presents an analysis of the performance metrics for each model on the training dataset. The results suggest that the model's performance slightly improved after incorporating transformations and selecting significant parameters. Below are the results $R^2$, residual standard error, and F-statistics of each model. It happened that the non-cleaned, non-imputed raw training data had the best fitting statistics. Though model 1 doesn't meet assumption requirements. Moel 3 looks like the best fit.
The F-statistic for models 2-3 was large, they all had significant p-values.
During the exploratory data analysis, it was noted that some of the predictor variables had correlations with each other, which is expected in baseball. However, this high correlation between variables could result in multicollinearity, causing unstable regression fits. To mitigate this issue, we employed the Variable Inflation Factor (VIF) method in the previous section to identify better models by selecting variables with VIF values less than 5.
The residuals plots from the part 3 showed that models 1-3 almost follow the normal distribution and almost constant variance.  

**ADD WORDS ABOUT MSE**
```{r echo=FALSE}
plot(compare_performance(lm1,lm2,lm3step,lm4step, rank=T))
```

```{r echo=FALSE}
r <- c(lm1Sum$r.squared, lm2Sum$r.squared, lm3sum$r.squared, lm4sum$r.squared)
mse <- c(lm1Sum$sigma, lm2Sum$sigma, lm3sum$sigma, lm4sum$sigma)
adjusted.r <- c(lm1Sum$adj.r.squared, lm2Sum$adj.r.squared, lm3sum$adj.r.squared, lm4sum$adj.r.squared)
modelDf <- data.frame(r,mse,adjusted.r)
kable(modelDf, caption = "Money Ball Dataset", digits = 2, format = "html", row.names = F) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover"),
    full_width = F,
    position = "center"
  )
```

Next, we apply the models to the evaluation dataset to make predictions. However, to ensure that the models 2-4 work properly, we will fill in the missing values in the evaluation data set using the same imputation method - mice, drop columns `INDEX`, `TEAM_BATTING_HBP`, for model 3 outliers were removed, for model 4 we will also make necessary transformations. 

```{r echo=FALSE}
#removed columns
evalDroppedDf <- evalDf %>% dplyr::select(-c(INDEX, TEAM_BATTING_HBP))
evalDroppedDf <- evalDroppedDf %>% mutate(TEAM_BATTING_1B = TEAM_BATTING_H - TEAM_BATTING_2B - TEAM_BATTING_3B - TEAM_BATTING_HR)

#no NA's
evalcleanDf <- mice(evalDroppedDf, m = 5, maxit = 50, seed = 123, printFlag = F)
evalcleanDf <- complete(evalcleanDf)

#with transformations
transformed_eval <- evalcleanDf |> mutate_each(funs(sqrt),
                   batting_hr_sqrt = TEAM_BATTING_HR,
                   batting_so_sqrt = TEAM_BATTING_SO,
                   baserun_cs_sqrt = TEAM_BASERUN_CS,
                   pitching_hr_sqrt = TEAM_PITCHING_HR)

```

```{r echo=FALSE}
lm1Pred <- lm1 %>% predict(evalcleanDf)
lm2Pred <- lm2 %>% predict(evalcleanDf)
aiclm3 <- lm3step %>% predict(evalcleanDf)
aiclm4 <- lm4step %>% predict(transformed_eval)
```
The table presents the predicted values of `TARGET_WINS` for each model. First thing to notice is that the first model predicts negative number of wins, which is unrealistic. Though we won't concentrate on this model. The third and fourth AIC-generated models (no outliers and transformed values) have similar outcomes, and both are performing well. The second model also produces similar results but there was an issue with model assumptions.
```{r echo=FALSE}
predsDf <- evalDf %>% 
  mutate(lm1 = lm1Pred, lm2 = lm2Pred, aic3 = aiclm3, aic4 = aiclm4) %>%
  dplyr::select(c(lm1, lm2, aic3, aic4))
  kable(head(predsDf, 15), caption = "Money Ball Dataset", digits = 2, format = "html", row.names = F) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover"),
    full_width = F,
    position = "center"
  )
```

We can also see when plotting the predictions that there doesn't seem to be much obvious difference between the models aside from the clearly outrageous outliers generated by the first model, and the 4th model showing a slightly tighter range in wins.


**TEXT IS NEEDED HERE**
```{r echo=FALSE}
xT <- evalcleanDf %>% 
  mutate(lm1 = lm1Pred, lm2 = lm2Pred, aic3 = aiclm3, aic4 = aiclm4)
par(mfrow=c(2,2))
plot(1:nrow(xT), xT$lm1, xlab="LM1", ylab="Wins", title="LM1")
plot(1:nrow(xT), xT$lm2, xlab="LM2", ylab="Wins", title="LM2")
plot(1:nrow(xT), xT$aic, xlab="AIC LM3", ylab="Wins", title="AIC LM3")
plot(1:nrow(xT), xT$aic4, xlab="AIC LM4", ylab="Wins", title="AIC LM4")
```


## 5. Conclusion
In general, we don't much faith in any of the models we produced when it comes to their predictive power. The model 1 that had the highest $R^2$ value was compromised by missing data, leading to highly inaccurate negative predictions, though model 3 has also high $R^2$ and doesn't produce negative results. The second and fourth models demonstrated a weak overall fit, as evidenced by their lower $R^2$ scores. Furthermore, none of the models displayed satisfactory performance when assessed for linearity or homogeneity of variance.
According to the analysis, Models 1 and 3 performed slightly better than Models 1 and 4 and could be considered as the preferred linear models based on the adjusted R2. However, Model 3 is more statistically significant compared to the others and uses fewer unnecessary variables for making predictions while still maintaining a similar level of adjusted R2. Considering these factors, and its better handling of multicollinearity (with fewer instances of sign-flipping in coefficients), Model 3 would be the recommended choice of model.



## References



## Appendix A: R code




## Appendix B: Lahman’s Baseball Database

Despite significant efforts to compensate for poor data quality, the resulting models are poor predictors of win totals. Moreover, the poor data quality is inconsistent with the overall state of baseball statistics. When it comes to the major sports, baseball has the most mature statistics available. Therefore finding better data is the best course of action for developing a better predictive model. 

We were able to locate a cleaner version of the same data set provided in the class. The Lahman's Baseball Database includes the same variables as the sample database with fewer errors and additional reference data that would allow us to connect the database to other sources. 

https://www.seanlahman.com/baseball-archive/statistics/

```{r}
teamDF <- loadLahmanData()
```

A significant advantage of Lahman's data set over the data set provided in class is that it includes information about the year and the team. This data is valuable when considering how baseball has changed over the years. The modern era in baseball is often delineated by the turn of the century. However, when looking at the past 120 years of baseball history, it is easy to pinpoint rule changes, evolutions in playing strategy, and league structure that have fundamentally impacted the game.  

When comparing statistics across time, it is common to use many of the breakdowns below to add context to the analysis:

- The Dead Ball Era (1901 - 1920)
- World War 2 (1941 - 1945)
- Segregation Era (1901 - 1947ish)
- Post-War Era/Yankees Era (1945 - late 50s/early 60s) 
- Westward Expansion (1953 - 1961)
- Dead Ball 2 (The Sixties, roughly)
- Designated Hitter Era (1973 - current, AL only) 
- Free Agency/Arbitration Era (1975 - current) 
- Steroid Era (unknown, but late 80s - 2005 seems likely) 
- Wild Card Era (1994 - current)

Surveying these periods would suggest that a more granular model has the potential to perform better. 

Although we could have chosen any number of the time periods above, exploring the statistical outliers highlights that many of these values correspond to the pre-1969 period. This delineation has some historical support. As Jayson Stark of ESPN argues in this article (https://www.espn.com/mlb/columns/story?columnist=stark_jayson&id=2471349) In 1969 the MLB underwent several rule changes and changes to the league structure that impacted win totals and team statistics. 1969 was the first year of division play and the expanded postseason. The Pitcher's Mound was lowered five inches. The Strike zone shrinks. Five-person rotations kicking in. The save was invented. And more expansion to the unbalanced schedules. 

Thus using 1900 as the beginning of the modern era and 1969 as an additional breakpoint, the dataset can be divided into three segments. The density profiles for the predictor variables approach a normal distribution when grouped by the three segments we identified. To support data exploration, we added a era_cat field to the data set.

```{r}
teamAdjDF <- loadLahmanShortData()

random_sample <- createDataPartition(teamAdjDF$TARGET_WINS,p = 0.8, list = FALSE)

trainingTeam_df <- teamAdjDF[random_sample, ]
testingTeam_df <- teamAdjDF[-random_sample, ]
```

In general the Lahman's data set contains fewer data gaps and the variables are more consistently distributed. There are some missing values in the data set including the Caught Stealing (CS/TEAM_BASERUN_CS) variable is missing 27.9%; the Batters Hit by Pitch (HBP/TEAM_BATTING_HBP) variable is missing 38.8%; and the Sacrifice Flies (SF) variable is missing 51.6% of the values. 

Most of the variables in the data set show some level of skewness, with the following variables having a Kurtosis measure of greater than 3, TEAM_BATTING_H, TEAM_BASERUN_SB, TEAM_BASERUN_CS, TEAM_PITCHING_H, and TEAM_FIELDING_E

The Lahman data set contains several variables with bimodal distributions, including, TEAM_BATTING_HR, TEAM_BATTING_SO, TEAM_BATTING_HR, and TEAM_PITCHING_SO. 

```{r}
print(
  dfSummary(teamDF, 
            varnumbers   = TRUE,
            na.col       = TRUE,
            graph.magnif = .8,
            tmp.img.dir  = "/tmp"),
  method = "render")
```

```{r}
m_df <- teamAdjDF[random_sample, ] %>% melt() 

m_df %>% ggplot(aes(x= value)) + 
      geom_density(color='#023020', fill='gray') + 
      facet_wrap(~variable, scales = 'free',  ncol = 4) + 
      theme_bw() +
      labs(title = 'Variable Density Plots')
```
When we group the statistics by the three categories presented earliers, we see a much cleaner density plot across all variables. There are few signs of bimodal distributions, and the skewness of individual variables is reduced greatly.

```{r}
m_df <- teamAdjDF[random_sample, ] %>% filter(era_cat == '1969+') %>% melt() 

m_df %>% ggplot(aes(x= value)) + 
      geom_density(color='#023020', fill='gray') + 
      facet_wrap(~variable, scales = 'free',  ncol = 4) + 
      theme_bw() +
      labs(title = 'Variable Density Plots (1969+)')
```

The training data set exibits the following characteristics.
```{r}
print(
  dfSummary(trainingTeam_df, 
            varnumbers   = TRUE,
            na.col       = TRUE,
            graph.magnif = .8,
            tmp.img.dir  = "/tmp"),
  method = "render")
```

```{r}
m_df <- trainingTeam_df %>% melt() 

m_df %>% ggplot(aes(x= value)) + 
    geom_density(color='#023020', fill='gray') + facet_wrap(~variable, scales = 'free',  ncol = 4) + theme_bw()


m_df %>% ggplot(aes(x = value)) +
  geom_boxplot(outlier.color = 'red', outlier.shape = 1) +
  facet_wrap(vars(variable),scales = "free", ncol = 4)
```

```{r}
rcore <- rcorr(as.matrix(trainingTeam_df %>% dplyr::select(where(is.numeric))))
coeff <- rcore$r
corrplot(coeff, tl.cex = .7 , method = 'pie')
```

**DATA PREPARATION**

The use of the era_cat variable allows us to group the data set into 3 categories that variables that approach normal distribution. 

Since segmenting the data set using the era_cat variable creates 3 categories of variables that approach the normal distribution, we will focus our data preparation step on missing values. The documentation for MICE package recommends that a 5% threshold should be observed for safely imputing missing values. Based on this rule of thumb we will drop the 'TEAM_BASERUN_CS' and the 'TEAM_BATTING_HBP' variables.

The remaining variables with missing data ('TEAM_BATTING_BB','TEAM_BATTING_SO' and 'TEAM_BASERUN_SB') will be impuned using the MICE package. Several methods can be used but for simplicity we selected m=5 the default method.
```{r}
trainingTeam_df <- trainingTeam_df %>% dplyr::select(-c(TEAM_BASERUN_CS,TEAM_BATTING_HBP))
testingTeam_df <- testingTeam_df %>% dplyr::select(-c(TEAM_BASERUN_CS,TEAM_BATTING_HBP))
```

```{r}
imputeDf <- mice(trainingTeam_df, m = 5, maxit = 50, seed = 123, printFlag = F)
imputeDf$meth
trainingTeam_df <- complete(imputeDf)
m_imputed <- melt(trainingTeam_df)
densityplot(imputeDf)
```

**Build Model**

The first model includes all the variables from the original data set with the exception of the yearID. It could be argued that year might have an impact on the numbers, however we are accounting for year with the era_cat variable. The initial model has an $AdjR^2 = 0.8078$, and the model selected by stepAIC includes the same variables and has the same $AdjR^2=8078$.
```{r}
lmA1 <- lm(TARGET_WINS ~ . -yearID, data = trainingTeam_df) #by=era_cat)
summary(lmA1)

lmA1.step <- stepAIC(lmA1, trace = FALSE, by=era_cat)
summary(lmA1.step)
```

The `TEAM_BATTING_3B` variable was dropped due to a low p-values, giving us the final model with an $AdjR^2=0.8078$. 

```{r}
lmA1.final <- update(lmA1.step, . ~ . -TEAM_BATTING_3B)
summary(lmA1.final)
```

There are a number of issues with the model diagnostics. The Linearity, and QQ plots show deviation from the straight line at the boundaries. In addition, the reference line for the Homogeneity of Variance graph is not flat. This indicates that residual variance is not consistent across the model values. Further exploration could be conducted to see if there are smaller date ranges and prediction ranges that generate a model that better aligns with the linear regression assumptions. 

```{r}
check_model(lmA1.final, check=c('ncv','qq','homogeneity','outliers'))
```

**PREDICT** 

The next step is to use the selected model to predict the testing data set. The Yardstick package was used to calculate model performance, including the $R^2$. With an $R^2=0.8318$ the performance on the testing data set is consistent with the model summary. In the Residual vs. TARGET_WINS graph, there appears to be wins range between 60 and 110 that generates more accurate forecasts. However, it should be noted that the residuals appear to drift downwards. TARGET_WINS less than the mean are overestimated and TARGET_WINS greater than the mean is over estimated. 
```{r}
imputeDf <- mice(testingTeam_df, m = 5, maxit = 50, seed = 123, printFlag = F)
imputeDf$meth
testingTeam_df <- complete(imputeDf)
m_imputed <- melt(testingTeam_df)
densityplot(imputeDf)
```

```{r}
lmA1Pred.final <- lmA1.final %>% predict(testingTeam_df)
```

```{r}
multi_metric <- metric_set(mape, smape, mase, mpe, rmse, rsq)
m <- testingTeam_df %>% multi_metric(truth=testingTeam_df$TARGET_WINS, estimate=lmA1Pred.final)

kable(m, digits = 4, format = "html", row.names = T) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover"),
    full_width = F,
    position = "center"
  )
```


```{r}
n <- nrow(testingTeam_df)
x <- testingTeam_df$TARGET_WINS
e <- lmA1Pred.final - testingTeam_df$TARGET_WINS 


plot(x, e,  
     xlab = "wins", 
     ylab = "residuals",
     bg = "steelblue", 
     col = "darkgray", cex = 1.5, pch = 21, frame = FALSE)
abline(h = 0, lwd = 2)
for (i in 1 : n) 
  lines(c(x[i], x[i]), c(e[i], 0), col = "red" , lwd = 1)
```

## Appendix c: Pythagorean Model

Bill James developed the Pythagorean winning percentage. The concept strives to calculate the number of games a team should win based on the total offense and the number of runs allowed. Since this model includes total runs scored vs. total runs allowed the expectation is that this model will be a good predictor of a team's wins in a given season. 

$Win Percentage = (Runs Scored)^2 / [ (Runs Scored)^2 + (Runs Allowed)^2]$


```{r}
random_sample <- createDataPartition(teamDF$TARGET_WINS, p = 0.8, list = FALSE)

trainingTeam_df <- teamDF[random_sample, ]
testingTeam_df <- teamDF[-random_sample, ]
```

As we expected, the Pythagorean model performs well $Adj R^2$ = 0.9112. However, this linear model differs slightly from the formal definition since it includes an intercept of 5.21 and a coefficient for the Pythagorean factor of 151.39. The formal definition of the model would have an intercept of 0 and a coefficient of 162. 

```{r}
lmp <- lm(TARGET_WINS ~ pythPercent, data = trainingTeam_df)
summary(lmp)
```

The diagnostic plots show residuals that are normally distributed with a linear relationship to the fitted values. The homogeneity of variance is curved, indicating that the variance of residuals is not consistent. 

```{r}
check_model(lmp, check=c('ncv','qq','homogeneity','outliers'))
```

The next step is to use the Pythagorean Model to predict the testing data set. The Yardstick package was used to calculate model performance, including the $R^2$.  With an $R^2=0.9131$ the performance on the testing data set is consistent with the model summary. In the Residual vs. TARGET_WINS graph there appears to be a wins range between 50 and 130 that generates more accurate forecasts.  

```{r}
lmpPred <- lmp %>% predict(testingTeam_df)
hist(lmpPred)
```

```{r}
m1 <- testingTeam_df %>% multi_metric(truth=testingTeam_df$TARGET_WINS, estimate=lmpPred)

kable(m1, digits = 4, format = "html", row.names = T) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover"),
    full_width = F,
    position = "center"
  )
```

```{r}
n <- nrow(testingTeam_df)
x <- testingTeam_df$TARGET_WINS
e <- lmpPred - testingTeam_df$TARGET_WINS 


plot(x, e,  
     xlab = "wins", 
     ylab = "residuals", 
     bg = "steelblue", 
     col = "darkgray", cex = 1.5, pch = 21, frame = FALSE)
abline(h = 0, lwd = 2)
for (i in 1 : n) 
  lines(c(x[i], x[i]), c(e[i], 0), col = "red" , lwd = 1)
```

Unsurprising the Pythagorean Model performs well when it comes to win projections. There is clear relationship between the total yearly run differential and the number of games won. It is a simple model but efficient. 


