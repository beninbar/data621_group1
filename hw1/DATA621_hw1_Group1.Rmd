---
title: 'Homework #1: Moneyball'
subtitle: 'Critical Thinking Group 1'
author: 'Ben Inbar, Cliff Lee, Daria Dubovskaia, David Simbandumwe, Jeff Parks, Nick Oliver'
output:
  html_document:
    toc: yes
    toc_float: yes
    theme: united
  pdf_document:
    toc: yes
editor_options:
  chunk_output_type: console
  markdown: 
    wrap: sentence
bibliography: references.bib
---

```{r setup, include=FALSE}
library(tidyverse)
library(reshape2)
library(kableExtra)
library(Matrix)
library(MASS)
library(mice)
library(Hmisc)
library(corrplot)
library(performance)
library(naniar)
library(psych)
library(GGally)
library(campfin)
library(caret)
library(yardstick)
library(summarytools)
library(sjPlot)
library(car)
library(rsample)
library(olsrr) 
library(jtools)

knitr::opts_chunk$set(echo = F, 
                      warning = F, 
                      message = F, 
                      eval = T , 
                      results="asis", 
                      fig.height=6, 
                      fig.width=8)
set.seed(1234)
```

```{r functions}

st_css()

 st_options(
   plain.ascii = FALSE,
   style = 'grid',
   dfSummary.style ='grid',
   freq.silent  = TRUE,
   headings     = FALSE,
   tmp.img.dir  = "./tmp",
   dfSummary.custom.1 =
     expression(
       paste(
         "Q1 - Q3 :",
         round(
           quantile(column_data, probs = .25, type = 2,
                    names = FALSE, na.rm = TRUE), digits = 1
         ), " - ",
         round(
           quantile(column_data, probs = .75, type = 2,
                    names = FALSE, na.rm = TRUE), digits = 1
         )
       )
     )
 )

source('./hw1/Functions.R', local = knitr::knit_global())
 
```

```{r load_data}
trainDf <- read.csv("https://raw.githubusercontent.com/cliftonleesps/data621_group1/main/hw1/moneyball-training-data.csv")
evalDf <- read.csv("https://raw.githubusercontent.com/cliftonleesps/data621_group1/main/hw1/moneyball-evaluation-data.csv")
```

## Overview

Using historical statistics to predict future outcomes, wins and losses, and opportunities for improving performance has gained significant attention in professional sports.

This analysis aims to develop several models to predict a baseball team's wins over a season based on team stats such as home runs, strikeouts, base hits, and more.

Our approach:

1.  We will begin by examining the data for missing values or outliers and take the necessary measures to clean the data.

2.  We will train and evaluate four different multivariate linear models that forecast seasonal wins using variants of the dataset split into training and evaluation sets.

3.  Finally, we will choose the best model that balances accuracy and simplicity for predicting seasonal wins.

------------------------------------------------------------------------

## 1. Date Exploration

The baseball training dataset contains 2,276 observations of 17 variables detailing various teams' performances from 1871 to 2006.
The descriptions of each column are below.

Some initial observations:

-   Due to the relatively long period, we expect to see outliers and missing data as the league modified official game rules; these rule changes undoubtedly caused teams and players to change their tactics in response.

-   The number of single base hits is noticeably missing from the dataset, but we can derive this value by subtracting other hit types (doubles, triples, home runs) from total hits.

-   Lastly, other columns representing game number (out of 162), inning number (1-9), and matching opponent columns would be helpful for predictions.

<img src="https://raw.githubusercontent.com/ex-pr/DATA607/Project-1/a1_moneyball.png" width="1200" style="display: block; margin-left: auto; margin-right: auto; width: 75%;"/>

### 1.1 Summary Statistics

The table below provides valuable descriptive statistics about the training data.
These variables are all integers, and many have a minimum of zero.

One interesting piece of information is the min/max of the TARGET_WINS variable, where a value of zero suggests teams that did not win a single game in a given season.
The maximum is 146, indicating no teams in the training dataset had a perfect 162-game season.

Also of note is the number of missing values from certain variables, which may be due to changes in official rules or tactics before the modern era.
We'll explore and handle these missing values in a later step.

Numerous extreme cases seem implausible; several teams have zero strikeouts by their pitchers over the season, and one achieved 20,000 strikeouts.
We'll also address these values in the coming steps.

```{r df_summary}
print(
  dfSummary(trainDf, 
            varnumbers   = TRUE,
            na.col       = TRUE,
            graph.magnif = .8,
            tmp.img.dir  = "/tmp"),
  method = "render"
)

```

### 1.2 Distribution

Next, we'll visually check for normality and outliers in the dependent and independent variables.

The density plots display near-normal distributions for most variables with some exceptions.
Right-skewness is evident for PITCHING_H (hits allowed) and FIELDING_E (fielding errors), and bimodal distributions are observed in the density plots for BATTING_HR (home runs) and BATTING_SO (strikeouts), implying two distinct clusters of high-scoring and low-scoring teams.

```{r density_plot}
m_df <- trainDf %>% dplyr::select(-c(INDEX)) %>% melt() 

m_df %>% ggplot(aes(x= value)) + 
geom_density(color='#023020', fill='gray') + facet_wrap(~variable, scales = 'free',  ncol = 4) + theme_bw()
```

Additionally, the boxplots indicate a large number of outliers exist outside of the interquartile ranges for many of the variables - all these effects will need to be carefully considered.

```{r boxplot}

m_df %>% ggplot(aes(x = value)) +
  geom_boxplot(outlier.color = 'red', outlier.shape = 1) +
  facet_wrap(vars(variable),scales = "free", ncol = 4)

```

The function featurePlot() demonstrates the relationships between each independent variable and the target `TARGET_WINS`.

```{r feature_plot}
featurePlot(trainDf[,2:ncol(trainDf)], trainDf[,1], plot = "scatter", type = c("p", "smooth"), span = 1)
```

### 1.3 Correlation Matrix

Plotting the correlations between the target TARGET_WINS and the variables (excluding INDEX and TEAM_BATTING_HBP), we can see that very few variables correlate with the target.
Variables with correlations close to zero are unlikely to offer significant insights into the factors contibuting to a team's victories.

```{r correlation_matrix}
rcore <- rcorr(as.matrix(trainDf %>% dplyr::select(where(is.numeric) & -INDEX & -TEAM_BATTING_HBP)))
coeff <- rcore$r
corrplot(coeff, tl.cex = .7 , method = 'circle')
```

```{r corr_numbers}
tst <- trainDf
tst <- tst[,-1 ]
kable(cor(drop_na(tst))[,1], "html", escape = F, col.names = c('Coefficient')) %>%
  kable_styling("striped", full_width = F) %>%
  column_spec(1, bold = T)
```

To avoid multicollinearity, we should exclude some variables from the model that have high degrees of correlation with each other.
By comparing offensive with defensive stats (variables starting with BATTING or BASERUN), we can see potential cases of multicollinearity.
We might interpret this as some teams being exceptional at both hitting (offense) and fielding (defense).

Further, a typical team's number of TEAM_BATTING_HR (home runs) and TEAM_PITCHING_HR (allowed home runs) has a correlation of nearly 1.0.
This is an unexpected correlation but can be explained by noticing most games are decided by a difference of one or two runs.
Any final models should probably exclude one of these two home run variables.

Alternatively, the correlation between a team's BATTING_H (hits) and PITCHING_H (hits allowed) is around 0.3 which seems reasonable.

Some other strong correlations are less obvious such as TEAM_FIELDING_E (errors) being strongly negatively correlated with TEAM_BATTING_BB (walks by batters) and TEAM_BATTING_HR (home runs).

Digging a little deeper we can see that TEAM_FIELDING_E (errors) has a strong negative Pearson correlation coefficient of r round(cor(trainDf\$TEAM_FIELDING_E, trainDf\$TEAM_BATTING_BB),3) with TEAM_BATTING_BB (walks by batters), and a strong positive correlation of r round(cor(trainDf\$TEAM_FIELDING_E, trainDf\$TEAM_PITCHING_H),3) with TEAM_PITCHING_H (pitching hits allowed).
Altogether, this seems to support an overall correlation of strong offensive stats with strong defensive stats (and vice versa.)

------------------------------------------------------------------------

## 2. Data Preparation

### 2.1 Missing Data

Let's take a closer look at the missing data using the plot below, highlighting six variables with various percentages of missing values:

```{r missing_data,fig.height=4}
gg_miss_var(trainDf, show_pct=TRUE)
```

Most notably, TEAM_BATTING_HBP (batter hit by pitch) is missing values in 91% of the cases; we'll remove this variable from the dataset because there is not enough information to estimate sensible values.

The variable TEAM_BASERUN_CS (caught stealing) is also missing data in 34% of the cases; instead of dropping this variable and losing the information present, we'll use imputation methods to fill in values we might reasonably expect.

The remaining variables are missing data in fewer than 25% of the cases, so we'll impute expected values here too.

Additionally, we'll drop the INDEX variable as it is just an identification label for each case.

```{r dropped_df}
droppedDf <- trainDf %>% dplyr::select(-c(INDEX, TEAM_BATTING_HBP))
```

### 2.2 Impute Missing Values (cleanDF)

To handle the cases with missing values, we'll create a new variable na_flag to label if any variables were missing for each case, and use the MICE (Multiple Imputation by Chained Equations) library to impute values for our missing data.

```{r missing_flags}
columnNames <- names(droppedDf)
droppedDf <- flag_na(droppedDf, columnNames)
columnNames <- names(evalDf)
evalDf <- flag_na(evalDf, columnNames)
```

One of the assumptions of MICE is that the missing values in our dataset occur randomly, and be estimated by using the existing values in the rest of the dataset.
The MICE algorithm performs several iterations over the data and generates data to complete the missing values.

One of the most common algorithms used by MICE is Predictive Mean Matching (PMM), which imputes missing values based on the means of the existing values.

Here are the new density plots of our five variables after imputation with MICE.
The blue plots represent the original values with missing data, and the red plots indicate multiple passes of the MICE algorithm with imputed values.

```{r density_plots_post,fig.height=4}
imputeDf <- mice(droppedDf, m = 5, maxit = 50, seed = 123, printFlag = F)
cleanDf <- complete(imputeDf)
m_imputed <- melt(cleanDf)
densityplot(imputeDf)
```

```{r drop_na_flag}
cleanDf <- cleanDf %>% dplyr::select(-na_flag)
droppedDf <- droppedDf %>% dplyr::select(-na_flag)
```

After imputation, the distribution of `TEAM_BATTING_SO` becomes roughly more normally distributed, so it may be beneficial for the model.
However, the remaining variables may need to be handled via alternative methods.

### 2.3 Remove Outliers (droppedDF)

After exploring outliers using methods based on standard deviation and IQR, we decided to try another approach to preserve as much information for our models as possible.

We used historical data from the Lahman's Baseball Database to identify outliers.
Early baseball teams played fewer than 20 games per season, and the calculation used in our project dataset to normalize the statistics to a 162-game season tended to create outliers.

We used the min and max from the modern era (post-1900) as a filter to alleviate the issues created by outlier values.
We compared each statistic listed in the column names to the min and max for the same statistic in the modern era, and we dropped the row if the values were below the min or above the max.

```{r drop_outliers}

# list of columns included in in the outlier filtering
columnNames <- c('TEAM_BATTING_H','TEAM_BATTING_2B','TEAM_BATTING_3B','TEAM_BATTING_HR',
                 'TEAM_BATTING_BB','TEAM_BATTING_SO','TEAM_BASERUN_SB','TEAM_BASERUN_CS',
                 'TEAM_PITCHING_H','TEAM_PITCHING_HR','TEAM_PITCHING_BB','TEAM_PITCHING_SO',
                 'TEAM_FIELDING_E','TEAM_FIELDING_DP','TEAM_BATTING_1B')

# Filter 
filterDf <- loadLahmanShortData() %>% filter(yearID >= 1900)
droppedDf <- filterOutliers(filterDf, cleanDf, columnNames)

# print stats for new model
print(
  dfSummary(droppedDf, 
            varnumbers   = TRUE,
            na.col       = TRUE,
            graph.magnif = .8,
            tmp.img.dir  = "/tmp"),
  method = "render"
)

m_df <- droppedDf %>% melt() 

m_df %>% ggplot(aes(x= value)) + 
    geom_density(color='#023020', fill='gray') + facet_wrap(~variable, scales = 'free',  ncol = 4) + theme_bw()

m_df %>% ggplot(aes(x = value)) +
  geom_boxplot(outlier.color = 'red', outlier.shape = 1) +
  facet_wrap(vars(variable),scales = "free", ncol = 4)

```

### 2.4 New Variables

We'll also derive a new column TEAM_BATTING_1B (single base hits) by subtracting doubles, triples, and home runs from the total number of hits per case.

```{r single_base_hits}
droppedDf <- droppedDf %>% mutate(TEAM_BATTING_1B = TEAM_BATTING_H - TEAM_BATTING_2B - TEAM_BATTING_3B - TEAM_BATTING_HR)
```

### 2.5 Transform Non-Normal Variables (transformedDF)

We should also try to transform some variables to fit a more normal distribution, including `TEAM_BATTING_HR`, `TEAM_BATTING_SO`, `TEAM_PITCHING_HR`, and `TEAM_BASERUN_CS`.

We apply a square-root transformation to these variables and remove the skewness by reducing the low-density ranges.
The plots below compare the original distributions of non-normal variables and transformed ones.

```{r transform_nonnormal}
# Reminder of our distributions
m_imputed %>% ggplot(aes(x= value)) + 
    geom_density(fill='gray') + facet_wrap(~variable, scales = 'free', ncol=4) +
    theme(strip.text.x = element_text(size = 6, angle = 0)) +
    labs(title='Training w/Imputed Missing Values')

# Transform some non-normal variables.
transformed_df <- cleanDf |> mutate_each(funs(sqrt),
                   batting_hr_sqrt = TEAM_BATTING_HR,
                   batting_so_sqrt = TEAM_BATTING_SO,
                   baserun_cs_sqrt = TEAM_BASERUN_CS,
                   pitching_hr_sqrt = TEAM_PITCHING_HR) 

visualize_transformed <- transformed_df |> melt()

# Plot.
visualize_transformed %>% ggplot(aes(x= value)) + 
    geom_density(fill='gray') + facet_wrap(~variable, scales = 'free', ncol=4) +
    theme(strip.text.x = element_text(size = 6, angle = 0)) +
    labs(title='Training w/Imputed Missing Values + SQRT Transform')
```

Our current dataset is free of missing data values, the irrelevant INDEX and the TEAM_BATTING_HBP variables, and the outliers.
As shown in the table below, no missing data values exist anymore, and we can analyze how the summary statistics may have altered with the imputed data.

```{r df_summary_normalized}
print(
  dfSummary(transformed_df, 
            varnumbers   = TRUE,
            na.col       = TRUE,
            graph.magnif = .8,
            tmp.img.dir  = "/tmp"),
  method = "render")

```

------------------------------------------------------------------------

## 3. Building Models

Having thoroughly explored our dataset and completed the data cleaning process, we can initiate the construction of our multivariate linear regression models.

We'll build our models using four variants of our training dataset:

1.  The original training dataset **(trainDf**),

2.  The training dataset with missing data variables imputed using MICE (**cleanDf**),

3.  The training dataset with outliers dropped, using stepwise selection (**droppedDf**), and

4.  The dataset with transforms of non-normal variables, using stepwise selection (**transformedDf**).

------------------------------------------------------------------------

### 3.1 Model 1 - Baseline

#### Dataset - trainDf

For the first multivariate linear regression model, we will select all the variables from the original un-cleaned dataset except for `INDEX` (just a row index) and `TEAM_BATTING_HBP` (missing data in 91% of cases).
We may use this model as a base model for comparison.

```{r lm1}
trainDf_1 <- trainDf %>% dplyr::select(-c(INDEX, TEAM_BATTING_HBP))
lm1 <- lm(TARGET_WINS ~ . , data = trainDf_1)
lm1Sum <- summary(lm1)
summ(lm1, digits = getOption("jtools-digits", 4))
```

\

#### Model Results

The F-statistic is 82.1, the adjusted R-squared is 0.433, and out of the 15 variables, 6 have statistically significant p-values.

The adjusted R2 indicates that only 43% of the variance in the response variable can be explained by the predictor variables.
The F-statistic is low and the model's p-value is not statistically significant.
We should probably look at other models for better performance.

#### Checking Model Assumptions

We evaluate the linear modeling assumptions using standard diagnostic plots, the Breusch--Pagan Test for Heteroscedasticity and Variance Inflation Factor (VIF) to assess colinearity.

The initial review of the diagnostic plots for this model shows some deviation from linear modeling assumptions.
The Q-Q plot shows some deviations from the normal distribution at the ends.
The residuals-fitted and standardized residuals-fitted plots show a curve in the main cluster, which indicates that we do not have constant variance.

```{r check_lm1}
check_model(lm1, check=c('ncv','qq','homogeneity','outliers'))
```

------------------------------------------------------------------------

### 3.2 Model 2 - Missing Values Imputed

#### Dataset - cleanDF

Our second model will be based on the cleaned dataset which doesn't have missing values.
We chose the variables `TEAM_PITCHING_BB`, `TEAM_BATTING_2B`, `TEAM_BATTING_3B`, `TEAM_FIELDING_E`, `TEAM_PITCHING_H`, `TEAM_BATTING_HR`, `TEAM_BATTING_H` based on our understanding of the data.

```{r lm2}
lm2 <- lm(formula = TARGET_WINS ~ TEAM_PITCHING_BB  + TEAM_BATTING_2B + TEAM_BATTING_3B +
   TEAM_FIELDING_E + TEAM_PITCHING_H + TEAM_BATTING_HR + TEAM_BATTING_H, data = cleanDf)
lm2Sum <- summary(lm2)
summ(lm2, digits = getOption("jtools-digits", 4))
```

\

#### Model Results

The F-statistic is 119.9, the adjusted R-squared is 0.268, and all 7 variables have statistically significant p-values.

The adjusted R2 indicates that only 26% of the variance in the response variable can be explained by the predictor variables, which is less than in the original model.

The F-statistic is high, the p-value of the model is close to zero, and the model's diagnostic checks are satisfactory.
We would dismiss the null hypothesis, which assumes no correlation between the explanatory and response variables.

#### Checking Model Assumptions

The initial review of the diagnostic plots for this model shows some deviation from linear modeling assumptions.
The Q-Q plot shows the normal distribution, but the residuals-fitted and standardzied residuals-fitted plots don't have constant variance for the fitted values below 60 and above 95.

```{r check_lm2}
check_model(lm2, check=c('ncv','qq','homogeneity','outliers'))
```

------------------------------------------------------------------------

### 3.3 Model 3 - Outliers Removed and Stepwise Selection

#### Dataset - droppedDF

The third model uses the cleaned dataset with outliers omitted, and we dropped `TEAM_BATTING_H` due to colinearity with `TEAM_BATTING_1B`, `TEAM_BATTING_2B`, `TEAM_BATTING_3B` and `TEAM_BATTING_HR`.

We use stepwise variable selection based on their AIC score to identify the optimal set of features.
The resulting model has significant p-values for the model and all predictor variables.

```{r lm3}

lm3 <- lm(TARGET_WINS ~ . -TEAM_BATTING_H, data=droppedDf)
lm3step <- stepAIC(lm3, trace = FALSE)
lm3sum <- summary(lm3step)
summ(lm3step, digits = getOption("jtools-digits", 4))

```

\

#### Model Results

The F-statistic is 	118, the adjusted R-squared 0.42, and all 12 variables have statistically significant p-values.

The adjusted R2 indicates that 42% of the variance in the response variable can be explained by the predictor variables.
The F-statistic is high, and the p-value of the model is close to zero, so we would dismiss the null hypothesis, which assumes that there is no correlation between the explanatory and response variables.

#### Check Model Assumptions

The initial review of the diagnostic plots for this model shows some deviation from linear modeling assumptions at the boundaries of the prediction range, but the overall QQ plot is almost a flat line; the residuals are flat and exhibit homoscedasticity of variance in the 65 - 95 fitted value range.

```{r check_lm3}
check_model(lm3step, check=c('ncv','qq','homogeneity','outliers'))
```

The Breusch--Pagan Test for Heteroscedasticity assumes the following Null and Alternate hypothesis.

-   H0 - Residuals are distributed with equal variance (i.e., homoscedasticity)
-   H1 - Residuals are distributed with unequal variance (i.e., heteroscedasticity)

For this model iteration, we reject the null hypothesis and conclude that this model violates the homoscedasticity function.

```{r bptest_lm3}
lmtest::bptest(lm3step)
```

The Variance Inflation Factor (VIF) calculation detects collinearity between the `TEAM_PITCHING_BB` and `TEAM_BATTING_BB` variables.
Both variables have a VIF score over 5 and are 0.93 correlated.
Additionally, variables `TEAM_PITCHING_SO`, `TEAM_BATTING_SO`, `TEAM_PITCHING_H` and `TEAM_BATTING_2B` have VIF over 5 as well.

```{r vif_lm3}

vif_values <- vif(lm3step)
vif_values <- rownames_to_column(as.data.frame(vif_values), var = "var")

vif_values %>%
  ggplot(aes(y=vif_values, x=var)) +
  coord_flip() + 
  geom_hline(yintercept=5, linetype="dashed", color = "red") +
  geom_bar(stat = 'identity', width=0.3 ,position=position_dodge()) 

df <- droppedDf[ , vif_values$var] %>% drop_na()
coeff <- cor(df)
corrplot(coeff, tl.cex = .7 , diag = FALSE ,type = 'upper' ,method = 'number')

```

#### Re-run Model with new Variable Selections

For the refined model we will drop `TEAM_PITCHING_BB`, `TEAM_BATTING_BB`, `TEAM_PITCHING_SO` `TEAM_BATTING_SO` `TEAM_PITCHING_H` and `TEAM_BATTING_2B`.

```{r lm3_final}
lm3step.final <- update(lm3step, . ~ . -TEAM_PITCHING_BB -TEAM_BATTING_BB -TEAM_PITCHING_SO - -TEAM_BATTING_SO -TEAM_PITCHING_H -TEAM_BATTING_2B)
lm3finalsum <- summary(lm3step.final)
summ(lm3step.final, digits = getOption("jtools-digits", 4))
```

\

#### Model Results

This model has a lower adjusted R2 of 0.37, but with a high F-statistic of 165.8 and statistically significant p-values, it should align better with the linear regression assumptions.

#### Check Model Assumptions

The diagnostic plots for the new model appear to be more closely with the linear regression assumptions.
We again reject the null hypothesis and conclude that this model violates the homoscedasticity function.

```{r check_lm3_final}
check_model(lm3step.final, check=c('ncv','qq','homogeneity','outliers'))
```

The Variance Inflation Factor (VIF) calculation does not detect collinearity across the remaining variables.
Overall, however, we will reject Model 3 because the residuals are not homoscedastic.

```{r vif_lm3_final}

vif_values <- vif(lm3step.final)
vif_values <- rownames_to_column(as.data.frame(vif_values), var = "var")

vif_values %>%
  ggplot(aes(y=vif_values, x=var)) +
  coord_flip() + 
  geom_hline(yintercept=5, linetype="dashed", color = "red") +
  geom_bar(stat = 'identity', width=0.3 ,position=position_dodge()) 

df <- droppedDf[ , vif_values$var] %>% drop_na()
coeff <- cor(df)
corrplot(coeff, tl.cex = .7 , diag = FALSE ,type = 'upper' ,method = 'number')

```

```{r bptest_lm3_final}
lmtest::bptest(lm3step.final)
```

```{r tab_lm3_final}
tab_model(lm3, lm3step, lm3step.final, 
          dv.labels = c('model 3','model 3 (StepAIC)','model 3 (Final)'),
          show.df = FALSE, show.aic = TRUE, show.fstat=TRUE, show.se = TRUE, show.ci=FALSE, show.stat=TRUE, digits.p=4)
```

------------------------------------------------------------------------

### 3.4 Model 4 - Normal Transformation and Stepwise Selection

#### Dataset - transformedDF

The fourth model uses the dataset with missing values imputed, outliers removed, and variables square-root transformed for more normal distributions.
Stepwise AIC selection is applied to select the optimal combination of variables.
The resulting model has significant p-values for the model and all predictor variables.

```{r lm4}
lm4 <- lm(TARGET_WINS~. -TEAM_PITCHING_HR -TEAM_BATTING_SO -TEAM_BASERUN_CS -TEAM_BATTING_HR -TEAM_PITCHING_BB -TEAM_PITCHING_H -pitching_hr_sqrt, data=transformed_df)
lm4step <- stepAIC(lm4, trace=FALSE)
lm4sum <- summary(lm4step)
summ(lm4step, digits = getOption("jtools-digits", 4))
```

\

#### Model Results

The F-statistic is 128, the adjusted R-squared 0.358, and the p-value of the model is close to zero, so we would dismiss the null hypothesis, which assumes that there is no correlation between the explanatory and response variables.

#### Check Model Assumptions

The review of the diagnostic plots shows that the overall QQ plot is almost a flat line; the residuals are flat and exhibit homoscedasticity of variance in the 60 - 100 fitted value range.

```{r check_lm4}
check_model(lm4step, check=c('ncv','qq','homogeneity','outliers'))
```

------------------------------------------------------------------------

## 4. Selecting Models

The table presents an analysis of the performance metrics for each model on the training dataset.
The results suggest that the model's performance slightly improved after incorporating transformations and selecting significant parameters.

The $R^2$, residual standard error, and F-statistics for each model are below.
The non-cleaned, non-imputed raw training data had the best-fitting statistics.
Though Model 1 doesn't meet assumption requirements.
Model 3 looks like the best fit.

The F-statistic for models 2-3 was large, with significant p-values.
During the exploratory data analysis, some of the predictor variables had correlations with each other, as expected in baseball.
However, this high correlation between variables could result in multicollinearity, causing unstable regression fits.
To mitigate this issue, we employed the Variable Inflation Factor (VIF) method in the previous section to identify better models by selecting variables with VIF values less than 5.

The residual plots from part 3 showed that models 1-3 almost follow the normal distribution and almost constant variance.

```{r compare_models}
plot(compare_performance(lm1,lm2,lm3step,lm4step, rank=T))
```

```{r compare_r2}
r <- c(lm1Sum$r.squared, lm2Sum$r.squared, lm3sum$r.squared, lm4sum$r.squared)
mse <- c(lm1Sum$sigma, lm2Sum$sigma, lm3sum$sigma, lm4sum$sigma)
adjusted.r <- c(lm1Sum$adj.r.squared, lm2Sum$adj.r.squared, lm3sum$adj.r.squared, lm4sum$adj.r.squared)
modelDf <- data.frame(r,mse,adjusted.r)
kable(modelDf, caption = "Moneyball Dataset", digits = 2, format = "html", row.names = F) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover"),
    full_width = F,
    position = "center"
  )
```

Next, we apply the models to the evaluation dataset to make predictions.
However, to ensure that Models 2-4 work properly, we fill in the missing values in the evaluation data set using the same MICE imputation method, drop the INDEX and TEAM_BATTING_HBP variables, remove outliers for Model 3, and make transformations for Model 4.

```{r predict_setup}
#removed columns
evalDroppedDf <- evalDf %>% dplyr::select(-c(INDEX, TEAM_BATTING_HBP))
evalDroppedDf <- evalDroppedDf %>% mutate(TEAM_BATTING_1B = TEAM_BATTING_H - TEAM_BATTING_2B - TEAM_BATTING_3B - TEAM_BATTING_HR)

#no NA's
evalcleanDf <- mice(evalDroppedDf, m = 5, maxit = 50, seed = 123, printFlag = F)
evalcleanDf <- complete(evalcleanDf)

#with transformations
transformed_eval <- evalcleanDf |> mutate_each(funs(sqrt),
                   batting_hr_sqrt = TEAM_BATTING_HR,
                   batting_so_sqrt = TEAM_BATTING_SO,
                   baserun_cs_sqrt = TEAM_BASERUN_CS,
                   pitching_hr_sqrt = TEAM_PITCHING_HR)

```

```{r predict}
lm1Pred <- lm1 %>% predict(evalcleanDf)
lm2Pred <- lm2 %>% predict(evalcleanDf)
aiclm3 <- lm3step %>% predict(evalcleanDf)
aiclm4 <- lm4step %>% predict(transformed_eval)
```

This table presents the predicted values of TARGET_WINS for each model.
The first thing to notice is that the first model predicts a negative number of wins, which is unrealistic.
The third and fourth AIC-generated models (no outliers and transformed values) have similar outcomes, with both performing well.
The second model also produced similar results but had an issue with model assumptions.

```{r predict_report}
predsDf <- evalDf %>% 
  mutate(lm1 = lm1Pred, lm2 = lm2Pred, aic3 = aiclm3, aic4 = aiclm4) %>%
  dplyr::select(c(lm1, lm2, aic3, aic4))
  kable(head(predsDf, 10), caption = "Moneyball Dataset", digits = 2, format = "html", row.names = F) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover"),
    full_width = F,
    position = "center"
  )
```

We can also see when plotting the predictions that there doesn't seem to be much difference between the models, aside from the unrealistic outliers generated by Model 1, and the slightly tighter range in wins in Model 4.

```{r predict_plot}
xT <- evalcleanDf %>% 
  mutate(lm1 = lm1Pred, lm2 = lm2Pred, aic3 = aiclm3, aic4 = aiclm4)
par(mfrow=c(2,2))
plot(1:nrow(xT), xT$lm1, xlab="LM1", ylab="Wins", title="LM1")
plot(1:nrow(xT), xT$lm2, xlab="LM2", ylab="Wins", title="LM2")
plot(1:nrow(xT), xT$aic, xlab="AIC LM3", ylab="Wins", title="AIC LM3")
plot(1:nrow(xT), xT$aic4, xlab="AIC LM4", ylab="Wins", title="AIC LM4")
```

------------------------------------------------------------------------

## 5. Conclusion

We don't generally place much faith in any of the models for their predictive power:

-   Model 1 produced the highest $R^2$, but was affected by missing data leading to negative win predictions.

-   Model 3 produced lower $R^2$ than Model 1 but didn't produce negative results.

-   Models 2 and 4 demonstrated a weak overall fit, as evidenced by their lower $R^2$ scores.

-   None of the models displayed satisfactory performance when assessed for linearity or homogeneity of variance. 

According to the analysis, Models 1 and 3 performed slightly better than Models 2 and 4, and would be the preferred linear models based solely on their adjusted $R^2$.
However, Model 3 is more statistically significant than the others and uses fewer unnecessary variables for making predictions while maintaining a similar level of adjusted $R^2$.

Overall, Model 3 would be the recommended choice based on these factors and its better handling of multicollinearity (with fewer instances of sign-flipping in coefficients.)

------------------------------------------------------------------------

## 6. References
1. Faraway, J. J. (2014). _Linear Models with R, Second Edition._ CRC Press.
2. Sheather, S. (2009). _A Modern Approach to Regression with R._ Springer Science & Business Media.
3. _Detecting Multicollinearity Using Variance Inflation Factors_ | STAT 462. (n.d.). https://online.stat.psu.edu/stat462/node/180/
4. James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2014. _An Introduction to Statistical Learning: With Applications in R._ Springer Publishing Company, Incorporated

## Appendix A: Lahman's Baseball Database

Despite significant efforts to compensate for poor data quality, the resulting models are poor predictors of win totals.

Moreover, the poor data quality is inconsistent with the overall state of baseball statistics.
When it comes to the major sports, baseball has the most mature statistics available.
Therefore finding better data is the best course of action for developing a better predictive model.

We were able to locate a cleaner version of the same data set provided in the class.
The Lahman's Baseball Database includes the same variables as the sample database with fewer errors and additional reference data that would allow us to connect the database to other sources.

<https://www.seanlahman.com/baseball-archive/statistics/>

```{r lahman_df}
teamDF <- loadLahmanData()
```

A significant advantage of Lahman's data set over the data set provided in class is that it includes information about the year and the team.
This data is valuable when considering how baseball has changed over the years.
The modern era in baseball is often delineated by the turn of the century.
However, when looking at the past 120 years of baseball history, it is easy to pinpoint rule changes, evolutions in playing strategy, and league structure that have fundamentally impacted the game.

When comparing statistics across time, it is common to use many of the breakdowns below to add context to the analysis:

-   The Dead Ball Era (1901 - 1920)
-   World War 2 (1941 - 1945)
-   Segregation Era (1901 - 1947ish)
-   Post-War Era/Yankees Era (1945 - late 50s/early 60s)
-   Westward Expansion (1953 - 1961)
-   Dead Ball 2 (The Sixties, roughly)
-   Designated Hitter Era (1973 - current, AL only)
-   Free Agency/Arbitration Era (1975 - current)
-   Steroid Era (unknown, but late 80s - 2005 seems likely)
-   Wild Card Era (1994 - current)

Surveying these periods would suggest that a more granular model has the potential to perform better.

Although we could have chosen any number of the time periods above, exploring the statistical outliers highlights that many of these values correspond to the pre-1969 period.
This delineation has some historical support.

As Jayson Stark of ESPN argues in this article (<https://www.espn.com/mlb/columns/story?columnist=stark_jayson&id=2471349>) In 1969 the MLB underwent several rule changes and changes to the league structure that impacted win totals and team statistics.
1969 was the first year of division play and the expanded postseason.
The Pitcher's Mound was lowered five inches.
The Strike zone shrinks.
Five-person rotations kicking in.
The save was invented.
And more expansion to the unbalanced schedules.

Thus using 1900 as the beginning of the modern era and 1969 as an additional breakpoint, the dataset can be divided into three segments.
The density profiles for the predictor variables approach a normal distribution when grouped by the three segments we identified.
To support data exploration, we added a era_cat field to the data set.

```{r lahmanshort_df}
teamAdjDF <- loadLahmanShortData()

random_sample <- createDataPartition(teamAdjDF$TARGET_WINS,p = 0.8, list = FALSE)

trainingTeam_df <- teamAdjDF[random_sample, ]
testingTeam_df <- teamAdjDF[-random_sample, ]
```

In general the Lahman's data set contains fewer data gaps and the variables are more consistently distributed.
There are some missing values in the data set including the Caught Stealing (CS/TEAM_BASERUN_CS) variable is missing 27.9%; the Batters Hit by Pitch (HBP/TEAM_BATTING_HBP) variable is missing 38.8%; and the Sacrifice Flies (SF) variable is missing 51.6% of the values.

Most of the variables in the data set show some level of skewness, with the following variables having a Kurtosis measure of greater than 3, TEAM_BATTING_H, TEAM_BASERUN_SB, TEAM_BASERUN_CS, TEAM_PITCHING_H, and TEAM_FIELDING_E

The Lahman data set contains several variables with bimodal distributions, including, TEAM_BATTING_HR, TEAM_BATTING_SO, TEAM_BATTING_HR, and TEAM_PITCHING_SO.

```{r lahman_summary}
print(
  dfSummary(teamDF, 
            varnumbers   = TRUE,
            na.col       = TRUE,
            graph.magnif = .8,
            tmp.img.dir  = "/tmp"),
  method = "render")
```

```{r lahman_plot}
m_df <- teamAdjDF[random_sample, ] %>% melt() 

m_df %>% ggplot(aes(x= value)) + 
      geom_density(color='#023020', fill='gray') + 
      facet_wrap(~variable, scales = 'free',  ncol = 4) + 
      theme_bw() +
      labs(title = 'Variable Density Plots')
```

When we group the statistics by the three categories presented earliers, we see a much cleaner density plot across all variables.
There are few signs of bimodal distributions, and the skewness of individual variables is reduced greatly.

```{r lahman_density}
m_df <- teamAdjDF[random_sample, ] %>% filter(era_cat == '1969+') %>% melt() 

m_df %>% ggplot(aes(x= value)) + 
      geom_density(color='#023020', fill='gray') + 
      facet_wrap(~variable, scales = 'free',  ncol = 4) + 
      theme_bw() +
      labs(title = 'Variable Density Plots (1969+)')
```

The training data set exhibits the following characteristics.

```{r lahman_training_summary}
print(
  dfSummary(trainingTeam_df, 
            varnumbers   = TRUE,
            na.col       = TRUE,
            graph.magnif = .8,
            tmp.img.dir  = "/tmp"),
  method = "render")
```

```{r lahman_training_plot}
m_df <- trainingTeam_df %>% melt() 

m_df %>% ggplot(aes(x= value)) + 
    geom_density(color='#023020', fill='gray') + facet_wrap(~variable, scales = 'free',  ncol = 4) + theme_bw()


m_df %>% ggplot(aes(x = value)) +
  geom_boxplot(outlier.color = 'red', outlier.shape = 1) +
  facet_wrap(vars(variable),scales = "free", ncol = 4)
```

```{r lahman_training_corr}
rcore <- rcorr(as.matrix(trainingTeam_df %>% dplyr::select(where(is.numeric))))
coeff <- rcore$r
corrplot(coeff, tl.cex = .7 , method = 'pie')
```

**DATA PREPARATION**

The use of the era_cat variable allows us to group the data set into 3 categories that variables that approach normal distribution.

Since segmenting the data set using the era_cat variable creates 3 categories of variables that approach the normal distribution, we will focus our data preparation step on missing values.
The documentation for MICE package recommends that a 5% threshold should be observed for safely imputing missing values.
Based on this rule of thumb we will drop the 'TEAM_BASERUN_CS' and the 'TEAM_BATTING_HBP' variables.

The remaining variables with missing data (`TEAM_BATTING_BB`, `TEAM_BATTING_SO` and `TEAM_BASERUN_SB`) will be imputed using the MICE package.
Several methods can be used but for simplicity we selected m=5 the default method.

```{r lahman_prep_data}
trainingTeam_df <- trainingTeam_df %>% dplyr::select(-c(TEAM_BASERUN_CS,TEAM_BATTING_HBP))
testingTeam_df <- testingTeam_df %>% dplyr::select(-c(TEAM_BASERUN_CS,TEAM_BATTING_HBP))
```

```{r lahman_prep_impute}
imputeDf <- mice(trainingTeam_df, m = 5, maxit = 50, seed = 123, printFlag = F)
#imputeDf$meth
trainingTeam_df <- complete(imputeDf)
m_imputed <- melt(trainingTeam_df)
densityplot(imputeDf)
```

**Build Model**

The first model includes all the variables from the original data set with the exception of the yearID.
It could be argued that year might have an impact on the numbers, however we are accounting for year with the era_cat variable.
The initial model has an $AdjR^2 = 0.8078$, and the model selected by stepAIC includes the same variables and has the same $AdjR^2=8078$.

```{r lahman_lma1}
lmA1 <- lm(TARGET_WINS ~ . -yearID, data = trainingTeam_df, by=era_cat)
tab_model(lmA1, show.df = FALSE, show.aic = TRUE, show.fstat=TRUE, show.se = TRUE, show.ci=FALSE, show.stat=TRUE, digits.p=4)
```

\

```{r lahman_lma1_step}
lmA1.step <- stepAIC(lmA1, trace = FALSE, by=era_cat)
tab_model(lmA1.step, show.df = FALSE, show.aic = TRUE, show.fstat=TRUE, show.se = TRUE, show.ci=FALSE, show.stat=TRUE, digits.p=4)
```

\

The `TEAM_BATTING_3B` variable was dropped due to low p-values, giving us the final model with an $AdjR^2=0.8078$.

```{r lahman_summary_lm1}
lmA1.final <- update(lmA1.step, . ~ . -TEAM_BATTING_3B)
tab_model(lmA1.final, show.df = FALSE, show.aic = TRUE, show.fstat=TRUE, show.se = TRUE, show.ci=FALSE, show.stat=TRUE, digits.p=4)
```

\

There are a number of issues with the model diagnostics.
The Linearity, and QQ plots show deviation from the straight line at the boundaries.
In addition, the reference line for the Homogeneity of Variance graph is not flat.
This indicates that residual variance is not consistent across the model values.
Further exploration could be conducted to see if there are smaller date ranges and prediction ranges that generate a model that better aligns with the linear regression assumptions.

```{r lahman_check_lm1}
check_model(lmA1.final, check=c('ncv','qq','homogeneity','outliers'))
```

**PREDICT**

The next step is to use the selected model to predict the testing data set.
The Yardstick package was used to calculate model performance, including the $R^2$.
With an $R^2=0.8318$ the performance on the testing data set is consistent with the model summary.
In the Residual vs. TARGET_WINS graph, there appears to be wins range between 60 and 110 that generates more accurate forecasts.
However, it should be noted that the residuals appear to drift downwards.
TARGET_WINS less than the mean are overestimated and TARGET_WINS greater than the mean is over estimated.

```{r lahman_predict}
imputeDf <- mice(testingTeam_df, m = 5, maxit = 50, seed = 123, printFlag = F)
#imputeDf$meth
testingTeam_df <- complete(imputeDf)
m_imputed <- melt(testingTeam_df)
densityplot(imputeDf)
```

```{r lahman_predict_final}
lmA1Pred.final <- lmA1.final %>% predict(testingTeam_df)
```

```{r lahman_predict_report}
multi_metric <- metric_set(mape, smape, mase, mpe, rmse, rsq)
m <- testingTeam_df %>% multi_metric(truth=testingTeam_df$TARGET_WINS, estimate=lmA1Pred.final)

kable(m, digits = 4, format = "html", row.names = T) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover"),
    full_width = F,
    position = "center"
  )
```

```{r lahman_predict_plot}
n <- nrow(testingTeam_df)
x <- testingTeam_df$TARGET_WINS
e <- lmA1Pred.final - testingTeam_df$TARGET_WINS 


plot(x, e,  
     xlab = "wins", 
     ylab = "residuals",
     bg = "steelblue", 
     col = "darkgray", cex = 1.5, pch = 21, frame = FALSE)
abline(h = 0, lwd = 2)
for (i in 1 : n) 
  lines(c(x[i], x[i]), c(e[i], 0), col = "red" , lwd = 1)
```

------------------------------------------------------------------------

## Appendix B: Pythagorean Model

Bill James developed the Pythagorean winning percentage.
The concept strives to calculate the number of games a team should win based on the total offense and the number of runs allowed.
Since this model includes total runs scored vs. total runs allowed the expectation is that this model will be a good predictor of a team's wins in a given season.

$Win Percentage = (Runs Scored)^2 / [ (Runs Scored)^2 + (Runs Allowed)^2]$

```{r py_df}
random_sample <- createDataPartition(teamDF$TARGET_WINS, p = 0.8, list = FALSE)

trainingTeam_df <- teamDF[random_sample, ]
testingTeam_df <- teamDF[-random_sample, ]
```

```{r py_lm}
lmp <- lm(TARGET_WINS ~ pythPercent, data = trainingTeam_df)

tab_model(lmp, show.df = FALSE, show.aic = TRUE, show.fstat=TRUE, show.se = TRUE, show.ci=FALSE, show.stat=TRUE, digits.p=4)
```

\

#### Model Results

As we expected, the Pythagorean model performs well $Adj R^2$ = 0.9112.
However, this linear model differs slightly from the formal definition since it includes an intercept of 5.21 and a coefficient for the Pythagorean factor of 151.39.
The formal definition of the model would have an intercept of 0 and a coefficient of 162.

#### Check Model Assumptions

The diagnostic plots show residuals that are normally distributed with a linear relationship to the fitted values.
The homogeneity of variance is curved, indicating that the variance of residuals is not consistent.

```{r py_check_lm}
check_model(lmp, check=c('ncv','qq','homogeneity','outliers'))
```

The next step is to use the Pythagorean Model to predict the testing data set.
The Yardstick package was used to calculate model performance, including the $R^2$.
With an $R^2=0.9131$ the performance on the testing data set is consistent with the model summary.
In the Residual vs. TARGET_WINS graph there appears to be a wins range between 50 and 130 that generates more accurate forecasts.

```{r py_predict}
lmpPred <- lmp %>% predict(testingTeam_df)
hist(lmpPred)
```

```{r py_predict_report}
m1 <- testingTeam_df %>% multi_metric(truth=testingTeam_df$TARGET_WINS, estimate=lmpPred)

kable(m1, digits = 4, format = "html", row.names = T) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover"),
    full_width = F,
    position = "center"
  )
```

```{r py_predict_plot}
n <- nrow(testingTeam_df)
x <- testingTeam_df$TARGET_WINS
e <- lmpPred - testingTeam_df$TARGET_WINS 

plot(x, e,  
     xlab = "wins", 
     ylab = "residuals", 
     bg = "steelblue", 
     col = "darkgray", cex = 1.5, pch = 21, frame = FALSE)
abline(h = 0, lwd = 2)
for (i in 1 : n) 
  lines(c(x[i], x[i]), c(e[i], 0), col = "red" , lwd = 1)
```

Unsurprisingly, the Pythagorean Model performs well when it comes to win projections.
There is clear relationship between the total yearly run differential and the number of games won.
It is a simple model but efficient.

## Appendix C: R code

```{r eval=FALSE, include=TRUE, echo=TRUE}
## ----setup, include=FALSE-------------------------------------------------------------------------------------------------------
library(tidyverse)
library(reshape2)
library(kableExtra)
library(Matrix)
library(MASS)
library(mice)
library(Hmisc)
library(corrplot)
library(performance)
library(naniar)
library(psych)
library(GGally)
library(campfin)
library(caret)
library(yardstick)
library(summarytools)
library(sjPlot)
library(car)
library(rsample)
library(olsrr) 

knitr::opts_chunk$set(echo = F, 
                      warning = F, 
                      message = F, 
                      eval = T , 
                      results="asis", 
                      fig.height=6, 
                      fig.width=8)
set.seed(1234)


## ----functions------------------------------------------------------------------------------------------------------------------

st_css()

 st_options(
   plain.ascii = FALSE,
   style = 'grid',
   dfSummary.style ='grid',
   freq.silent  = TRUE,
   headings     = FALSE,
   tmp.img.dir  = "./tmp",
   dfSummary.custom.1 =
     expression(
       paste(
         "Q1 - Q3 :",
         round(
           quantile(column_data, probs = .25, type = 2,
                    names = FALSE, na.rm = TRUE), digits = 1
         ), " - ",
         round(
           quantile(column_data, probs = .75, type = 2,
                    names = FALSE, na.rm = TRUE), digits = 1
         )
       )
     )
 )

source('./hw1/Functions.R', local = knitr::knit_global())
 


## ----load_data------------------------------------------------------------------------------------------------------------------
trainDf <- read.csv("https://raw.githubusercontent.com/cliftonleesps/data621_group1/main/hw1/moneyball-training-data.csv")
evalDf <- read.csv("https://raw.githubusercontent.com/cliftonleesps/data621_group1/main/hw1/moneyball-evaluation-data.csv")


## ----df_summary-----------------------------------------------------------------------------------------------------------------
print(
  dfSummary(trainDf, 
            varnumbers   = TRUE,
            na.col       = TRUE,
            graph.magnif = .8,
            tmp.img.dir  = "/tmp"),
  method = "render"
)



## ----density_plot---------------------------------------------------------------------------------------------------------------
m_df <- trainDf %>% dplyr::select(-c(INDEX)) %>% melt() 

m_df %>% ggplot(aes(x= value)) + 
geom_density(color='#023020', fill='gray') + facet_wrap(~variable, scales = 'free',  ncol = 4) + theme_bw()


## ----boxplot--------------------------------------------------------------------------------------------------------------------

m_df %>% ggplot(aes(x = value)) +
  geom_boxplot(outlier.color = 'red', outlier.shape = 1) +
  facet_wrap(vars(variable),scales = "free", ncol = 4)



## ----feature_plot---------------------------------------------------------------------------------------------------------------
featurePlot(trainDf[,2:ncol(trainDf)], trainDf[,1], plot = "scatter", type = c("p", "smooth"), span = 1)


## ----correlation_matrix---------------------------------------------------------------------------------------------------------
rcore <- rcorr(as.matrix(trainDf %>% dplyr::select(where(is.numeric) & -INDEX & -TEAM_BATTING_HBP)))
coeff <- rcore$r
corrplot(coeff, tl.cex = .7 , method = 'circle')


## ----corr_numbers---------------------------------------------------------------------------------------------------------------
tst <- trainDf
tst <- tst[,-1 ]
kable(cor(drop_na(tst))[,1], "html", escape = F, col.names = c('Coefficient')) %>%
  kable_styling("striped", full_width = F) %>%
  column_spec(1, bold = T)


## ----missing_data,fig.height=4--------------------------------------------------------------------------------------------------
gg_miss_var(trainDf, show_pct=TRUE)


## ----dropped_df-----------------------------------------------------------------------------------------------------------------
droppedDf <- trainDf %>% dplyr::select(-c(INDEX, TEAM_BATTING_HBP))


## ----missing_flags--------------------------------------------------------------------------------------------------------------
columnNames <- names(droppedDf)
droppedDf <- flag_na(droppedDf, columnNames)
columnNames <- names(evalDf)
evalDf <- flag_na(evalDf, columnNames)


## ----density_plots_post,fig.height=4--------------------------------------------------------------------------------------------
imputeDf <- mice(droppedDf, m = 5, maxit = 50, seed = 123, printFlag = F)
cleanDf <- complete(imputeDf)
m_imputed <- melt(cleanDf)
densityplot(imputeDf)


## ----drop_na_flag---------------------------------------------------------------------------------------------------------------
cleanDf <- cleanDf %>% dplyr::select(-na_flag)
droppedDf <- droppedDf %>% dplyr::select(-na_flag)


## ----drop_outliers--------------------------------------------------------------------------------------------------------------

# list of columns included in in the outlier filtering
columnNames <- c('TEAM_BATTING_H','TEAM_BATTING_2B','TEAM_BATTING_3B','TEAM_BATTING_HR',
                 'TEAM_BATTING_BB','TEAM_BATTING_SO','TEAM_BASERUN_SB','TEAM_BASERUN_CS',
                 'TEAM_PITCHING_H','TEAM_PITCHING_HR','TEAM_PITCHING_BB','TEAM_PITCHING_SO',
                 'TEAM_FIELDING_E','TEAM_FIELDING_DP','TEAM_BATTING_1B')

# Filter 
filterDf <- loadLahmanShortData() %>% filter(yearID >= 1900)
droppedDf <- filterOutliers(filterDf, cleanDf, columnNames)

# print stats for new model
print(
  dfSummary(droppedDf, 
            varnumbers   = TRUE,
            na.col       = TRUE,
            graph.magnif = .8,
            tmp.img.dir  = "/tmp"),
  method = "render"
)

m_df <- droppedDf %>% melt() 

m_df %>% ggplot(aes(x= value)) + 
    geom_density(color='#023020', fill='gray') + facet_wrap(~variable, scales = 'free',  ncol = 4) + theme_bw()

m_df %>% ggplot(aes(x = value)) +
  geom_boxplot(outlier.color = 'red', outlier.shape = 1) +
  facet_wrap(vars(variable),scales = "free", ncol = 4)



## ----single_base_hits-----------------------------------------------------------------------------------------------------------
droppedDf <- droppedDf %>% mutate(TEAM_BATTING_1B = TEAM_BATTING_H - TEAM_BATTING_2B - TEAM_BATTING_3B - TEAM_BATTING_HR)


## ----transform_nonnormal--------------------------------------------------------------------------------------------------------
# Reminder of our distributions
m_imputed %>% ggplot(aes(x= value)) + 
    geom_density(fill='gray') + facet_wrap(~variable, scales = 'free', ncol=4) +
    theme(strip.text.x = element_text(size = 6, angle = 0)) +
    labs(title='Training w/Imputed Missing Values')

# Transform some non-normal variables.
transformed_df <- cleanDf |> mutate_each(funs(sqrt),
                   batting_hr_sqrt = TEAM_BATTING_HR,
                   batting_so_sqrt = TEAM_BATTING_SO,
                   baserun_cs_sqrt = TEAM_BASERUN_CS,
                   pitching_hr_sqrt = TEAM_PITCHING_HR) 

visualize_transformed <- transformed_df |> melt()

# Plot.
visualize_transformed %>% ggplot(aes(x= value)) + 
    geom_density(fill='gray') + facet_wrap(~variable, scales = 'free', ncol=4) +
    theme(strip.text.x = element_text(size = 6, angle = 0)) +
    labs(title='Training w/Imputed Missing Values + SQRT Transform')


## ----df_summary_normalized------------------------------------------------------------------------------------------------------
print(
  dfSummary(transformed_df, 
            varnumbers   = TRUE,
            na.col       = TRUE,
            graph.magnif = .8,
            tmp.img.dir  = "/tmp"),
  method = "render")



## ----lm1------------------------------------------------------------------------------------------------------------------------
trainDf_1 <- trainDf %>% dplyr::select(-c(INDEX, TEAM_BATTING_HBP))
lm1 <- lm(TARGET_WINS ~ . , data = trainDf_1)
lm1Sum <- summary(lm1)
tab_model(lm1Sum, show.df = FALSE, show.aic = TRUE, show.fstat=TRUE, show.se = TRUE, show.ci=FALSE, show.stat=TRUE, digits.p=4)


## ----check_lm1------------------------------------------------------------------------------------------------------------------
check_model(lm1, check=c('ncv','qq','homogeneity','outliers'))


## ----lm2------------------------------------------------------------------------------------------------------------------------
lm2 <- lm(formula = TARGET_WINS ~ TEAM_PITCHING_BB  + TEAM_BATTING_2B + TEAM_BATTING_3B +
   TEAM_FIELDING_E + TEAM_PITCHING_H + TEAM_BATTING_HR + TEAM_BATTING_H, data = cleanDf)
lm2Sum <- summary(lm2)
tab_model(lm2Sum, show.df = FALSE, show.aic = TRUE, show.fstat=TRUE, show.se = TRUE, show.ci=FALSE, show.stat=TRUE, digits.p=4)


## ----check_lm2------------------------------------------------------------------------------------------------------------------
check_model(lm2, check=c('ncv','qq','homogeneity','outliers'))


## ----lm3------------------------------------------------------------------------------------------------------------------------

lm3 <- lm(TARGET_WINS ~ . -TEAM_BATTING_H, data=droppedDf, by=na_flag)
lm3step <- stepAIC(lm3, trace = FALSE)
lm3sum <- summary(lm3step)
tab_model(lm3step, show.df = FALSE, show.aic = TRUE, show.fstat=TRUE, show.se = TRUE, show.ci=FALSE, show.stat=TRUE, digits.p=4)



## ----check_lm3------------------------------------------------------------------------------------------------------------------
check_model(lm3step, check=c('ncv','qq','homogeneity','outliers'))


## ----bptest_lm3-----------------------------------------------------------------------------------------------------------------
lmtest::bptest(lm3step)


## ----vif_lm3--------------------------------------------------------------------------------------------------------------------

vif_values <- vif(lm3step)
vif_values <- rownames_to_column(as.data.frame(vif_values), var = "var")

vif_values %>%
  ggplot(aes(y=vif_values, x=var)) +
  coord_flip() + 
  geom_hline(yintercept=5, linetype="dashed", color = "red") +
  geom_bar(stat = 'identity', width=0.3 ,position=position_dodge()) 

df <- droppedDf[ , vif_values$var] %>% drop_na()
coeff <- cor(df)
corrplot(coeff, tl.cex = .7 , diag = FALSE ,type = 'upper' ,method = 'number')



## ----lm3_final------------------------------------------------------------------------------------------------------------------
lm3step.final <- update(lm3step, . ~ . -TEAM_PITCHING_BB -TEAM_BATTING_BB -TEAM_PITCHING_SO - -TEAM_BATTING_SO -TEAM_PITCHING_H -TEAM_BATTING_2B)
lm3finalsum <- summary(lm3step.final)
tab_model(lm3step.final, show.df = FALSE, show.aic = TRUE, show.fstat=TRUE, show.se = TRUE, show.ci=FALSE, show.stat=TRUE, digits.p=4)


## ----check_lm3_final------------------------------------------------------------------------------------------------------------
check_model(lm3step.final, check=c('ncv','qq','homogeneity','outliers'))


## ----vif_lm3_final--------------------------------------------------------------------------------------------------------------

vif_values <- vif(lm3step.final)
vif_values <- rownames_to_column(as.data.frame(vif_values), var = "var")

vif_values %>%
  ggplot(aes(y=vif_values, x=var)) +
  coord_flip() + 
  geom_hline(yintercept=5, linetype="dashed", color = "red") +
  geom_bar(stat = 'identity', width=0.3 ,position=position_dodge()) 

df <- droppedDf[ , vif_values$var] %>% drop_na()
coeff <- cor(df)
corrplot(coeff, tl.cex = .7 , diag = FALSE ,type = 'upper' ,method = 'number')



## ----bptest_lm3_final-----------------------------------------------------------------------------------------------------------
lmtest::bptest(lm3step.final)


## ----tab_lm3_final--------------------------------------------------------------------------------------------------------------
tab_model(lm3, lm3step, lm3step.final, 
          dv.labels = c('model 3','model 3 (StepAIC)','model 3 (Final)'),
          show.df = FALSE, show.aic = TRUE, show.fstat=TRUE, show.se = TRUE, show.ci=FALSE, show.stat=TRUE, digits.p=4)


## ----lm4------------------------------------------------------------------------------------------------------------------------
lm4 <- lm(TARGET_WINS~. -TEAM_PITCHING_HR -TEAM_BATTING_SO -TEAM_BASERUN_CS -TEAM_BATTING_HR -TEAM_PITCHING_BB -TEAM_PITCHING_H -pitching_hr_sqrt, data=transformed_df)
lm4step <- stepAIC(lm4, trace=FALSE)
lm4sum <- summary(lm4step)
tab_model(lm4step, show.df = FALSE, show.aic = TRUE, show.fstat=TRUE, show.se = TRUE, show.ci=FALSE, show.stat=TRUE, digits.p=4)


## ----check_lm4------------------------------------------------------------------------------------------------------------------
check_model(lm4step, check=c('ncv','qq','homogeneity','outliers'))


## ----compare_models-------------------------------------------------------------------------------------------------------------
plot(compare_performance(lm1,lm2,lm3step,lm4step, rank=T))


## ----compare_r2-----------------------------------------------------------------------------------------------------------------
r <- c(lm1Sum$r.squared, lm2Sum$r.squared, lm3sum$r.squared, lm4sum$r.squared)
mse <- c(lm1Sum$sigma, lm2Sum$sigma, lm3sum$sigma, lm4sum$sigma)
adjusted.r <- c(lm1Sum$adj.r.squared, lm2Sum$adj.r.squared, lm3sum$adj.r.squared, lm4sum$adj.r.squared)
modelDf <- data.frame(r,mse,adjusted.r)
kable(modelDf, caption = "Moneyball Dataset", digits = 2, format = "html", row.names = F) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover"),
    full_width = F,
    position = "center"
  )


## ----predict_setup--------------------------------------------------------------------------------------------------------------
#removed columns
evalDroppedDf <- evalDf %>% dplyr::select(-c(INDEX, TEAM_BATTING_HBP))
evalDroppedDf <- evalDroppedDf %>% mutate(TEAM_BATTING_1B = TEAM_BATTING_H - TEAM_BATTING_2B - TEAM_BATTING_3B - TEAM_BATTING_HR)

#no NA's
evalcleanDf <- mice(evalDroppedDf, m = 5, maxit = 50, seed = 123, printFlag = F)
evalcleanDf <- complete(evalcleanDf)

#with transformations
transformed_eval <- evalcleanDf |> mutate_each(funs(sqrt),
                   batting_hr_sqrt = TEAM_BATTING_HR,
                   batting_so_sqrt = TEAM_BATTING_SO,
                   baserun_cs_sqrt = TEAM_BASERUN_CS,
                   pitching_hr_sqrt = TEAM_PITCHING_HR)



## ----predict--------------------------------------------------------------------------------------------------------------------
lm1Pred <- lm1 %>% predict(evalcleanDf)
lm2Pred <- lm2 %>% predict(evalcleanDf)
aiclm3 <- lm3step %>% predict(evalcleanDf)
aiclm4 <- lm4step %>% predict(transformed_eval)


## ----predict_report-------------------------------------------------------------------------------------------------------------
predsDf <- evalDf %>% 
  mutate(lm1 = lm1Pred, lm2 = lm2Pred, aic3 = aiclm3, aic4 = aiclm4) %>%
  dplyr::select(c(lm1, lm2, aic3, aic4))
  kable(head(predsDf, 10), caption = "Moneyball Dataset", digits = 2, format = "html", row.names = F) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover"),
    full_width = F,
    position = "center"
  )


## ----predict_plot---------------------------------------------------------------------------------------------------------------
xT <- evalcleanDf %>% 
  mutate(lm1 = lm1Pred, lm2 = lm2Pred, aic3 = aiclm3, aic4 = aiclm4)
par(mfrow=c(2,2))
plot(1:nrow(xT), xT$lm1, xlab="LM1", ylab="Wins", title="LM1")
plot(1:nrow(xT), xT$lm2, xlab="LM2", ylab="Wins", title="LM2")
plot(1:nrow(xT), xT$aic, xlab="AIC LM3", ylab="Wins", title="AIC LM3")
plot(1:nrow(xT), xT$aic4, xlab="AIC LM4", ylab="Wins", title="AIC LM4")


## ----lahman_df------------------------------------------------------------------------------------------------------------------
teamDF <- loadLahmanData()


## ----lahmanshort_df-------------------------------------------------------------------------------------------------------------
teamAdjDF <- loadLahmanShortData()

random_sample <- createDataPartition(teamAdjDF$TARGET_WINS,p = 0.8, list = FALSE)

trainingTeam_df <- teamAdjDF[random_sample, ]
testingTeam_df <- teamAdjDF[-random_sample, ]


## ----lahman_summary-------------------------------------------------------------------------------------------------------------
print(
  dfSummary(teamDF, 
            varnumbers   = TRUE,
            na.col       = TRUE,
            graph.magnif = .8,
            tmp.img.dir  = "/tmp"),
  method = "render")


## ----lahman_plot----------------------------------------------------------------------------------------------------------------
m_df <- teamAdjDF[random_sample, ] %>% melt() 

m_df %>% ggplot(aes(x= value)) + 
      geom_density(color='#023020', fill='gray') + 
      facet_wrap(~variable, scales = 'free',  ncol = 4) + 
      theme_bw() +
      labs(title = 'Variable Density Plots')


## ----lahman_density-------------------------------------------------------------------------------------------------------------
m_df <- teamAdjDF[random_sample, ] %>% filter(era_cat == '1969+') %>% melt() 

m_df %>% ggplot(aes(x= value)) + 
      geom_density(color='#023020', fill='gray') + 
      facet_wrap(~variable, scales = 'free',  ncol = 4) + 
      theme_bw() +
      labs(title = 'Variable Density Plots (1969+)')


## ----lahman_training_summary----------------------------------------------------------------------------------------------------
print(
  dfSummary(trainingTeam_df, 
            varnumbers   = TRUE,
            na.col       = TRUE,
            graph.magnif = .8,
            tmp.img.dir  = "/tmp"),
  method = "render")


## ----lahman_training_plot-------------------------------------------------------------------------------------------------------
m_df <- trainingTeam_df %>% melt() 

m_df %>% ggplot(aes(x= value)) + 
    geom_density(color='#023020', fill='gray') + facet_wrap(~variable, scales = 'free',  ncol = 4) + theme_bw()


m_df %>% ggplot(aes(x = value)) +
  geom_boxplot(outlier.color = 'red', outlier.shape = 1) +
  facet_wrap(vars(variable),scales = "free", ncol = 4)


## ----lahman_training_corr-------------------------------------------------------------------------------------------------------
rcore <- rcorr(as.matrix(trainingTeam_df %>% dplyr::select(where(is.numeric))))
coeff <- rcore$r
corrplot(coeff, tl.cex = .7 , method = 'pie')


## ----lahman_prep_data-----------------------------------------------------------------------------------------------------------
trainingTeam_df <- trainingTeam_df %>% dplyr::select(-c(TEAM_BASERUN_CS,TEAM_BATTING_HBP))
testingTeam_df <- testingTeam_df %>% dplyr::select(-c(TEAM_BASERUN_CS,TEAM_BATTING_HBP))


## ----lahman_prep_impute---------------------------------------------------------------------------------------------------------
imputeDf <- mice(trainingTeam_df, m = 5, maxit = 50, seed = 123, printFlag = F)
#imputeDf$meth
trainingTeam_df <- complete(imputeDf)
m_imputed <- melt(trainingTeam_df)
densityplot(imputeDf)


## ----lahman_lma1----------------------------------------------------------------------------------------------------------------
lmA1 <- lm(TARGET_WINS ~ . -yearID, data = trainingTeam_df, by=era_cat)
tab_model(lmA1, show.df = FALSE, show.aic = TRUE, show.fstat=TRUE, show.se = TRUE, show.ci=FALSE, show.stat=TRUE, digits.p=4)


## ----lahman_lma1_step-----------------------------------------------------------------------------------------------------------
lmA1.step <- stepAIC(lmA1, trace = FALSE, by=era_cat)
tab_model(lmA1.step, show.df = FALSE, show.aic = TRUE, show.fstat=TRUE, show.se = TRUE, show.ci=FALSE, show.stat=TRUE, digits.p=4)


## ----lahman_summary_lm1---------------------------------------------------------------------------------------------------------
lmA1.final <- update(lmA1.step, . ~ . -TEAM_BATTING_3B)
tab_model(lmA1.final, show.df = FALSE, show.aic = TRUE, show.fstat=TRUE, show.se = TRUE, show.ci=FALSE, show.stat=TRUE, digits.p=4)


## ----lahman_check_lm1-----------------------------------------------------------------------------------------------------------
check_model(lmA1.final, check=c('ncv','qq','homogeneity','outliers'))


## ----lahman_predict-------------------------------------------------------------------------------------------------------------
imputeDf <- mice(testingTeam_df, m = 5, maxit = 50, seed = 123, printFlag = F)
#imputeDf$meth
testingTeam_df <- complete(imputeDf)
m_imputed <- melt(testingTeam_df)
densityplot(imputeDf)


## ----lahman_predict_final-------------------------------------------------------------------------------------------------------
lmA1Pred.final <- lmA1.final %>% predict(testingTeam_df)


## ----lahman_predict_report------------------------------------------------------------------------------------------------------
multi_metric <- metric_set(mape, smape, mase, mpe, rmse, rsq)
m <- testingTeam_df %>% multi_metric(truth=testingTeam_df$TARGET_WINS, estimate=lmA1Pred.final)

kable(m, digits = 4, format = "html", row.names = T) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover"),
    full_width = F,
    position = "center"
  )


## ----lahman_predict_plot--------------------------------------------------------------------------------------------------------
n <- nrow(testingTeam_df)
x <- testingTeam_df$TARGET_WINS
e <- lmA1Pred.final - testingTeam_df$TARGET_WINS 


plot(x, e,  
     xlab = "wins", 
     ylab = "residuals",
     bg = "steelblue", 
     col = "darkgray", cex = 1.5, pch = 21, frame = FALSE)
abline(h = 0, lwd = 2)
for (i in 1 : n) 
  lines(c(x[i], x[i]), c(e[i], 0), col = "red" , lwd = 1)


## ----py_df----------------------------------------------------------------------------------------------------------------------
random_sample <- createDataPartition(teamDF$TARGET_WINS, p = 0.8, list = FALSE)

trainingTeam_df <- teamDF[random_sample, ]
testingTeam_df <- teamDF[-random_sample, ]


## ----py_lm----------------------------------------------------------------------------------------------------------------------
lmp <- lm(TARGET_WINS ~ pythPercent, data = trainingTeam_df)

tab_model(lmp, show.df = FALSE, show.aic = TRUE, show.fstat=TRUE, show.se = TRUE, show.ci=FALSE, show.stat=TRUE, digits.p=4)


## ----py_check_lm----------------------------------------------------------------------------------------------------------------
check_model(lmp, check=c('ncv','qq','homogeneity','outliers'))


## ----py_predict-----------------------------------------------------------------------------------------------------------------
lmpPred <- lmp %>% predict(testingTeam_df)
hist(lmpPred)


## ----py_predict_report----------------------------------------------------------------------------------------------------------
m1 <- testingTeam_df %>% multi_metric(truth=testingTeam_df$TARGET_WINS, estimate=lmpPred)

kable(m1, digits = 4, format = "html", row.names = T) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover"),
    full_width = F,
    position = "center"
  )


## ----py_predict_plot------------------------------------------------------------------------------------------------------------
n <- nrow(testingTeam_df)
x <- testingTeam_df$TARGET_WINS
e <- lmpPred - testingTeam_df$TARGET_WINS 

plot(x, e,  
     xlab = "wins", 
     ylab = "residuals", 
     bg = "steelblue", 
     col = "darkgray", cex = 1.5, pch = 21, frame = FALSE)
abline(h = 0, lwd = 2)
for (i in 1 : n) 
  lines(c(x[i], x[i]), c(e[i], 0), col = "red" , lwd = 1)


```