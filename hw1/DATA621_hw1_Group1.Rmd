---
title: "Homework #1: Moneyball"
output:
  prettydoc::html_pretty:
    theme: leonids
    highlight: github
    self_contained: no
editor_options: 
  chunk_output_type: console
bibliography: references.bib
---

```{r setup, include=FALSE}
library(tidyverse)
library(reshape2)
library(kableExtra)
library(Matrix)
library(MASS)
library(mice)
library(Hmisc)
library(corrplot)
library(performance)
library(naniar)
library(psych)
library(GGally)
library(campfin)
library(caret)
library(yardstick)


knitr::opts_chunk$set(echo = TRUE, warning = F, message = F)
set.seed(1234)

```

Critical Thinking Group 1 (Ben Inbar, Cliff Lee, Daria Dubovskaia, David Simbandumwe, Jeff Parks, Nick Oliver)


<b><i>
Notes / Questions

- added boxplot and density plot; thanks!
- do we want to use a corr plot with the numbers; I changed it to circles for presentation purposes.
  - there are a couple of variable with high correlations that we might want to explore
  - Batting_hr - Pitching_hr = .97 - this is VERY, VERY SUSPICIOUS. Do most baseball games end with similar runs for both teams? Aren't there many one-sided wins?
  - Batting_hr - Batting_so = .73 - this might be plausible. Some teams swing for the fences instead of trying to get on base; I'm looking at you, Chicago Cubs!
  - what is the cutoff in correlation that will require adjustment - good question. Above 0.9? Most baseball stats measure distinct actions and occurances. 0.9 correlations show a serious data problem or something else is fishy.
  
  - Review the distributions plots for each attribute focus on non normally distributed attributes - please read the initial attempt below.
  - Box plots show outliers discuss valid data based on https://www.baseball-reference.com/leagues/majors/bat.shtml

  - Given the number of games in a year I don't believe that 0 is a valid measurement for most of the attributes
  - use MICE to fill missing values vs Median or Mean
  
  - <del>add feature TEAM_1B? By Cliff</del>
  - define range for this stats. By David
  
|             - TEAM_BATTING_BB 12.5 ≤ 512.4 ≤ 878.4 ==> No zero values
|             - TEAM_BATTING_SO 44.2 ≤ 812 ≤ 1641.6 ==> No zero values
|             - TEAM_BASERUN_SB 3.7 ≤ 97 ≤ 697.2 ==> No zero values
|             - TEAM_BASERUN_CS 8.1 ≤ 45.2 ≤ 200.9 ==> No zero values
|             - TEAM_PITCHING_BB 21.7 ≤ 515 ≤ 881.4 ==> No zero values

|             - TEAM_FIELDING_DP 0 ≤ 145 ≤ 228.3 ==> actuall is a zero value

|             - TEAM_PITCHING_H ==> actual distribution is more standard
  
  - try to build model around pythagorean winning per
  - 1: no transformations, 2 - manual transformations, 3- box-cox
  - zeroes to NA, apply mice after
  - outliers: check the real data, maybe substitute with median/mean

</i></b>


## Overview
The use of historical statistics to predict future outcomes, particularly wins and losses, and identify opportunities for improving team or individual performance, has gained significant attention in professional sports. The aim of this analysis is to develop several models that can predict a baseball team's wins over a season based on team stats such as homeruns, strikeouts, base hits, and more. We will begin by examining the data for any issues, such as missing data, or outliers, and take the necessary measures to clean the data. We will subsequently create and evaluate three different linear models that forecast seasonal wins using the dataset, which includes both training and evaluation data. We will train the models using the main training data and then evaluate their performance against the evaluation data to determine their effectiveness. Finally, we will choose the best model that balances accuracy and simplicity for predicting seasonal wins.

## 1. DATA EXPLORATION
The baseball training dataset contains 2,276 observations of 17 variables detailing various teams' performances per year from 1871 to 2006. The description of the columns is shown below. Due to the relatively long period, we expect to see outliers and missing data as the league modified official game rules; these rule changes undoubtedly caused teams and players to change their tactics in response. Additionally, the number of single base hits is noticeably missing from the columns. However, we will derive this value as the number of other types of hits (doubles, triples, home runs) can be subtracted from total hits. Lastly, other columns representing game number (out of 162), inning number (1-9), and matching opponent columns would have been vastly useful for predictions. One last noticeable omission from the original dataset is of the number of single base hits. However, this value can possibly be calculated as a difference between other types of hits (doubles, triples, home runs) and total hits. 
<p align="center">
  <img src="https://raw.githubusercontent.com/ex-pr/DATA607/Project-1/a1_moneyball.png">
</p>

```{r initialization, include=FALSE}

trainDf <- read.csv("https://raw.githubusercontent.com/cliftonleesps/data621_group1/main/hw1/moneyball-training-data.csv")
evalDf <- read.csv("https://raw.githubusercontent.com/cliftonleesps/data621_group1/main/hw1/moneyball-evaluation-data.csv")

```


#### 1.1 Summary Statistics

The table below shows us some valuable descriptive statistics for the training data. The data set contains all integers. We can see that many of the variables have a minimum of 0 but not all. The means and medians of each variable are all relatively close in value for each individual variable. This tells us that most data is free from extreme outliers as they tend to skew the mean relative to the median.

One interesting piece of information is the min/max of the `TARGET_WINS` variable. The minimum is 0 meaning there are teams that did not win a single game. The maximum is 146 which indicates no team in the training dataset had a perfect season, as we know from the data a season consists of 162 games.

Also of note is the number of missing values from certain variables. Most notably the `TEAM_BATTING_HBP` (batters hit by pitch variable). With 91% of the data missing we will remove this variable from our dataset because there simply is not enough information to impute a sensible value. The column `TEAM_BASERUN_CS` (caught stealing) had 34% of the missing data, we may consider removing it later. The missing data for these two columns may be due to a change official rules or tactics before the modern era of baseball.


```{r summary_chart, echo=FALSE}
stats <- trainDf %>%
  dplyr::select(-c(INDEX)) %>%
  describe()

displayDf <- as.data.frame(stats) %>%
  dplyr::mutate(missing = 2276 - n) %>%
  dplyr::select(-c(vars, n, trimmed, mad, range))
kable(displayDf, digits = 2, format = "html", row.names = T) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover"),
    full_width = F,
    position = "center"
  )
```

#### 1.2 Distribution and Box Plots

Next, we'll visually check for normal distributions and box plots in both the dependent and independent variables. The density plot below shows normalcy in most features except for extremely right skewed features such as hits allowed (PITCHING_H) or errors (FIELDING_E). Homeruns by batters (BATTING_HR) and strikeouts by batters (BATTING_SO) variables seem bimodal. It implies the existence of two distinct clusters within the baseball season data, where teams tended to score more in one of the clusters. \
Box plots for these further show a high number of outliers exist outside of the interquartile ranges so their effects should be carefully considered and we may deal with non-unimodal distributions.

```{r density_boxplot, echo=FALSE, warning=FALSE, message=FALSE, fig.height=8, fig.width=10}
m_df <- trainDf%>%
  dplyr::select(-c(INDEX)) %>%
  melt() 

m_df %>% ggplot(aes(x= value)) + 
    geom_density(color='#023020', fill='gray') + facet_wrap(~variable, scales = 'free',  ncol = 4) + theme_bw()


m_df %>% ggplot(aes(x = value)) +
  geom_boxplot(outlier.color = 'red', outlier.shape = 1) +
  facet_wrap(vars(variable),scales = "free", ncol = 4)

```

**We should DO this instead? Will take less space**
```{r wins_features, echo=FALSE, fig.height=8, fig.width=10}
featurePlot(trainDf[,2:ncol(trainDf)], trainDf[,1], plot = "scatter", type = c("p", "smooth"), span = 1)
```

#### 1.3 Correlation Matrix

Plotting the correlations between TARGET_WINS and the variables (excluding INDEX and TEAM_BATTING_HBP) we can see that very few variables are strongly correlated with the target variable. Columns with correlations close to zero are unlikely to offer significant insights into the factors that contribute to a team's victories.\

To avoid multicolinearity, we should not include features that have strong correlation. Comparing offensive (any column starting with `BATTING` or `BASERUN`) to defensive stats unexpectedly shows some correlation, pointing to potential problems. Qualitatively, the matrix implies some teams or players are exceptional both at hitting (offensive) and fielding (defensive). Furthermore, a typical team's number of batted home runs and allowed home runs has a correlation of nearly 1.0. This is an unexpected correlation but can be explained by noticing most games are decided by a difference of one or two runs (whether the games are high scoring or not). Any final models should include one of these two home run variables. Alternatively, the correlation between a team's hits (`BATTING_H`) and hits allowed (`PITCHING_H`) is around 0.3 which is seems reasonable.

There are some other strong correlations that are less obvious such as Errors (`TEAM_FIELDING_E`) being strongly negatively correlated with walks by batters (`TEAM_BATTING_BB`), strike outs (`TEAM_BATTING_SO`).  All combined together, teams that get a lot of hits do not generally make fielding errors.

Digging a little deeper we can see there is a Pearson correlation coefficient of `r cor(trainDf$TEAM_FIELDING_E, trainDf$TEAM_BATTING_BB)` for errors and walks by batters which indicates a strong negative correlation between the two variables. Looking at errors compared with team pitching hits allowed we see a correlation of `r cor(trainDf$TEAM_FIELDING_E, trainDf$TEAM_PITCHING_H)` which indicates a strong positive correlation. 


```{r correlation_matrix, echo=FALSE}
rcore <- rcorr(as.matrix(trainDf %>% dplyr::select(where(is.numeric) & -INDEX & -TEAM_BATTING_HBP)))
coeff <- rcore$r
corrplot(coeff, tl.cex = .7 , method = 'circle')
```

```{r corr_numbers, echo=FALSE}
tst <- trainDf
tst <- tst[,-1 ]

kable(cor(drop_na(tst))[,1], "html", escape = F, col.names = c('Coefficient')) %>%
  kable_styling("striped", full_width = F) %>%
  column_spec(1, bold = T)


```

Lastly lets take a closer look at the missing data. We've already determined that the batter hit by pitch (`TEAM_BATTING_HBP`) variable is missing 91% of its data but what of the other variables. We will just drop the column from the further analysis.

Using the plot below we can visualize the missingness of the remaining variables. There are 5 variables that contain varying degrees of missing data. We will use the information to fill in the missing values in our data preparation step.

`TEAM_BASERUN_CS` appears to be missing the second most amount of values but at only 772 missing values out of 2276 this is much less of a concern than the HBP variable we identified earlier. The remaining variables that are missing data have less than 25% of their data missing so should be safe to impute. 

```{r missing_data, echo=FALSE}
gg_miss_var(trainDf)
```


## 2. DATA PREPARATION


### 2.1 Missing Data

<b><i>
Notes / Questions

- BASE_CS is missing 33% of its values is it an issue to fill that many values
- added the na_flag to the data set
- do we want to calculate singles?
- should we include Hits and Singles, Doubles, Triples, HR - this is the same data captured differently
- Slugging Percentage is an interesting measure that we could add https://en.wikipedia.org/wiki/Slugging_percentage @enwiki:1123388426
- Here are some advanced stats that might be interesting https://www.mlb.com/glossary/advanced-stats
- It would be interesting to test Pythagorean Winning Percentage https://www.mlb.com/glossary/advanced-stats/pythagorean-winning-percentage

</i></b>

First what we will do is drop the `INDEX` and `TEAM_BATTING_HBP` variables from our dataset. We've already determined that the `TEAM_BATTING_HBP` variable is missing 91% of its data and the `INDEX` variable is simply an index of the row number.

```{r echo=FALSE}
droppedDf <- trainDf %>% dplyr::select(-c(INDEX, TEAM_BATTING_HBP))
```

We'll also derive a new column for single base hits derived from subtracting double, triples and home runs from the total number of hits.

```{r single_base_hits}
droppedDf <- droppedDf %>% mutate(TEAM_BATTING_1B = TEAM_BATTING_H - TEAM_BATTING_2B - TEAM_BATTING_3B - TEAM_BATTING_HR)
```


MAYBE For easier understanding of the data, we will remove `TEAM` word from the column names?
```{r}
#names(droppedDf) <- names(droppedDf) %>% 
 # str_replace_all('TEAM_', '')

#names(evalDf) <- names(evalDf) %>% 
  #str_replace_all('TEAM_', '')
```

```{r}

columnNames <- names(droppedDf)
droppedDf <- flag_na(droppedDf, columnNames)


columnNames <- names(evalDf)
evalDf <- flag_na(evalDf, columnNames)

```


Next we will use the `mice` library to impute the missing values in the `trainDf` data.frame. MICE is actually an acronym which stands for multiple imputation by chained equations. In order to use MICE one must assume that the missing values are missing at random, meaning the missingness can be accounted for by variables where there is complete information. Then as the name implies MICE runs multiple iterations over the data and generates the data to fill in the missing values.

**Part below is the modification from Ben**
We check what impute method we use for each column. "pmm" is predictive mean matching, replacing missing data with column means.

Let's also take a look at the density plots pre and post-imputation to make sure densities look similar. Unfortunately, for `TEAM_BASERUN_SB`, `TEAM_BATTING_SO`, and `TEAM_FIELDING DP` they do not. But in the case of `TEAM_BATTING_SO` our distribution becomes roughly more normal, so it may be beneficial. For the `TEAM_BASERUN_SB` and `TEAM_BASERUN_CS` and `TEAM_FIELDING_DP` we may need to consider alternative methods.

```{r}
imputeDf <- mice(droppedDf, m = 5, maxit = 50, seed = 123, printFlag = F)
imputeDf$meth
cleanDf <- complete(imputeDf)
m_imputed <- melt(cleanDf)
densityplot(imputeDf)
```

We should also try to transform some variables so that they may fit a more normal distribution, particularly `TEAM_BATTING_HR`, `TEAM_BATTING_SO`, `TEAM_PITCHING_HR`, but also `TEAM_PITCHING_H`, `TEAM_PITCHING_BB`, `TEAM_PITCHING_SO`, and `TEAM_FIELDING_E`. Square root doesn't seem to help much.

```{r}
# Reminder of our distributions
m_imputed %>% ggplot(aes(x= value)) + 
    geom_density(fill='gray') + facet_wrap(~variable, scales = 'free') +
    theme(strip.text.x = element_text(size = 6, angle = 0))
# Transform some non-normal variables.
transformed_df <- cleanDf |> mutate_each(funs(sqrt),
                   batting_hr_log = TEAM_BATTING_HR,
                   batting_so_log = TEAM_BATTING_SO,
                   baserun_cs_log = TEAM_BASERUN_CS,
                   pitching_hr_log = TEAM_PITCHING_HR) |> melt()
# Plot. Not great.
transformed_df %>% ggplot(aes(x= value)) + 
    geom_density(fill='gray') + facet_wrap(~variable, scales = 'free') +
    theme(strip.text.x = element_text(size = 6, angle = 0))
library(scales)
# May need to try Boxcox after fitting the model. What we can do is remove large low density ranges for some variables.
transformed_df <- cleanDf |> subset(TEAM_PITCHING_H < 5000, TEAM_PITCHING_BB < 1100) |> melt()
transformed_df |> ggplot(aes(x= value)) + 
    geom_density(fill='gray') + facet_wrap(~variable, scales = 'free') +
    theme(strip.text.x = element_text(size = 6, angle = 0))
#TEAM_PITCHING_BB < 1100, TEAM_PITCHING_SO < 2500, TEAM_FIELDING_E < 500)
```

**End of Ben's modification**

We now have a data set free from missing data values with the meaningless `INDEX` variable removed and the `TEAM_BATTING_HBP` removed as well due to it containing 91% missing values. We can observe using the table below that there are no longer any missing values. 

One interesting comparison we can make is for a variable that had a large number of missing values, we can look at how the summary statistics may have changed with the imputed data. 

`TEAM_BASERUN_CS` was missing 772 values or about 34% of data. We can observe that the mean and median did change from 52.80 to 75.71 and 49.0 to 57.0 The min of 0 and max of 201 did not change which is good.

```{r}
stats <- cleanDf  %>%
  describe()
displayDf <- as.data.frame(stats) %>%
  dplyr::mutate(missing = 2276 - n) %>%
  dplyr::select(-c(vars, n, trimmed, mad, range))
kable(displayDf, caption = "Money Ball Dataset", digits = 2, format = "html", row.names = T) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover"),
    full_width = F,
    position = "center"
  )
```



### 3. BUILD MODELS

Now I will build three separate linear models and compare their performance and fit. 

For the first model I will use the naive method of selecting every variable in the raw un-cleaned data set. Our intuition tells us this is unlikely to be the best performing model as we already know there are some issues with the data and we are using every variable. 

<b><i>
Notes / Questions

- we could try building a model that does not include any of the outlier variables values
- there is also saber metrics model that Pythagorean Winning Percentage taht is supposed to preduct wins https://www.mlb.com/glossary/advanced-stats/pythagorean-winning-percentage

</i></b>
  

```{r}
lm1 <- lm(TARGET_WINS ~ ., data = trainDf)
lm1Sum <- summary(lm1)
```

For the second model I will use the cleaned data set with missing values imputed using the MICE method. In addition I will try to select variables which result in a better fit using my out intuition. 

```{r}
lm2 <- lm(formula = TARGET_WINS ~  
    TEAM_PITCHING_HR + TEAM_PITCHING_BB  + TEAM_BATTING_2B + TEAM_BATTING_3B +
   TEAM_FIELDING_E, data = cleanDf)
lm2Sum <- summary(lm2)
```

```{r}
lm3 <- lm(TARGET_WINS ~ ., data=cleanDf)
aicLm <- stepAIC(lm3, trace = FALSE)
aicSum <- summary(aicLm)
```

Below are the results $R^2$, residual standard error, and F-statistics of each model. Surprisingly the non-cleaned, non-imputed raw training data had the best fitting statistics. 

```{r}
plot(compare_performance(lm1,lm2,aicLm, rank=T))
```

```{r}
r <- c(lm1Sum$r.squared, lm2Sum$r.squared, aicSum$r.squared)
rsse <- c(lm1Sum$sigma, lm2Sum$sigma, aicSum$sigma)
adjusted.r <- c(lm1Sum$adj.r.squared, lm2Sum$adj.r.squared, aicSum$adj.r.squared)
modelDf <- data.frame(r,rsse,adjusted.r)
kable(modelDf, caption = "Money Ball Dataset", digits = 2, format = "html", row.names = F) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover"),
    full_width = F,
    position = "center"
  )
```


## 4. SELECT MODELS

I will then use the evaluation data set to make predictions using the three models. In order to make the first and third prediction models  work I will need to impute the missing values from the evaluation dataset. I will use the same method as I did for the training data set. If I do not add these missing values the prediction results will return no values for the rows that contain the missing values.


Below is a table containing the predicted TARGET_WINS for each model. Some things that stand out at a first glance are that the first model which is producing negative value predictions. Obviously it isn't possible to have a negative amount of wins, so this model is not very useful. The second and third model which are both based on the cleaned and imputed data do not suffer from these issues of predicting large negative values. In general both the AIC generated model and the second model are producing similar results.

```{r}
evalImputeDf <- mice(evalDf, m = 5, maxit = 50, seed = 123, printFlag = F)
evalImputeDf <- complete(evalImputeDf)
```

```{r}
lm1Pred <- lm1 %>% predict(evalImputeDf)
lm2Pred <- lm2 %>% predict(evalImputeDf)
aicPred <- aicLm %>% predict(evalImputeDf)
```

```{r}
predsDf <- evalImputeDf %>% 
  mutate(lm1 = lm1Pred, lm2 = lm2Pred, aic = aicPred) %>%
  dplyr::select(c(lm1, lm2, aic))
  kable(head(predsDf, 15), caption = "Money Ball Dataset", digits = 2, format = "html", row.names = F) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover"),
    full_width = F,
    position = "center"
  )
```


We can also see when plotting the predictions that there doesn't seem to be much obvious difference between the models aside from the clearly outrageous outliers generated by the first model.

```{r}
xT <- evalImputeDf %>% 
  mutate(lm1 = lm1Pred, lm2 = lm2Pred, aic = aicPred)
par(mfrow=c(2,2))
plot(1:nrow(xT), xT$lm1, xlab="LM1", ylab="Wins", title="LM1")
plot(1:nrow(xT), xT$lm2, xlab="LM2", ylab="Wins", title="LM2")
plot(1:nrow(xT), xT$aic, xlab="AIC", ylab="Wins", title="AIC")
```

We can the graphs below to check the validity of our models. All three models suffer from a lack of linearity which indicates that a linear regression model may not be the greatest technique for predicting values from this data with the given variables. The two models that included all the most variables (model 1 & model 3), suffer from co-linearity issues.

### Model 1

```{r}
check_model(lm1, check = c("vif","normality","linearity","homogeneity"))

```

### Model 2

```{r}
check_model(lm2, check = c("vif","normality","linearity","homogeneity"))
```

### Model 3

```{r}
check_model(aicLm, check = c("vif","normality","linearity","homogeneity"))
```

## Conclusion



<b><i>
Notes / Questions

- This is actually a better source to do the research :) https://www.seanlahman.com/baseball-archive/statistics/

</i></b>



Overall none of the models that I was able to generate instill much confidence in their ability to predict. The model with the best fit according to the $R^2$ statistic was filled with missing data that caused clearly incorrect negative predictions.

The second and third models both had significantly lower $R^2$ scores which indicated a poor fit overall. In addition, none of the models performed well when checked for linearity or homogeneity of variance. While the second model did not suffer from colinearity issues the other two models did.



## Appendix A

Even with significant efforts to compensate for poor data quality the resulting models are poor predictors of win totals. Finding better data is the best course of action for developing a better predictive model.

https://www.seanlahman.com/baseball-archive/statistics/


```{r}

teamDF <- evalDf <- read.csv("https://raw.githubusercontent.com/cliftonleesps/data621_group1/main/hw1/Teams.csv")

teamDF <- teamDF %>% dplyr::select('yearID','lgID','teamID','franchID','divID','Rank','G','W','L',
                         'R','AB','H','X2B','X3B','HR','BB','SO','SB','CS','HBP','SF','RA','ER','ERA',
                         'CG','SHO','SV','IPouts','HA','HRA','BBA','SOA','E','DP','FP','name')




# normalize data 162 games
teamDF$gFactor <-  162 / (teamDF$W + teamDF$L)

teamDF$TARGET_WINS <-  teamDF$gFactor * teamDF$W
teamDF$TEAM_BATTING_H <- teamDF$gFactor * teamDF$H
teamDF$TEAM_BATTING_2B  <- teamDF$gFactor * teamDF$X2B
teamDF$TEAM_BATTING_3B  <- teamDF$gFactor * teamDF$X3B
teamDF$TEAM_BATTING_HR  <- teamDF$gFactor * teamDF$HR
teamDF$TEAM_BATTING_BB  <- teamDF$gFactor * teamDF$BB
teamDF$TEAM_BATTING_SO  <- teamDF$gFactor * teamDF$SO
teamDF$TEAM_BASERUN_SB  <- teamDF$gFactor * teamDF$SB
teamDF$TEAM_BASERUN_CS  <- teamDF$gFactor * teamDF$CS
teamDF$TEAM_BATTING_HBP <- teamDF$gFactor * teamDF$HBP
teamDF$TEAM_PITCHING_H  <- teamDF$gFactor * teamDF$HA
teamDF$TEAM_PITCHING_HR <- teamDF$gFactor * teamDF$HRA
teamDF$TEAM_PITCHING_BB <- teamDF$gFactor * teamDF$BBA
teamDF$TEAM_PITCHING_SO <- teamDF$gFactor * teamDF$SOA
teamDF$TEAM_FIELDING_E  <- teamDF$gFactor * teamDF$E
teamDF$TEAM_FIELDING_DP <- teamDF$gFactor * teamDF$DP 


# Pythagorean winning percentage
#  (runs scored ^ 2) / [(runs scored ^ 2) + (runs allowed ^ 2)]
teamDF$pythPercent <- (teamDF$R^2) / ((teamDF$R^2) + (teamDF$RA^2))


teamAdjDF <- teamDF %>% dplyr::select("TARGET_WINS","TEAM_BATTING_H","TEAM_BATTING_2B","TEAM_BATTING_3B","TEAM_BATTING_HR",
                "TEAM_BATTING_BB","TEAM_BATTING_SO","TEAM_BASERUN_SB","TEAM_BASERUN_CS",
                "TEAM_BATTING_HBP","TEAM_PITCHING_H","TEAM_PITCHING_HR","TEAM_PITCHING_BB",
                "TEAM_PITCHING_SO","TEAM_FIELDING_E","TEAM_FIELDING_DP")


```



```{r}

tDF <- teamAdjDF 

t = preProcess(tDF, c("BoxCox", "center", "scale"))
tDF <- data.frame(predict(t, tDF))

teamAdjDF$BC_TEAM_BASERUN_SB <- tDF$TEAM_BASERUN_SB
teamAdjDF$BC_TEAM_BATTING_H <- tDF$TEAM_BATTING_H
teamAdjDF$BC_TEAM_FIELDING_E <- tDF$TEAM_FIELDING_E

```




```{r}

random_sample <- createDataPartition(teamAdjDF$TARGET_WINS,
								p = 0.8, list = FALSE)

trainingTeam_df <- teamAdjDF[random_sample, ]
testingTeam_df <- teamAdjDF[-random_sample, ]


```







```{r}

m_df <- trainingTeam_df %>% melt() 

m_df %>% ggplot(aes(x= value)) + 
    geom_density(color='#023020', fill='gray') + facet_wrap(~variable, scales = 'free',  ncol = 4) + theme_bw()


m_df %>% ggplot(aes(x = value)) +
  geom_boxplot(outlier.color = 'red', outlier.shape = 1) +
  facet_wrap(vars(variable),scales = "free", ncol = 4)

```



```{r}

rcore <- rcorr(as.matrix(trainingTeam_df %>% dplyr::select(where(is.numeric))))
coeff <- rcore$r
corrplot(coeff, tl.cex = .7 , method = 'number')

```


```{r}

imputeDf <- mice(trainingTeam_df, m = 5, maxit = 50, seed = 123, printFlag = F)
imputeDf$meth
trainingTeam_df <- complete(imputeDf)
m_imputed <- melt(trainingTeam_df)
densityplot(imputeDf)

```




```{r}

lmA1 <- lm(TARGET_WINS ~ . -TEAM_BASERUN_SB - TEAM_BATTING_H - TEAM_FIELDING_E, data = trainingTeam_df)
lmA1Sum <- summary(lmA1)
lmA1Sum

```


```{r}

lmA1.step <- stepAIC(lmA1, trace = FALSE)
lmA1Sum.step <- summary(lmA1.step)
lmA1Sum.step

```



```{r}

check_model(lmA1.step, check = c("vif","normality","linearity","homogeneity"))

```




```{r}

imputeDf <- mice(testingTeam_df, m = 5, maxit = 50, seed = 123, printFlag = F)
imputeDf$meth
testingTeam_df <- complete(imputeDf)
m_imputed <- melt(testingTeam_df)
densityplot(imputeDf)


```




```{r}

lmA1Pred.step <- lmA1.step %>% predict(testingTeam_df)

```





```{r}


multi_metric <- metric_set(mape, smape, mase, mpe, rmse, rsq)
m <- testingTeam_df %>% multi_metric(truth=testingTeam_df$TARGET_WINS, estimate=lmA1Pred.step)

```








## Appendix B:



```{r}

random_sample <- createDataPartition(teamDF$TARGET_WINS, p = 0.8, list = FALSE)

trainingTeam_df <- teamDF[random_sample, ]
testingTeam_df <- teamDF[-random_sample, ]


```





```{r}

lmp <- lm(TARGET_WINS ~ pythPercent, data = trainingTeam_df)
summary(lmp)


```


```{r}

check_model(lmp, check = c("vif","normality","linearity","homogeneity"))

```


```{r}


lmpPred <- lmp %>% predict(testingTeam_df)


```



```{r}

m1 <- testingTeam_df %>% multi_metric(truth=testingTeam_df$TARGET_WINS, estimate=lmpPred)

```



## References
