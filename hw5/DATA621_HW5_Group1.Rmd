---
title: 'Homework #5: Count Regression (Wines)'
subtitle: 'Critical Thinking Group 1'
author: 'Ben Inbar, Cliff Lee, Daria Dubovskaia, David Simbandumwe, Jeff Parks'
output:
  html_document:
    toc: yes
    toc_float: yes
    theme: united
  pdf_document:
    toc: yes
editor_options:
  chunk_output_type: console
  markdown: 
    wrap: sentence
---

```{r, include=FALSE}
# if error for kableExtra, do
# devtools::install_github("kupietz/kableExtra")
```

```{r setup, include=FALSE}
# chunks
knitr::opts_chunk$set(echo=FALSE, eval=TRUE, include=TRUE, 
message=FALSE, warning=FALSE, fig.height=5, fig.align='center')

# libraries
library(tidyverse)
library(kableExtra)
library(MASS) # glm.nb()
library(mice)
library(pscl) # zeroinfl()
library(skimr)
library(sjPlot)
library(mpath)

# ggplot
theme_set(theme_light())

# random seed
set.seed(42)
```

```{css}
# only required for final knit
#h4, h5 {margin-top: 20px;}
```

```{r common functions}
nice_table <- function(df){
  table <- df %>% kable %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"),
                html_font = 'monospace',
                full_width = FALSE)
  return(table)
}

model_diag <- function(model){
  model_sum <- summary(model)
  aic <- AIC(model)
  ar2 <- model_sum$adj.r.squared
  disp <- sum(resid(model,'pearson')^2)/model$df.residual
  loglik <- logLik(model)
  
  vec <- c(ifelse(is.null(aic), NA, aic),
           ifelse(is.null(ar2), NA, ar2),
           ifelse(is.null(disp), NA, disp),
           ifelse(is.null(loglik), NA, loglik))
  
  names(vec) <- c('AIC','Adj R2','Dispersion','Log-Lik')
  return(vec)
}
```

# Assignment

*In this homework assignment, you will explore, analyze and model a data set containing information on approximately 12,000 commercially available wines. The variables are mostly related to the chemical properties of the wine being sold. The response variable is the number of sample cases of wine that were purchased by wine distribution companies after sampling a wine. Your objective is to build a count regression model to predict the number of cases of wine that will be sold given certain properties of the wine.*

We'll build two poisson regressions, two negative binomial regressions, and two multivariate linear regression models.

------------------------------------------------------------------------

# Data Exploration

In order to explore summary stats and distribution characteristics of our dataset, we'll need to first conduct some basic transformations and cleanup:

```{r}
# load data
df_train <- read.csv('https://raw.githubusercontent.com/cliftonleesps/data621_group1/main/hw5/data/wine-training-data.csv')
df_predict <- read.csv('https://raw.githubusercontent.com/cliftonleesps/data621_group1/main/hw5/data/wine-evaluation-data.csv')

# fix index name in predict for bind
df_predict <- df_predict %>% rename(INDEX=IN)

# union for exploration and prep
df_train <- df_train %>% mutate(source='train')
df_predict <- df_predict %>% mutate(source='predict')
df_all <- bind_rows(df_train, df_predict)

# fix column labels
names(df_all) <- str_to_lower(str_replace_all(names(df_all), c(" " = "_" , "," = "", "\\*" = "", "\\(" = "", "\\)" = "", "`" = "", "\\/" = "_")))

# drop un-necessary columns
df_all <- df_all %>% dplyr::select(!index)
```

-   The provided dataset contains a single response variable `target`, a numeric variable indicating the number of cases purchased.
-   The 'evaluation' dataset contains no values for `target`, suggesting this data might be used for prediction rather than validation and evaluation of model performance. For clarity we'll rename this this dataset 'prediction' instead and create a separate validation hold-out from the training data.
-   There is a numeric `index` column labeling the observations which can be excluded from the models.
-   `r nrow(df_predict)` observations (or `r round(nrow(df_predict)/nrow(df_all),2)*100`%) of the total dataset have been set aside for prediction.
-   The combined training and prediction datasets consist of `r nrow(df_all)` observations containing `r ncol(df_all)-2` predictor variables:

```{r}
# summary stats for all non-dummy variables
df_all %>% 
  dplyr::select(!c(source, target)) %>%
  skim() %>%
  dplyr::select(skim_variable, complete_rate, n_missing, 
                numeric.p0, numeric.p100) %>%
  rename(variable=skim_variable, min=numeric.p0, max=numeric.p100) %>%
  mutate(complete_rate=round(complete_rate,2), 
         min=round(min,2), max=round(max,2)) %>%
  arrange(variable) %>%
  nice_table()
```

```{r summary1}
# DT::datatable(
#       df_all[1:25,],
#       extensions = c('Scroller'),
#       options = list(scrollY = 350,
#                      scrollX = 500,
#                      deferRender = TRUE,
#                      scroller = TRUE,
#                      dom = 'lBfrtip',
#                      fixedColumns = TRUE, 
#                      searching = FALSE), 
#       rownames = FALSE) 
```

#### Definitions

-   `AcidIndex`: Proprietary method of testing total acidity of wine by using a weighted average
-   `Alcohol`: Alcohol Content
-   `Chlorides`: Chloride content of wine
-   `CitricAcid`: Citric Acid Content
-   `Density`: Density of Wine
-   `FixedAcidity`: Fixed Acidity of Wine
-   `FreeSulfurDioxide`: Sulfur Dioxide content of wine
-   `LabelAppeal`: Marketing Score indicating the appeal of label design for consumers. High numbers suggest customers like the label design. Negative numbers suggest customers don't like the design.
-   `ResidualSugar`: Residual Sugar of wine
-   `Stars`: Wine rating by a team of experts. 4 Stars = Excellent, 1 Star = Poor. A high number of stars suggests high sales
-   `Sulphates`: Sulfate content of wine
-   `TotalSulfurDioxide`: Total Sulfur Dioxide of Wine
-   `VolatileAcidity`: Volatile Acid content of wine
-   `pH`: pH of wine

------------------------------------------------------------------------

#### Transformed Variables?

One of the first characteristics that stand out is the presence of negative values for many chemical compounds, and the relative normality of their distributions.
This suggests they have already been power-transformed to produce normal distributions for modeling.

Variables related to sugars, chlorides, acidity, sulfides and sulfates all seem to fall in this category.
Considering that we are analyzing very tiny amounts of chemical compounds, we might assume their natural distributions may be highly skewed.

```{r}
numeric_cols <- sapply(df_all, is.numeric)

df_all[,numeric_cols] %>%
  dplyr::select(!target) %>%
  pivot_longer(everything(),names_to = c('variables'),values_to = c('values')) %>% 
  ggplot(aes(x=values)) +
  geom_histogram(alpha=0.5, colour='black', size=0.2) +
  facet_wrap(vars(variables), scales="free")
```

We tried exponentiation of these variables by the natural log and other values, but did not arrive at an obvious or consistent transformation approach - so we may not be able to interpret model results on the scale of the original values for these variables.

------------------------------------------------------------------------

### Handling Missing Data

Next we'll find and impute any missing data.
There are 8 predictor variables that contain NAs:

```{r}
# working copy
df_prep <- df_all

# find columns with NAs
x <- df_prep %>%
  dplyr::select(!target) %>%
  apply(2, function(col) sum(is.na(col)))

x[x>0] %>% as.data.frame() %>% 
  setNames('is_na') %>% 
  mutate(pct = round(is_na/nrow(df_prep),2)) %>%
  arrange(desc(pct)) %>%
  nice_table()
```

Heeding the warning in the assignment, *"sometimes, the fact that a variable is missing is actually predictive of the target"*, we'll consider each of these variables carefully.
While there may be data "missing completely at random" (MCAR) that we wish to impute, this may not always be the case.

#### Missing Data - Stars

The predictor `Stars` suggests that out of 16,000 wine samples, about 25% have never been professionally reviewed.
If we assume that the existence of a review has some impact on the sales of a wine brand (whatever the reviewer's sentiment), then imputing mean or predicted values here might distort our model.

To enable further analysis we'll convert `stars` from a numeric to a factor, with a level '0' representing our NA values.

```{r}
df_prep <- df_prep %>%
  mutate(stars = as.factor(ifelse(is.na(stars),0,stars)))
```

#### Missing Data - Chemical Compounds

Next we consider some of the missing chemical compounds in our wines; alcohol, sugars, chlorides, sulfites and sulfates, and measures such as `ph`.

First, can safely assume that all wines in this dataset have an actual `ph` score greater than zero (which would represent the most acidic rank, such as powerful industrial acids.) We'll want to impute more reasonable values for these.

Based on some reading into the organic wines segment, there is a growing demand in the market for specialty products such as low-sulfite, low-sugar and low-alcohol wines.
However, this still represents a very small segment of the overall market, and chemically it's not likely for these compounds to be completely absent from the final product.

Additionally, the predictors `freesulfurdioxide` and `totalsulfurdioxide` are linked - the amount of 'Free' SO2 in wine is always a subset of the 'Total' S02 present.
We only identified 59 cases where both these values were NA, while over 1500 cases had missing values for only one or the other.

Based on these observations, we'll use the MICE imputation method to predict and impute the missing values for `residualsugar`, `chlorides`, `freesulphurdioxide`, `totalsulfurdioxide`, `sulphates`, `alchohol` and `ph`.

Target/source labels and non-chemical predictors `labelappeal` and `stars` were excluded as predictors for the imputation.

```{r}
# MICE setup
init = mice(df_prep, maxit=0) 
meth = init$method
predM = init$predictorMatrix

# omit from MICE predictors
predM[, c('target','labelappeal','stars','source')] = 0

# omit from MICE imputation
meth[c('target','labelappeal','stars','source')] = ''
```

```{r, include=FALSE}
impute = mice(df_prep, method=meth, predictorMatrix=predM, m=5)
df_prep_imputed <- complete(impute)
```

#### Data Sparseness - Label Appeal

`labelappeal` is a numeric score of consumer ratings for a wine brand's label design.
It has also been pre-transformed to produce a normal distribution for modeling; however this is a very sparse variable with nearly half the cases having a value of zero.

This may be candidate for handling with Zero-Inflated models.
We won't change the values here, but will convert `labelappeal` from a numeric to a factor.

```{r}
df_prep_imputed <- df_prep_imputed %>%
  mutate(labelappeal = as.factor(labelappeal))
```

------------------------------------------------------------------------

#### Examine Final Dataset

We now have reasonably imputed values, and nearly-normal distributions for our numeric predictors, taking special note of the frequency of zero values for `labelappeal` and `stars`.

```{r}
n_missing <- apply(df_prep_imputed, 2, function(col) sum(is.na(col)))
n_zero <- colSums(df_prep_imputed==0)

df <- data.frame(n_missing, n_zero)
df <- cbind(variable=rownames(df),df)
rownames(df) <- NULL

df %>% 
  dplyr::filter(variable != 'source' & variable != 'target') %>%
  arrange(variable) %>% 
  nice_table()
```

```{r}
numeric_cols <- sapply(df_prep_imputed, is.numeric)

df_prep_imputed[,numeric_cols] %>%
  dplyr::select(!target) %>%
  pivot_longer(everything(),names_to = c('variables'),values_to = c('values')) %>% 
  ggplot(aes(x=values)) +
  geom_histogram(alpha=0.5, colour='black', size=0.2) +
  facet_wrap(vars(variables), scales="free")
```

#### Split Datasets

With transformations complete, we split back into training and prediction datasets based on our `source_flag`, and create a 15% validation hold-out from the training data.

```{r}
# split back to train and test, remove source flag
df_train_all <- df_prep_imputed %>% filter(source == 'train') %>% dplyr::select(!source)

df_predict <- df_prep_imputed %>% filter(source == 'predict') %>% dplyr::select(!source)

# create a validation holdout from the training dataset
row_sample <- sample(c(TRUE,FALSE), nrow(df_train_all), replace=TRUE, prob=c(0.85,0.15))

df_train <- df_train_all[row_sample,]
df_valid <- df_train_all[!row_sample,]
```

------------------------------------------------------------------------

# Build Models

## Poisson Regression 1

Poisson Regression assumes that the variance and mean of our dependent variable `target` are roughly equal, otherwise we may be looking at over- or under-dispersion.

```{r, echo=TRUE}
pr1 <- glm(target ~ ., family = 'poisson', data = df_train)
```

```{r}
summary(pr1)

pr1_diag <- model_diag(pr1)

pr1_diag[c('AIC','Dispersion','Log-Lik')] %>% 
  round(2) %>% nice_table()
```

We note that our model has generated 'dummies' from our categorical variables `labelappeal` and `stars`, and of the 20 total predictors, all but five have statistical significance.

Notably, our Dispersion Parameter is `r round(pr1_diag['Dispersion'],2)`, which suggests a degree of under-dispersion in the data.

### Diagnostics

By graphing our target values (green) against our predicted values (blue) we can easily see this model tends to under-predict the higher count levels, and wildly over-predict the lower count levels.

```{r, fig.align='default', out.width="50%", fig.height=4}
# predict based on our validation holdout
pr1_valid <- predict(pr1, newdata = df_valid)

# bind the predicted and actuals for comparison
pr1_eval <- bind_cols(target = df_valid$target, predicted= pr1_valid)
pr1_eval[is.na(pr1_eval)] <- 0

# Diagnostic plots

# xy plot of model fitted and residuals
bind_cols(fitted=unname(fitted(pr1)), resid=unname(resid(pr1))) %>%
  ggplot(aes(x=fitted, y=resid)) +
  geom_point(alpha = .3) +
  geom_hline(yintercept=2, linetype='dashed') +
  geom_hline(yintercept=-2, linetype='dashed')

# xy plot of validation predictions and targets
pr1_eval %>%
  ggplot(aes(x = target, y = predicted)) +
  geom_point(alpha = .3) +
  geom_smooth(method="lm", color='grey', alpha=.3, se=FALSE)

# density plot of validation predictions and targets
pr1_eval %>%
  ggplot() +
  geom_density(aes(x=target), fill='green', alpha=0.25) +
  geom_density(aes(x=round(predicted,0)), fill='blue', alpha=0.25)
```

------------------------------------------------------------------------

## Poisson Regression 2

We'll build a Zero-Inflated Poisson model to handle the large number of zero values in our `labelappeal` and `stars` predictors, to see if we can improve model accuracy.

```{r, echo=TRUE}
pr2 <- zeroinfl(target ~ . | ., data=df_train, dist = 'poisson')
```

```{r}
summary(pr2)

pr2_diag <- model_diag(pr2)

pr2_diag[c('AIC','Dispersion','Log-Lik')] %>% 
  round(2) %>% nice_table()
```

### Diagnostics

Using a Zero-Inflated model, the Dispersion Parameter drops significantly, but we are getting a better overall result for counts of 3 or more.
By graphing our target values (green) against our predicted values (blue) we can see we are getting much greater accuracy rate for most of the mid- and upper counts.

Notably, we are still under-predicting counts of 1-2, and greatly over-predicting counts of zero.

```{r, fig.align='default', out.width="50%", fig.height=4}
# predict based on our validation holdout
pr2_valid <- predict(pr2, newdata = df_valid)

# bind the predicted and actuals for comparison
pr2_eval <- bind_cols(target = df_valid$target, predicted= pr2_valid)
pr2_eval[is.na(pr2_eval)] <- 0

# Diagnostic plots

# xy plot of model fitted and residuals
bind_cols(fitted=unname(fitted(pr2)), resid=unname(resid(pr2))) %>%
  ggplot(aes(x=fitted, y=resid)) +
  geom_point(alpha = .3) +
  geom_hline(yintercept=2, linetype='dashed') +
  geom_hline(yintercept=-2, linetype='dashed')

# xy plot of validation predictions and targets
pr2_eval %>%
  ggplot(aes(x = target, y = predicted)) +
  geom_point(alpha = .3) +
  geom_smooth(method="lm", color='grey', alpha=.3, se=FALSE)

# density plot of validation predictions and targets
pr2_eval %>%
  ggplot() +
  geom_density(aes(x=target), fill='green', alpha=0.25) +
  geom_density(aes(x=round(predicted,0)), fill='blue', alpha=0.25)
```

------------------------------------------------------------------------

## Negative Binomial Regression 1

Generally, we would use Negative Binomial Regression in cases of over-dispersion (where the variance of our dependent variable is significantly greater than the mean.) This does not appear to be the case with our dataset, but we'll apply it here and examine the results:

```{r, echo=TRUE}
nb1 <- glm.nb(target ~ ., data = df_train)
```

```{r}
summary(nb1)

nb1_diag <- model_diag(nb1)

nb1_diag[c('AIC','Dispersion','Log-Lik')] %>% 
  round(2) %>% nice_table()
```

### Diagnostics

As expected, the Negative Binomial Regression does not outperform the Poisson.

```{r, fig.align='default', out.width="50%", fig.height=4}
# predict based on our validation holdout
nb1_valid <- predict(nb1, newdata = df_valid)

# bind the predicted and actuals for comparison
nb1_eval <- bind_cols(target = df_valid$target, predicted= nb1_valid)
nb1_eval[is.na(nb1_eval)] <- 0

# Diagnostic plots

# xy plot of model fitted and residuals
bind_cols(fitted=unname(fitted(nb1)), resid=unname(resid(nb1))) %>%
  ggplot(aes(x=fitted, y=resid)) +
  geom_point(alpha = .3) +
  geom_hline(yintercept=2, linetype='dashed') +
  geom_hline(yintercept=-2, linetype='dashed')

# xy plot of validation predictions and targets
nb1_eval %>%
  ggplot(aes(x = target, y = predicted)) +
  geom_point(alpha = .3) +
  geom_smooth(method="lm", color='grey', alpha=.3, se=FALSE)

# density plot of validation predictions and targets
nb1_eval %>%
  ggplot() +
  geom_density(aes(x=target), fill='green', alpha=0.25) +
  geom_density(aes(x=round(predicted,0)), fill='blue', alpha=0.25)
```

------------------------------------------------------------------------

## Negative Binomial Regression 2

We'll build a Zero-Inflated Negative Binomial model to handle the large number of zero values in our `labelappeal` and `stars` predictors, to see if we can improve model accuracy.

```{r, echo=TRUE}
nb2 <- zeroinfl(target ~ . | ., data=df_train, dist = 'negbin')
```

```{r}
summary(nb2)

nb2_diag <- model_diag(nb2)

nb2_diag[c('AIC','Dispersion','Log-Lik')] %>% 
  round(2) %>% nice_table()
```

### Diagnostics

The Zero-Inflated Negative Binomial model sees similar improvement as with the Zero-Inflated Poisson, but as before does not outperform the Poisson.

```{r, fig.align='default', out.width="50%", fig.height=4}
# predict based on our validation holdout
nb2_valid <- predict(nb2, newdata = df_valid)

# bind the predicted and actuals for comparison
nb2_eval <- bind_cols(target = df_valid$target, predicted= nb2_valid)
nb2_eval[is.na(nb2_eval)] <- 0

# Diagnostic plots

# xy plot of model fitted and residuals
bind_cols(fitted=unname(fitted(nb2)), resid=unname(resid(nb2))) %>%
  ggplot(aes(x=fitted, y=resid)) +
  geom_point(alpha = .3) +
  geom_hline(yintercept=2, linetype='dashed') +
  geom_hline(yintercept=-2, linetype='dashed')

# xy plot of validation predictions and targets
nb2_eval %>%
  ggplot(aes(x = target, y = predicted)) +
  geom_point(alpha = .3) +
  geom_smooth(method="lm", color='grey', alpha=.3, se=FALSE)

# density plot of validation predictions and targets
nb2_eval %>%
  ggplot() +
  geom_density(aes(x=target), fill='green', alpha=0.25) +
  geom_density(aes(x=round(predicted,0)), fill='blue', alpha=0.25)
```

------------------------------------------------------------------------

## Multiple Linear Regression 1

For our first Multiple Linear Regression, we'll use all predictors.

```{r, echo=TRUE}
lm1 <- lm(target ~ ., data=df_train)
```

```{r}
summary(lm1)

lm1_diag <- model_diag(lm1)

lm1_diag[c('AIC','Adj R2')] %>% 
  round(2) %>% nice_table()
```

### Diagnostics

...

```{r, fig.align='default', out.width="50%", fig.height=5}
# residual plots
plot(lm1)
```

```{r}
# validate and calculate RMSE
lm1_valid <- predict(lm1, newdata = df_valid)
lm1_eval <- bind_cols(target = df_valid$target, predicted=lm1_valid)
lm1_rmse <- sqrt(mean((lm1_eval$target - lm1_eval$predicted)^2)) 

# plot targets vs predicted
lm1_eval %>%
  ggplot(aes(x = target, y = predicted)) +
  geom_point(alpha = .3) +
  geom_smooth(method="lm", color='grey', alpha=.3, se=FALSE) +
  labs(title=paste('RMSE:',round(lm1_rmse,1)))
```

------------------------------------------------------------------------

## Multiple Linear Regression 2

For our second Multiple Linear Regression, we'll add stepwise feature selection.

```{r, echo=TRUE}
lm2_all <- lm(target ~ ., data=df_train)
lm2 <- stepAIC(lm2_all, trace=FALSE, direction='both')
```

```{r}
summary(lm2)

lm2_diag <- model_diag(lm2)

lm2_diag[c('AIC','Adj R2')] %>% 
  round(2) %>% nice_table()
```

### Diagnostics

...

```{r, fig.align='default',out.width="50%", fig.height=5}
# residual plots
plot(lm2)
```

```{r}
# validate and calculate RMSE
lm2_valid <- predict(lm2, newdata = df_valid)
lm2_eval <- bind_cols(target = df_valid$target, predicted=lm2_valid)
lm2_rmse <- sqrt(mean((lm2_eval$target - lm2_eval$predicted)^2)) 

# plot targets vs predicted
lm2_eval %>%
  ggplot(aes(x = target, y = predicted)) +
  geom_point(alpha = .3) +
  geom_smooth(method="lm", color='grey', alpha=.3, se=FALSE) +
  labs(title=paste('RMSE:',round(lm2_rmse,1)))
```



## Lasso

Fit zero-inflated regression models for count data via penalized maximum likelihood.



```{r}
fit.lasso <- zipath(target~.|.,data = df_train, family = "negbin")


```


```{r}
minBic <- which.min(BIC(fit.lasso))
coef(fit.lasso, minBic)
```


### Diagnostics

```{r}

#lasso_valid <- predict(fit.lasso, newdata=df_valid, which=fit.lasso$lambda.which)
lasso_valid <- predict(fit.lasso, newdata=df_valid, which=minBic)
lasso_valid <- data.frame(lasso_valid)
colnames(lasso_valid) <- c('predicted')
lasso_valid$target <- df_valid$target
lasso_eval <- bind_cols(target = df_valid$target, predicted=lasso_valid)
lasso_rmse <- sqrt(mean((lasso_eval$target - lasso_eval$predicted)^2)) 


# plot targets vs predicted
lasso_valid %>%
  ggplot(aes(x = target, y = predicted)) +
  geom_point(alpha = .3) +
  geom_smooth(method="lm", color='grey', alpha=.3, se=FALSE) +
  labs(title=paste('RMSE:',round(lasso_rmse,1)))

```









------------------------------------------------------------------------

# Model Evaluation



```{r}

tab_model(pr1, nb1,
          dv.labels = c('Poisson','Negative Binomial'),
          show.df = FALSE, show.aic = TRUE, show.fstat=TRUE, show.se = TRUE, 
          show.ci=FALSE, show.p = TRUE,show.stat=TRUE, digits.p=4)

```


```{r}
tab_model(lm1, lm2,
          dv.labels = c('Multiple Linear Regression 1','Multiple Linear Regression 2'),
          show.df = FALSE, show.aic = TRUE, show.fstat=TRUE, show.se = TRUE, 
          show.ci=FALSE, show.p = TRUE,show.stat=TRUE, digits.p=4)
```


```{r}

tab_model(pr2, nb2,
          dv.labels = c( 'Poisson zeroinfl', 'Negative Binomial zeroinfl'),
          show.df = FALSE, show.aic = TRUE, show.fstat=TRUE, show.se = TRUE, 
          show.zeroinf = TRUE, show.p = TRUE, show.ci=FALSE, show.stat=TRUE, 
          digits.p=4)

```


------------------------------------------------------------------------

# Predictions

------------------------------------------------------------------------

# Conclusion

------------------------------------------------------------------------

# Appendix

## References

**'Total Sulfur Dioxide -- Why it Matters, Too!'**\
Iowa State University\
<https://www.extension.iastate.edu/wine/total-sulfur-dioxide-why-it-matters-too/>

## R Code
